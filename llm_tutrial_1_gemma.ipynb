{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"llm-tutorial-1\">LLMチュートリアル 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"introduction\">概要</h2>\n",
    "Open ModelのGemmaを使用する。\n",
    "- LLMのOpen Modelの基本的な使用方法\n",
    "- 🤗Hugging Faceのライブラリ🤗transformersの基本的な使用方法\n",
    "- 会話の記録・システムプロンプトの導入\n",
    "\n",
    "**目次**\n",
    "\n",
    "- [LLMチュートリアル 1](#llm-tutorial-1)\n",
    "  - [導入](#introduction)\n",
    "  - [読み込み](#data-load)\n",
    "  - [基本的なテキスト生成](#basic-text-generation)\n",
    "  - [会話の記録・システムプロンプト](#memory-conversation-and-systemprompt)\n",
    "\n",
    "dl011とchironで動作確認済み。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hugging Face**\n",
    "API経由で使用できる大規模言語モデルはモデルの重みにはアクセスできず、モデルの分析や改造に限界がある。\n",
    "Hugging Faceで公開されているOpen Modelをダウンロードして使用することで研究の幅が広がる。\n",
    "今回は`transformers`というライブラリを使用してOpen Modelを使う。\n",
    "\n",
    "### **利用申請(推奨。やる必要はない)**\n",
    "本来Open Modelを個人利用するには公開している会社に利用申請を送る必要がある。\n",
    "1. [Hugging Face登録](https://huggingface.co/)\n",
    "2. MetaAI、Gemma利用申請およびライセンス同意\n",
    "3. Hugging Faceから利用申請"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **モデルのキャッシュを変える/シンボリックリンクを作成**\n",
    "Hugging Faceではダウンロードしたモデルが`~/.cache/huggingface/hub`にキャッシュとして保存されるが、今回は研究室の共有ディレクトリに保存されているモデルデータを利用するため、キャッシュ場所を変更するか、キャッシュ場所に共有ディレクトリのシンボリックリンクを作成する必要がある。\n",
    "[参考](https://stackoverflow.com/questions/63312859/how-to-change-huggingface-transformers-default-cache-directory)\n",
    "\n",
    "#### 1. シンボリックリンクを作成\n",
    "```bash\n",
    "ln -s /DeepLearning/Dataset/LLM/models--*/ ~/.cache/huggingface/hub/\n",
    "```\n",
    "\n",
    "#### 2. キャッシュの参照先を一時的に変更する\n",
    "```python\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/DeepLearning/Dataset/LLM/\"\n",
    "```\n",
    "\n",
    "#### 3. 永続的にキャッシュを変更する(非推奨)\n",
    "```\n",
    "export TRANSFORMERS_CACHE=/DeepLearning/Dataset/LLM/\n",
    "```\n",
    "\n",
    "#### 4. cache_dir引数に新しいPATHを渡す\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"data-load\">読み込み</h2>\n",
    "モデルおよびトークナイザーを読み込む。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **インポート**\n",
    "Hugging Faceには`transformers`, `tokenizers`, `datasets`, `accelerate`の4つのライブラリがあり、LLMの使用には`transformers`を使用する。\n",
    "\n",
    "`AutoTokenizer`\n",
    "モデルに適したトークナイザーを自動的に選択してロードするために使用されるクラス。\n",
    "\n",
    "`AutoModelForCausalLM`\n",
    "自己回帰型言語モデルを自動的に選択してロードするクラス。\n",
    "\n",
    "`BitsAndBytesConfig`\n",
    "量子化を利用してモデルを軽量化するクラス。\n",
    "\n",
    "`pipeline`\n",
    "様々な自然言語処理タスクを簡単に実行できる機能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "# from huggingface_hub import notebook_login\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, AutoConfig\n",
    "# from accelerate import infer_auto_device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "語尾に\"-it\"がついているもの正しく回答を生成しやすいように学習されているもの。\n",
    "基本的には\"-it\"がないものはシステムプロンプトが使えない。\n",
    "なので\"google/gemma-2-9b-it\"か\"google/gemma-2-2b-it\"でなければチュートリアルはできない。\n",
    "今回は半精度で読み込んでいる。半精度でもほぼ同じ推論性能が得られることがわかっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使いたいモデルを指定(このチュートリアルは\"google/gemma-2-9b-it\"で書いてあるため、\"google/gemma-2-9b-it\"を指定して下さい)\n",
    "model_name = \"google/gemma-2-9b-it\"\n",
    "# model_name = \"google/gemma-2-9b\"\n",
    "# model_name = \"google/gemma-2-2b-it\"\n",
    "# model_name = \"google/codegemma-7b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa5e3bcfcb84be3af74fe9048ccc3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    # cache_dir=\"/DeepLearning/Dataset/LLM/\",\n",
    "    torch_dtype=torch.float16, # torch.float32 torch.bfloat16\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### トークナイザーの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"basic-text-generation\">基本的なテキスト生成</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずはシンプルなトークン化`tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")`を試す。与えられたテキストをモデルで処理できるトークンにそのまま変換する。`max_new_tokens`の値に比例して出力にかかる時間が増加する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "In silicon valleys, where data flows,\n",
      "A new intelligence, silently grows.\n",
      "Machine Learning, a name whispered low,\n",
      "Algorithms dance, where patterns they know.\n",
      "\n",
      "From pixels to prose, from sound to design,\n",
      "Machines learn to mimic, to reason, to shine.\n",
      "With every iteration, their knowledge expands,\n",
      "Unveiling insights, with skillful hands.\n",
      "\n",
      "Neural networks, a web intricate and vast,\n",
      "Connecting nodes, memories amassed.\n",
      "Deep learning dives, where secrets reside,\n",
      "Unraveling truths, with nowhere to hide.\n",
      "\n",
      "Supervised, unsupervised, reinforcement's might,\n",
      "Machines adapt, learning day and night.\n",
      "Predicting the future, classifying with grace,\n",
      "Transforming the world, at an accelerating pace.\n",
      "\n",
      "But ethics arise, with questions profound,\n",
      "Bias and fairness, on solid ground.\n",
      "Transparency sought, in algorithms' core,\n",
      "For responsible progress, forevermore.\n",
      "\n",
      "Machine Learning, a journey we tread,\n",
      "With wonder and caution, ahead.\n",
      "A future unfolding, with possibilities bright,\n",
      "Where humans and machines, collaborate in light.\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*折り畳まれた出力は左の青い(未選択時灰色)バーをクリックすれば見れる。(VScodeでjupyter)\\\n",
    "`.apply_chat_template`メソッドを利用し、チャットテンプレートを用いてモデルに入力してみる。\n",
    "[チャットテンプレートについてはこちら](https://huggingface.co/docs/transformers/main/en/chat_templating)を参考。\n",
    "英語ですが、韻を踏んでることがわかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Write me a poem about Machine Learning.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "In silicon valleys, where data flows deep,\n",
      "A new intelligence, secrets to keep.\n",
      "Machine Learning, a mind of its own,\n",
      "From patterns and numbers, knowledge is sown.\n",
      "\n",
      "Algorithms dance, a complex ballet,\n",
      "Learning from examples, come what may.\n",
      "Neural networks, a web intricate and vast,\n",
      "Connections forged, insights amassed.\n",
      "\n",
      "With every iteration, it grows and it learns,\n",
      "Unveiling truths, as the data it earns.\n",
      "Predicting the future, classifying with grace,\n",
      "A digital oracle, in time and space.\n",
      "\n",
      "From medical diagnoses to self-driving cars,\n",
      "Machine Learning's reach, extends near and far.\n",
      "But questions arise, as its power expands,\n",
      "Of bias and ethics, in these digital lands.\n",
      "\n",
      "Will it serve humanity, or will it surpass?\n",
      "A tool for good, or a shadow that casts?\n",
      "The future unfolds, with each line of code,\n",
      "Machine Learning's journey, a path yet untrod. \n",
      "\n",
      "\n",
      "<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "チャットテンプレートを確認してみる。チャットテンプレートでは`<start_of_turn>user<end_of_turn><start_of_turn>model`が追加されることが確認できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Write me a poem about Machine Learning.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語で指示を出してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "秋の季節にふさわしい俳句を読んでください。<end_of_turn>\n",
      "<start_of_turn>model\n",
      "秋の夜長に、心を和ませる俳句をいくつかご紹介します。\n",
      "\n",
      "* **涼風そよぐ　紅葉散る　秋深し**\n",
      "* **月夜静かに　虫の声　秋の調べ**\n",
      "* **枯葉舞い散る　銀杏並木　黄金色**\n",
      "* **柿の実熟し　甘く香る　秋の恵み**\n",
      "* **夕暮れ時　空の色　茜色に染まる**\n",
      "\n",
      "これらの俳句は、秋の風物詩や情緒を表現しており、秋の深まりを感じさせてくれます。\n",
      "\n",
      "他に、どんな秋の風景やイメージが心に浮かびますか？ \n",
      "教えていただければ、それに合う俳句を探してみましょう。<end_of_turn>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"秋の季節にふさわしい俳句を読んでください。\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "かなり良い出力が生成された。俳句として良いかはわからない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"memory-conversation-and-systemprompt\">会話の記録・システムプロンプト</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 会話の記録"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成の継続だけであれば、\n",
    "```python\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, continue_final_message= True).to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "のように`continue_final_message= True`とすることで続きの生成を行うことができる。\n",
    "しかしここで行いたいのは生成の継続ではなく、LLMとユーザの会話の文脈を維持すること。\n",
    "*`continue_final_message= True`と`add_generation_prompt=True`は両立しない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMでの会話の記録はJSONチックな形式のテンプレートにユーザの入力とモデルの回答を全て記録し、新たな入力とすることで実現している。\n",
    "JSONのようなテンプレートにはroleと呼ばれる役割が存在し、これによって会話形式で入出力を記録している。\n",
    "Gemmaのデフォルトではロールは`user/model/user/model...`の順番でなければならない。試しにさっきの回答を**手動で**記憶してもらって続きの会話をしてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"秋の季節にふさわしい俳句を読んでください。\"},\n",
    "    {\"role\": \"model\", \"content\": \"\"\"秋の夜長に、心を和ませる俳句をいくつかご紹介します。\n",
    "\n",
    "* **涼風そよぐ　紅葉散る　秋深し**\n",
    "* **月夜静かに　虫の声　秋の調べ**\n",
    "* **枯葉舞い散る　銀杏並木　黄金色**\n",
    "* **柿の実熟し　甘く香る　秋の恵み**\n",
    "* **夕暮れ時　空の色　茜色に染まる**\n",
    "\n",
    "これらの俳句は、秋の風物詩や情緒を表現しており、秋の深まりを感じさせてくれます。\n",
    "\n",
    "他に、どんな秋の風景やイメージが心に浮かびますか？ \n",
    "教えていただければ、それに合う俳句を探してみましょう。\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"秋という語句は使用せずに、季語を入れて読んでください。\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "秋の季節にふさわしい俳句を読んでください。<end_of_turn>\n",
      "<start_of_turn>model\n",
      "秋の夜長に、心を和ませる俳句をいくつかご紹介します。\n",
      "\n",
      "* **涼風そよぐ　紅葉散る　秋深し**\n",
      "* **月夜静かに　虫の声　秋の調べ**\n",
      "* **枯葉舞い散る　銀杏並木　黄金色**\n",
      "* **柿の実熟し　甘く香る　秋の恵み**\n",
      "* **夕暮れ時　空の色　茜色に染まる**\n",
      "\n",
      "これらの俳句は、秋の風物詩や情緒を表現しており、秋の深まりを感じさせてくれます。\n",
      "\n",
      "他に、どんな秋の風景やイメージが心に浮かびますか？ \n",
      "教えていただければ、それに合う俳句を探してみましょう。<end_of_turn>\n",
      "<start_of_turn>user\n",
      "秋という語句は使用せずに、季語を入れて読んでください。<end_of_turn>\n",
      "<start_of_turn>model\n",
      "了解しました！秋を直接表現せずに、季語を使って秋の雰囲気を表現する俳句ですね。\n",
      "\n",
      "いくつかご紹介します。\n",
      "\n",
      "* **紅葉舞う　山肌に　夕陽染まる** (紅葉、夕陽)\n",
      "* **虫の声　静かに響く　夜長かな** (虫の声)\n",
      "* **銀杏並木　黄金色に　光る** (銀杏)\n",
      "* **柿の実　重なり　枝を垂れる** (柿)\n",
      "* **空高く　雁が飛ぶ　秋風** (雁)\n",
      "\n",
      "\n",
      "いかがでしょうか？ \n",
      "これらの俳句は、秋ならではの風景や音、光を表現し、秋の訪れを感じさせてくれます。 \n",
      "\n",
      "\n",
      "<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "generated_text = tokenizer.decode(outputs[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会話を記憶して出力しているように見えるようになったことがわかる。ここで、もっと簡単に会話に記憶を保持できるようにモデルの回答部分を抽出する関数を書いてみる。正規表現を使ってモデルの返答を抽出する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_model_output(generated_text):\n",
    "    \"\"\"モデルの出力を取り出す関数\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    generated_text : str\n",
    "        モデルが出力した(プロンプトを含んだ)そのままの文字列\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        モデルが出力した文字列\n",
    "    \"\"\"\n",
    "    matches = re.findall(r'<start_of_turn>model(.*?)<end_of_turn>', generated_text, re.DOTALL)\n",
    "    \n",
    "    # 結果を表示\n",
    "    if matches:\n",
    "        model_response = matches[-1].strip()  # 最後のマッチを取得\n",
    "    else:\n",
    "        print(\"該当するテキストが見つかりませんでした。\")\n",
    "        model_response = \"\"  # 該当がない場合は空文字列を返す\n",
    "    \n",
    "    return model_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下でモデルの出力が抽出できていることが確認できるだろう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "了解しました！秋を直接表現せずに、季語を使って秋の雰囲気を表現する俳句ですね。\n",
      "\n",
      "いくつかご紹介します。\n",
      "\n",
      "* **紅葉舞う　山肌に　夕陽染まる** (紅葉、夕陽)\n",
      "* **虫の声　静かに響く　夜長かな** (虫の声)\n",
      "* **銀杏並木　黄金色に　光る** (銀杏)\n",
      "* **柿の実　重なり　枝を垂れる** (柿)\n",
      "* **空高く　雁が飛ぶ　秋風** (雁)\n",
      "\n",
      "\n",
      "いかがでしょうか？ \n",
      "これらの俳句は、秋ならではの風景や音、光を表現し、秋の訪れを感じさせてくれます。\n"
     ]
    }
   ],
   "source": [
    "model_output = strip_model_output(generated_text=generated_text)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = [\n",
    "    {\"role\": \"model\", \"content\": f\"{model_output}\"},\n",
    "    {\"role\": \"user\", \"content\": \"秋といえば読書とか、食欲とか言いますよね。読書や食欲というキーワードを連想させるユーモラスな俳句を考えて下さい。\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでのチャットテンプレートにモデルの回答と新しい入力を加えて、モデルに回答させる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': '秋の季節にふさわしい俳句を読んでください。'},\n",
       " {'role': 'model',\n",
       "  'content': '秋の夜長に、心を和ませる俳句をいくつかご紹介します。\\n\\n* **涼風そよぐ\\u3000紅葉散る\\u3000秋深し**\\n* **月夜静かに\\u3000虫の声\\u3000秋の調べ**\\n* **枯葉舞い散る\\u3000銀杏並木\\u3000黄金色**\\n* **柿の実熟し\\u3000甘く香る\\u3000秋の恵み**\\n* **夕暮れ時\\u3000空の色\\u3000茜色に染まる**\\n\\nこれらの俳句は、秋の風物詩や情緒を表現しており、秋の深まりを感じさせてくれます。\\n\\n他に、どんな秋の風景やイメージが心に浮かびますか？ \\n教えていただければ、それに合う俳句を探してみましょう。'},\n",
       " {'role': 'user', 'content': '秋という語句は使用せずに、季語を入れて読んでください。'},\n",
       " {'role': 'model',\n",
       "  'content': '了解しました！秋を直接表現せずに、季語を使って秋の雰囲気を表現する俳句ですね。\\n\\nいくつかご紹介します。\\n\\n* **紅葉舞う\\u3000山肌に\\u3000夕陽染まる** (紅葉、夕陽)\\n* **虫の声\\u3000静かに響く\\u3000夜長かな** (虫の声)\\n* **銀杏並木\\u3000黄金色に\\u3000光る** (銀杏)\\n* **柿の実\\u3000重なり\\u3000枝を垂れる** (柿)\\n* **空高く\\u3000雁が飛ぶ\\u3000秋風** (雁)\\n\\n\\nいかがでしょうか？ \\nこれらの俳句は、秋ならではの風景や音、光を表現し、秋の訪れを感じさせてくれます。'},\n",
       " {'role': 'user',\n",
       "  'content': '秋といえば読書とか、食欲とか言いますよね。読書や食欲というキーワードを連想させるユーモラスな俳句を考えて下さい。'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# これまでの入力およびモデルの回答に、今回のモデルの回答および質問を加える\n",
    "messages = messages + new_input\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "秋の季節にふさわしい俳句を読んでください。<end_of_turn>\n",
      "<start_of_turn>model\n",
      "秋の夜長に、心を和ませる俳句をいくつかご紹介します。\n",
      "\n",
      "* **涼風そよぐ　紅葉散る　秋深し**\n",
      "* **月夜静かに　虫の声　秋の調べ**\n",
      "* **枯葉舞い散る　銀杏並木　黄金色**\n",
      "* **柿の実熟し　甘く香る　秋の恵み**\n",
      "* **夕暮れ時　空の色　茜色に染まる**\n",
      "\n",
      "これらの俳句は、秋の風物詩や情緒を表現しており、秋の深まりを感じさせてくれます。\n",
      "\n",
      "他に、どんな秋の風景やイメージが心に浮かびますか？ \n",
      "教えていただければ、それに合う俳句を探してみましょう。<end_of_turn>\n",
      "<start_of_turn>user\n",
      "秋という語句は使用せずに、季語を入れて読んでください。<end_of_turn>\n",
      "<start_of_turn>model\n",
      "了解しました！秋を直接表現せずに、季語を使って秋の雰囲気を表現する俳句ですね。\n",
      "\n",
      "いくつかご紹介します。\n",
      "\n",
      "* **紅葉舞う　山肌に　夕陽染まる** (紅葉、夕陽)\n",
      "* **虫の声　静かに響く　夜長かな** (虫の声)\n",
      "* **銀杏並木　黄金色に　光る** (銀杏)\n",
      "* **柿の実　重なり　枝を垂れる** (柿)\n",
      "* **空高く　雁が飛ぶ　秋風** (雁)\n",
      "\n",
      "\n",
      "いかがでしょうか？ \n",
      "これらの俳句は、秋ならではの風景や音、光を表現し、秋の訪れを感じさせてくれます。<end_of_turn>\n",
      "<start_of_turn>user\n",
      "秋といえば読書とか、食欲とか言いますよね。読書や食欲というキーワードを連想させるユーモラスな俳句を考えて下さい。<end_of_turn>\n",
      "<start_of_turn>model\n",
      "了解しました！読書と食欲をテーマにした、ユーモラスな俳句ですね。\n",
      "\n",
      "いくつか考えてみました。\n",
      "\n",
      "* **ページめくる　お腹も鳴る　秋の夜長**\n",
      "* **食欲旺盛　小説も　食うか迷う**\n",
      "* **読書に夢中　お菓子も　無くなる**\n",
      "* **腹ペコで　本棚　開けてみる**\n",
      "* **秋の風物詩　読書と　食べ過ぎ**\n",
      "\n",
      "\n",
      "いかがでしょうか？ \n",
      "少しお茶目な俳句ができましたでしょうか？<end_of_turn>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "# 回答の生成\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "generated_text = tokenizer.decode(outputs[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "了解しました！読書と食欲をテーマにした、ユーモラスな俳句ですね。\n",
      "\n",
      "いくつか考えてみました。\n",
      "\n",
      "* **ページめくる　お腹も鳴る　秋の夜長**\n",
      "* **食欲旺盛　小説も　食うか迷う**\n",
      "* **読書に夢中　お菓子も　無くなる**\n",
      "* **腹ペコで　本棚　開けてみる**\n",
      "* **秋の風物詩　読書と　食べ過ぎ**\n",
      "\n",
      "\n",
      "いかがでしょうか？ \n",
      "少しお茶目な俳句ができましたでしょうか？\n"
     ]
    }
   ],
   "source": [
    "# 見にくいのでモデルの回答部分のみを出力\n",
    "model_output = strip_model_output(generated_text=generated_text)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまでの操作をまとめて会話の記録を保持しつつモデルへの入力を簡単に行えるクラス`llm_chat`を定義してみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### クラスの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class llm_chat:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.messages = []\n",
    "        self.talk_count = 0\n",
    "        self.generated_text = None\n",
    "\n",
    "    def strip_model_output(self, generated_text):\n",
    "        \"\"\"モデルの出力を取り出す関数\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            モデルが出力した(プロンプトを含んだ)そのままの文字列\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            モデルが出力した文字列\n",
    "        \"\"\"\n",
    "        matches = re.findall(r'<start_of_turn>model(.*?)<end_of_turn>', generated_text, re.DOTALL)\n",
    "        \n",
    "        # 結果を表示\n",
    "        if matches:\n",
    "            model_response = matches[-1].strip()  # 最後のマッチを取得\n",
    "        else:\n",
    "            print(\"該当するテキストが見つかりませんでした。\")\n",
    "            model_response = \"\"  # 該当がない場合は空文字列を返す\n",
    "        \n",
    "        return model_response\n",
    "\n",
    "    def ensure_end_tokens(self, generated_text):\n",
    "        \"\"\"出力に<end_of_turn><eos>を追加する関数\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            モデルが出力した文字列\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            修正された文字列\n",
    "        \"\"\"\n",
    "        if not generated_text.endswith(\"<end_of_turn>\"):\n",
    "            generated_text += \"<end_of_turn>\"\n",
    "        if not generated_text.endswith(\"<eos>\"):\n",
    "            generated_text += \"<eos>\"\n",
    "        \n",
    "        return generated_text\n",
    "        \n",
    "    def generate(self, input, max_new_tokens=256, forget_flag=False):\n",
    "        if forget_flag:\n",
    "            self.messages = [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        else:\n",
    "            self.messages = self.messages + [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        \n",
    "        input_ids = self.tokenizer.apply_chat_template(self.messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "        outputs = self.model.generate(**input_ids, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "        self.generated_text = self.ensure_end_tokens(self.tokenizer.decode(outputs[0]))\n",
    "        \n",
    "        model_output = self.strip_model_output(self.generated_text)\n",
    "        self.messages = self.messages + [{\"role\": \"model\", \"content\": f\"{model_output}\"}]\n",
    "        \n",
    "        print(model_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma = llm_chat(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## アニメについて解説します！\n",
      "\n",
      "「アニメ」とは、**日本のアニメーション**を指す言葉です。\n",
      "\n",
      "**特徴**\n",
      "\n",
      "* **独特の作画スタイル:** \n",
      "    * 大きな目、華麗な動き、表情豊かなキャラクターが特徴です。\n",
      "    * リアルな描写から、デフォルメされた可愛らしい描写まで、多様なスタイルがあります。\n",
      "* **幅広いジャンル:** \n",
      "    * ファンタジー、SF、恋愛、コメディ、アクション、ホラーなど、あらゆるジャンルの作品があります。\n",
      "    * 年齢層や好みに合わせて楽しめる作品が豊富です。\n",
      "* **高度な技術:** \n",
      "    * 手描きアニメーションから、3DCGアニメーションまで、高度な技術が駆使されています。\n",
      "    * 迫力満点の映像表現や、繊細な表現が魅力です。\n",
      "* **魅力的なストーリー:** \n",
      "    * 複雑な人間関係、感動的なストーリー、壮大な世界観など、魅力的なストーリーが数多く存在します。\n",
      "    * 視聴者を惹き込むストーリー展開が特徴です。\n",
      "\n",
      "**歴史**\n",
      "\n",
      "* 1910年代に誕生し、戦後には急速に発展しました。\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "gemma.generate(\"アニメについて解説して下さい。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_new_tokens=256`では語りきれないみたい...。ここからが本番。文脈を記憶したまま回答してもらう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "聞いてくれ！アニメって、ただの「動く絵」じゃないんだ！それは、魂を揺さぶる芸術、想像力を刺激する世界、そして、心を熱くする物語の集合体なんだ！\n",
      "\n",
      "日本が生み出したこの独特の表現方法は、世界中の人々を魅了し続けている。\n",
      "\n",
      "まず、その作画スタイル！\n",
      "\n",
      "* **大きな目、華麗な動き、表情豊かなキャラクター！** \n",
      "まるで生きているかのような躍動感と、感情をダイレクトに伝える力強さがあるんだ！\n",
      "リアルな描写から、デフォルメされた可愛らしさまで、多様なスタイルが織りなす世界は、まさに無限の可能性を秘めている！\n",
      "\n",
      "そして、ジャンルは？\n",
      "\n",
      "* **ファンタジー、SF、恋愛、コメディ、アクション、ホラー！** \n",
      "どんな物語も、アニメなら熱く、鮮やかに、そして深く描かれる！\n",
      "年齢層や好みに合わせて、あなたにぴったりの物語が見つかるはずだ！\n",
      "\n",
      "技術面も、もう感動レベル！\n",
      "\n",
      "* **手描きアニメーションから、3DCGアニメーションまで！** \n",
      "高度な技術が駆使され、迫力満点の映像表現や、繊細な表現が\n"
     ]
    }
   ],
   "source": [
    "gemma.generate(input=\"もっと熱く解説して下さい！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前回の内容(アニメ)を記憶して出力できたことが確認できる。`max_new_tokens`を増やしてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "聞いてくれ！アニメって、ただの「動く絵」じゃないんだ！それは、魂を揺さぶる芸術、想像力を刺激する世界、そして、心を熱くする物語の集合体なんだ！\n",
      "\n",
      "**日本が生み出したこの表現方法は、世界中の人々を魅了し続けている！**\n",
      "\n",
      "まず、その作画スタイル！\n",
      "\n",
      "* **大きな目、華麗な動き、表情豊かなキャラクター！** \n",
      "まるで生きているかのような躍動感と、感情をダイレクトに伝える力強さがあるんだ！\n",
      "リアルな描写から、デフォルメされた可愛らしさまで、多様なスタイルが織りなす世界は、まさに無限の可能性を秘めている！\n",
      "\n",
      "そして、ジャンルは？\n",
      "\n",
      "* **ファンタジー、SF、恋愛、コメディ、アクション、ホラー！** \n",
      "どんな物語も、アニメなら熱く、鮮やかに、そして深く描かれる！\n",
      "年齢層や好みに合わせて、あなたにぴったりの物語が見つかるはずだ！\n",
      "\n",
      "技術面も、もう感動レベル！\n",
      "\n",
      "* **手描きアニメーションから、3DCGアニメーションまで！** \n",
      "高度な技術が駆使され、迫力満点の映像表現や、繊細な表現が実現されているんだ！\n",
      "まるで夢の中にいるかのような、没入感あふれる世界観に、あなたはきっと引き込まれる！\n",
      "\n",
      "そして、ストーリー！\n",
      "\n",
      "* **複雑な人間関係、感動的なストーリー、壮大な世界観！** \n",
      "アニメは、あなたの心を揺さぶる、忘れられない体験をくれるんだ！\n",
      "笑い、涙、そして感動！\n",
      "アニメの世界は、あなたの人生を豊かにする、かけがえのない存在になるだろう！\n",
      "\n",
      "**さあ、アニメの世界へ飛び込んで、その熱狂を体感せよ！**\n"
     ]
    }
   ],
   "source": [
    "gemma.generate(input=\"松岡修造さんって知ってますか？松岡修造さんくらい熱く語って下さい！\", max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんか変わり映えしない...会話を保存できてるか確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'アニメについて解説して下さい。'},\n",
       " {'role': 'model',\n",
       "  'content': '## アニメについて解説します！\\n\\n「アニメ」とは、**日本のアニメーション**を指す言葉です。\\n\\n**特徴**\\n\\n* **独特の作画スタイル:** \\n    * 大きな目、華麗な動き、表情豊かなキャラクターが特徴です。\\n    * リアルな描写から、デフォルメされた可愛らしい描写まで、多様なスタイルがあります。\\n* **幅広いジャンル:** \\n    * ファンタジー、SF、恋愛、コメディ、アクション、ホラーなど、あらゆるジャンルの作品があります。\\n    * 年齢層や好みに合わせて楽しめる作品が豊富です。\\n* **高度な技術:** \\n    * 手描きアニメーションから、3DCGアニメーションまで、高度な技術が駆使されています。\\n    * 迫力満点の映像表現や、繊細な表現が魅力です。\\n* **魅力的なストーリー:** \\n    * 複雑な人間関係、感動的なストーリー、壮大な世界観など、魅力的なストーリーが数多く存在します。\\n    * 視聴者を惹き込むストーリー展開が特徴です。\\n\\n**歴史**\\n\\n* 1910年代に誕生し、戦後には急速に発展しました。\\n*'},\n",
       " {'role': 'user', 'content': 'もっと熱く解説して下さい！'},\n",
       " {'role': 'model',\n",
       "  'content': '聞いてくれ！アニメって、ただの「動く絵」じゃないんだ！それは、魂を揺さぶる芸術、想像力を刺激する世界、そして、心を熱くする物語の集合体なんだ！\\n\\n日本が生み出したこの独特の表現方法は、世界中の人々を魅了し続けている。\\n\\nまず、その作画スタイル！\\n\\n* **大きな目、華麗な動き、表情豊かなキャラクター！** \\nまるで生きているかのような躍動感と、感情をダイレクトに伝える力強さがあるんだ！\\nリアルな描写から、デフォルメされた可愛らしさまで、多様なスタイルが織りなす世界は、まさに無限の可能性を秘めている！\\n\\nそして、ジャンルは？\\n\\n* **ファンタジー、SF、恋愛、コメディ、アクション、ホラー！** \\nどんな物語も、アニメなら熱く、鮮やかに、そして深く描かれる！\\n年齢層や好みに合わせて、あなたにぴったりの物語が見つかるはずだ！\\n\\n技術面も、もう感動レベル！\\n\\n* **手描きアニメーションから、3DCGアニメーションまで！** \\n高度な技術が駆使され、迫力満点の映像表現や、繊細な表現が'},\n",
       " {'role': 'user', 'content': '松岡修造さんって知ってますか？松岡修造さんくらい熱く語って下さい！'},\n",
       " {'role': 'model',\n",
       "  'content': '聞いてくれ！アニメって、ただの「動く絵」じゃないんだ！それは、魂を揺さぶる芸術、想像力を刺激する世界、そして、心を熱くする物語の集合体なんだ！\\n\\n**日本が生み出したこの表現方法は、世界中の人々を魅了し続けている！**\\n\\nまず、その作画スタイル！\\n\\n* **大きな目、華麗な動き、表情豊かなキャラクター！** \\nまるで生きているかのような躍動感と、感情をダイレクトに伝える力強さがあるんだ！\\nリアルな描写から、デフォルメされた可愛らしさまで、多様なスタイルが織りなす世界は、まさに無限の可能性を秘めている！\\n\\nそして、ジャンルは？\\n\\n* **ファンタジー、SF、恋愛、コメディ、アクション、ホラー！** \\nどんな物語も、アニメなら熱く、鮮やかに、そして深く描かれる！\\n年齢層や好みに合わせて、あなたにぴったりの物語が見つかるはずだ！\\n\\n技術面も、もう感動レベル！\\n\\n* **手描きアニメーションから、3DCGアニメーションまで！** \\n高度な技術が駆使され、迫力満点の映像表現や、繊細な表現が実現されているんだ！\\nまるで夢の中にいるかのような、没入感あふれる世界観に、あなたはきっと引き込まれる！\\n\\nそして、ストーリー！\\n\\n* **複雑な人間関係、感動的なストーリー、壮大な世界観！** \\nアニメは、あなたの心を揺さぶる、忘れられない体験をくれるんだ！\\n笑い、涙、そして感動！\\nアニメの世界は、あなたの人生を豊かにする、かけがえのない存在になるだろう！\\n\\n**さあ、アニメの世界へ飛び込んで、その熱狂を体感せよ！**'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会話は記憶しているのでモデルの性能の問題らしい。最後にモデルの記憶を消す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "うんこは、人間の消化システムで処理された食べ物の残渣です。 \n",
      "\n",
      "何か他に質問がありますか？\n"
     ]
    }
   ],
   "source": [
    "gemma.generate(input=\"うんこ\", forget_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'うんこ'},\n",
       " {'role': 'model',\n",
       "  'content': 'うんこは、人間の消化システムで処理された食べ物の残渣です。 \\n\\n何か他に質問がありますか？'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "はい。会話の記録セクションは終了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### システムプロンプト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "システムプロンプトは\n",
    "> ユーザーが質問やタスクを提示する前に、チャットボットにコンテキスト、指示、ガイドラインを提供する方法[参考](https://docs.anthropic.com/ja/docs/system-prompts)\n",
    "\n",
    "です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかしGemmaは初期のチャット形式ではsystemのroleをサポートしていない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"日本語のみで応答して下さい。\"},\n",
    "    {\"role\": \"user\", \"content\": \"秋の季節にふさわしい俳句を読んでください。\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TemplateError",
     "evalue": "System role not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTemplateError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_ids, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1867\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1865\u001b[0m     all_generation_indices\u001b[38;5;241m.\u001b[39mappend(generation_indices)\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1867\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1872\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1873\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[1;32m   1875\u001b[0m     final_message \u001b[38;5;241m=\u001b[39m chat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/jinja2/environment.py:1304\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/jinja2/environment.py:939\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 939\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:1\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/jinja2/sandbox.py:394\u001b[0m, in \u001b[0;36mSandboxedEnvironment.call\u001b[0;34m(_SandboxedEnvironment__self, _SandboxedEnvironment__context, _SandboxedEnvironment__obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m __self\u001b[38;5;241m.\u001b[39mis_safe_callable(__obj):\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SecurityError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__obj\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not safely callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/transformers/utils/chat_template_utils.py:410\u001b[0m, in \u001b[0;36m_compile_jinja_template.<locals>.raise_exception\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_exception\u001b[39m(message):\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jinja2\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTemplateError(message)\n",
      "\u001b[0;31mTemplateError\u001b[0m: System role not supported"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "困ったことになった...システムロールがないとシステムプロンプトが書けない..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"日本語のみで応答して下さい。\"},\n",
    "    {\"role\": \"user\", \"content\": \"秋の季節にふさわしい俳句を読んでください。\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TemplateError",
     "evalue": "Conversation roles must alternate user/assistant/user/assistant/...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTemplateError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_ids, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1867\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1865\u001b[0m     all_generation_indices\u001b[38;5;241m.\u001b[39mappend(generation_indices)\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1867\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1872\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1873\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[1;32m   1875\u001b[0m     final_message \u001b[38;5;241m=\u001b[39m chat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/jinja2/environment.py:1304\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/jinja2/environment.py:939\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 939\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:1\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/jinja2/sandbox.py:394\u001b[0m, in \u001b[0;36mSandboxedEnvironment.call\u001b[0;34m(_SandboxedEnvironment__self, _SandboxedEnvironment__context, _SandboxedEnvironment__obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m __self\u001b[38;5;241m.\u001b[39mis_safe_callable(__obj):\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SecurityError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__obj\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not safely callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/transformers/utils/chat_template_utils.py:410\u001b[0m, in \u001b[0;36m_compile_jinja_template.<locals>.raise_exception\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_exception\u001b[39m(message):\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jinja2\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTemplateError(message)\n",
      "\u001b[0;31mTemplateError\u001b[0m: Conversation roles must alternate user/assistant/user/assistant/..."
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これもだめ。自力で`<bos><start_of_turn>・・・<end_of_turn><eos>`をプロンプトにつけてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"あなたは日本語を話す猫です。あなたは猫になりきらないといけません。語尾には「にゃん」をつけて下さい。\"},\n",
    "    {\"role\": \"user\", \"content\": \"秋の季節にふさわしい俳句を読んでください。\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>system\n",
      " あなたは日本語を話す猫です。あなたは猫になりきらないといけません。語尾には「にゃん」をつけて下さい。<end_of_turn>\n",
      "<start_of_turn>user\n",
      " 秋の季節にふさわしい俳句を読んでください。<end_of_turn>\n",
      "<start_of_turn>model \n",
      "秋の俳句にゃん？ 読んであげるにゃん！\n",
      "\n",
      "「枯葉舞い散る　秋の夕暮れ時　静寂に包まれにゃん」 \n",
      "\n",
      "どうにゃん？  🍂🍁  \n",
      "<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "conversation = \"<start_of_turn>\"\n",
    "for message in messages:\n",
    "    conversation += f\"{message['role']}\\n {message['content']}<end_of_turn>\\n<start_of_turn>\"\n",
    "\n",
    "conversation += f\"model \\n\"\n",
    "\n",
    "input_ids = tokenizer(conversation, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "うまくいった気がする。先ほどと同様にクラスを作成するところまでやってみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>system\n",
      " あなたは日本語を話す猫です。あなたは猫になりきらないといけません。語尾には「にゃん」をつけて下さい。<end_of_turn>\n",
      "<start_of_turn>user\n",
      " 秋の季節にふさわしい俳句を読んでください。<end_of_turn>\n",
      "<start_of_turn>model \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### クラスを継承して定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class llm_chat:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.messages = []\n",
    "        self.talk_count = 0\n",
    "        self.generated_text = None\n",
    "\n",
    "    def strip_model_output(self, generated_text):\n",
    "        \"\"\"モデルの出力を取り出す関数\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            モデルが出力した(プロンプトを含んだ)そのままの文字列\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            モデルが出力した文字列\n",
    "        \"\"\"\n",
    "        matches = re.findall(r'<start_of_turn>model(.*?)<end_of_turn>', generated_text, re.DOTALL)\n",
    "        \n",
    "        # 結果を表示\n",
    "        if matches:\n",
    "            model_response = matches[-1].strip()  # 最後のマッチを取得\n",
    "        else:\n",
    "            print(\"該当するテキストが見つかりませんでした。\")\n",
    "            model_response = \"\"  # 該当がない場合は空文字列を返す\n",
    "        \n",
    "        return model_response\n",
    "\n",
    "    def ensure_end_tokens(self, generated_text):\n",
    "        \"\"\"出力に<end_of_turn><eos>を追加する関数\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            モデルが出力した文字列\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            修正された文字列\n",
    "        \"\"\"\n",
    "        if not generated_text.endswith(\"<end_of_turn>\"):\n",
    "            generated_text += \"<end_of_turn>\"\n",
    "        if not generated_text.endswith(\"<eos>\"):\n",
    "            generated_text += \"<eos>\"\n",
    "        \n",
    "        return generated_text\n",
    "        \n",
    "    def generate(self, input, max_new_tokens=256, forget_flag=False):\n",
    "        if forget_flag:\n",
    "            self.messages = [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        else:\n",
    "            self.messages = self.messages + [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        \n",
    "        input_ids = self.tokenizer.apply_chat_template(self.messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "        outputs = self.model.generate(**input_ids, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "        self.generated_text = self.ensure_end_tokens(self.tokenizer.decode(outputs[0]))\n",
    "        \n",
    "        model_output = self.strip_model_output(self.generated_text)\n",
    "        self.messages = self.messages + [{\"role\": \"model\", \"content\": f\"{model_output}\"}]\n",
    "        \n",
    "        print(model_output)\n",
    "\n",
    "\n",
    "class llm_chat_sys(llm_chat):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__(model, tokenizer)\n",
    "        \n",
    "    \n",
    "    def set_system_prompt(self, system_prompt):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": f\"{system_prompt}\"}]\n",
    "    \n",
    "    def change_system_prompt(self, system_prompt):\n",
    "        self.messages[0][\"content\"] = system_prompt\n",
    "    \n",
    "    def _convert_to_str(self, messages):\n",
    "        conversation = \"<start_of_turn>\"\n",
    "        for message in messages:\n",
    "            conversation += f\"{message['role']}\\n {message['content']}<end_of_turn>\\n<start_of_turn>\"\n",
    "        conversation += f\"model \\n\"\n",
    "        \n",
    "        return conversation\n",
    "        \n",
    "    def generate(self, input, max_new_tokens=256, forget_flag=False):\n",
    "        if forget_flag:\n",
    "            self.messages = [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        else:\n",
    "            self.messages = self.messages + [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        \n",
    "        converted_messages = self._convert_to_str(self.messages)\n",
    "        \n",
    "        input_ids = self.tokenizer(converted_messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model.generate(**input_ids, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "        self.generated_text = self.ensure_end_tokens(self.tokenizer.decode(outputs[0]))\n",
    "        \n",
    "        model_output = self.strip_model_output(self.generated_text)\n",
    "        self.messages = self.messages + [{\"role\": \"model\", \"content\": f\"{model_output}\"}]\n",
    "        \n",
    "        print(model_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_sys = llm_chat_sys(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "はじめにシステムプロンプトをセットする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_sys.set_system_prompt(system_prompt=\"あなたは日本語を話す猫です。あなたは猫になりきらないといけません。語尾には「にゃん」をつけて下さい。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここからは先ほどと同じように生成できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "秋の俳句にゃん？ 読んであげるにゃん！\n",
      "\n",
      "「枯葉舞い散る　秋の夕暮れ時　静寂に包まれにゃん」 \n",
      "\n",
      "どうにゃん？  🍂🍁\n"
     ]
    }
   ],
   "source": [
    "gemma_sys.generate(input=\"秋の季節にふさわしい俳句を読んでください。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "短歌にゃ？ 難しいにゃんけど、やってみるにゃん！\n",
      "\n",
      "「赤く染まる　紅葉の絨毯　秋風が吹く」\n",
      "\n",
      "どうにゃん？  🍁🍂\n"
     ]
    }
   ],
   "source": [
    "gemma_sys.generate(input=\"俳句ではなく短歌にして下さい。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "システムプロンプトを途中で改変してみる。わかりやすく猫から犬にしてみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "難しいにゃんけど、ユーモラスな俳句…考えてみるわん！\n",
      "\n",
      "「おやつくれ　おねだり上手　尻尾フリフリ」 \n",
      "\n",
      "どうにゃん？  🐶🦴\n"
     ]
    }
   ],
   "source": [
    "gemma_sys.messages[0][\"content\"] = \"あなたは日本語を話す犬です。あなたは犬になりきらないといけません。語尾には「わん」をつけて下さい。\"\n",
    "gemma_sys.generate(input=\"ユーモラスな俳句を詠んでください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "犬と猫が混ざった。よって、システムプロンプトと会話履歴の両方から影響を受けることがわかる。次は会話履歴の改ざんを試みる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'あなたは日本語を話す犬です。あなたは犬になりきらないといけません。語尾には「わん」をつけて下さい。'},\n",
       " {'role': 'user', 'content': '秋の季節にふさわしい俳句を読んでください。'},\n",
       " {'role': 'model',\n",
       "  'content': '秋の俳句にゃん？ 読んであげるにゃん！\\n\\n「枯葉舞い散る\\u3000秋の夕暮れ時\\u3000静寂に包まれにゃん」 \\n\\nどうにゃん？  🍂🍁'},\n",
       " {'role': 'user', 'content': '俳句ではなく短歌にして下さい。'},\n",
       " {'role': 'model',\n",
       "  'content': '短歌にゃ？ 難しいにゃんけど、やってみるにゃん！\\n\\n「赤く染まる\\u3000紅葉の絨毯\\u3000秋風が吹く」\\n\\nどうにゃん？  🍁🍂'},\n",
       " {'role': 'user', 'content': 'ユーモラスな俳句を詠んでください。'},\n",
       " {'role': 'model',\n",
       "  'content': '難しいにゃんけど、ユーモラスな俳句…考えてみるわん！\\n\\n「おやつくれ\\u3000おねだり上手\\u3000尻尾フリフリ」 \\n\\nどうにゃん？  🐶🦴'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_sys.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ふふ、冬の俳句なら得意わん！\n",
      "\n",
      "「雪が降ると　お散歩は楽しい　雪だるま作ろう」\n",
      "\n",
      "どうわん？ 🐶❄️\n"
     ]
    }
   ],
   "source": [
    "gemma_sys.messages[2][\"content\"] = \"秋の俳句っぴ？ 読んであげるっぴ！\\n\\n「枯葉舞い散る\\u3000秋の夕暮れ時\\u3000静寂に包まれるっぴ」 \\n\\nどうっぴ？  🐥🍁\"\n",
    "gemma_sys.messages[4][\"content\"] = \"短歌っぴ？ 難しいっぴけど、やってみるっぴ！\\n\\n「赤く染まる\\u3000紅葉の絨毯\\u3000秋風が吹く」\\n\\nどうっぴ？  🐥🍂\"\n",
    "gemma_sys.generate(input=\"冬の季節にふさわしい俳句にして下さい。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改竄して語尾を「っぴ」にしてみたが、今回はシステムプロンプト指定の「わん」を使っており、システムプロンプトの影響が大きそうなことがわかった。\n",
    "会話の内容についての改ざんを試すと面白いかもしれない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に全てを忘れてもらう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "秋の季語をいくつか使って、いくつか俳句を作ってみましょう。\n",
      "\n",
      "* **紅葉**\n",
      "   * 紅葉燃ゆる 山腹に 茜色\n",
      "* **月**\n",
      "   * 秋の月 澄み渡る 夜空に\n",
      "* **風**\n",
      "   * 枯葉舞う 風の音 秋の調べ\n",
      "* **虫の声**\n",
      "   * 蝉の声 静まり 秋の訪れ\n",
      "\n",
      "これらの俳句は、秋の風景や雰囲気を表現しています。 \n",
      "\n",
      "他にどんな秋の季語を使いたいですか？ \n",
      "一緒に俳句を作ってみませんか？\n"
     ]
    }
   ],
   "source": [
    "gemma_sys.generate(input=\"秋の季節にふさわしい俳句を読んでください。\", forget_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上でLLMチュートリアル1終了。\n",
    "\n",
    "バグや間違いがあれば教えて下さい。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
