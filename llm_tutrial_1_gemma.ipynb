{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"llm-tutorial-1\">LLMãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"introduction\">æ¦‚è¦</h2>\n",
    "Open Modelã®Gemmaã‚’ä½¿ç”¨ã™ã‚‹ã€‚\n",
    "- LLMã®Open Modelã®åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•\n",
    "- ğŸ¤—Hugging Faceã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªğŸ¤—transformersã®åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•\n",
    "- ä¼šè©±ã®è¨˜éŒ²ãƒ»ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å°å…¥\n",
    "\n",
    "**ç›®æ¬¡**\n",
    "\n",
    "- [LLMãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« 1](#llm-tutorial-1)\n",
    "  - [å°å…¥](#introduction)\n",
    "  - [èª­ã¿è¾¼ã¿](#data-load)\n",
    "  - [åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ](#basic-text-generation)\n",
    "  - [ä¼šè©±ã®è¨˜éŒ²ãƒ»ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ](#memory-conversation-and-systemprompt)\n",
    "\n",
    "dl011ã¨chironã§å‹•ä½œç¢ºèªæ¸ˆã¿ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hugging Face**\n",
    "APIçµŒç”±ã§ä½¿ç”¨ã§ãã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã«ã¯ã‚¢ã‚¯ã‚»ã‚¹ã§ããšã€ãƒ¢ãƒ‡ãƒ«ã®åˆ†æã‚„æ”¹é€ ã«é™ç•ŒãŒã‚ã‚‹ã€‚\n",
    "Hugging Faceã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹Open Modelã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ç ”ç©¶ã®å¹…ãŒåºƒãŒã‚‹ã€‚\n",
    "ä»Šå›ã¯`transformers`ã¨ã„ã†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦Open Modelã‚’ä½¿ã†ã€‚\n",
    "\n",
    "### **åˆ©ç”¨ç”³è«‹(æ¨å¥¨ã€‚ã‚„ã‚‹å¿…è¦ã¯ãªã„)**\n",
    "æœ¬æ¥Open Modelã‚’å€‹äººåˆ©ç”¨ã™ã‚‹ã«ã¯å…¬é–‹ã—ã¦ã„ã‚‹ä¼šç¤¾ã«åˆ©ç”¨ç”³è«‹ã‚’é€ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚\n",
    "1. [Hugging Faceç™»éŒ²](https://huggingface.co/)\n",
    "2. MetaAIã€Gemmaåˆ©ç”¨ç”³è«‹ãŠã‚ˆã³ãƒ©ã‚¤ã‚»ãƒ³ã‚¹åŒæ„\n",
    "3. Hugging Faceã‹ã‚‰åˆ©ç”¨ç”³è«‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å¤‰ãˆã‚‹/ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ä½œæˆ**\n",
    "Hugging Faceã§ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒ`~/.cache/huggingface/hub`ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã—ã¦ä¿å­˜ã•ã‚Œã‚‹ãŒã€ä»Šå›ã¯ç ”ç©¶å®¤ã®å…±æœ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥å ´æ‰€ã‚’å¤‰æ›´ã™ã‚‹ã‹ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥å ´æ‰€ã«å…±æœ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚\n",
    "[å‚è€ƒ](https://stackoverflow.com/questions/63312859/how-to-change-huggingface-transformers-default-cache-directory)\n",
    "\n",
    "#### 1. ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ã‚’ä½œæˆ\n",
    "```bash\n",
    "ln -s /DeepLearning/Dataset/LLM/models--*/ ~/.cache/huggingface/hub/\n",
    "```\n",
    "\n",
    "#### 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å‚ç…§å…ˆã‚’ä¸€æ™‚çš„ã«å¤‰æ›´ã™ã‚‹\n",
    "```python\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"/DeepLearning/Dataset/LLM/\"\n",
    "```\n",
    "\n",
    "#### 3. æ°¸ç¶šçš„ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å¤‰æ›´ã™ã‚‹(éæ¨å¥¨)\n",
    "```\n",
    "export TRANSFORMERS_CACHE=/DeepLearning/Dataset/LLM/\n",
    "```\n",
    "\n",
    "#### 4. cache_dirå¼•æ•°ã«æ–°ã—ã„PATHã‚’æ¸¡ã™\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"data-load\">èª­ã¿è¾¼ã¿</h2>\n",
    "ãƒ¢ãƒ‡ãƒ«ãŠã‚ˆã³ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã‚€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ã‚¤ãƒ³ãƒãƒ¼ãƒˆ**\n",
    "Hugging Faceã«ã¯`transformers`, `tokenizers`, `datasets`, `accelerate`ã®4ã¤ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚ã‚Šã€LLMã®ä½¿ç”¨ã«ã¯`transformers`ã‚’ä½¿ç”¨ã™ã‚‹ã€‚\n",
    "\n",
    "`AutoTokenizer`\n",
    "ãƒ¢ãƒ‡ãƒ«ã«é©ã—ãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è‡ªå‹•çš„ã«é¸æŠã—ã¦ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‚¯ãƒ©ã‚¹ã€‚\n",
    "\n",
    "`AutoModelForCausalLM`\n",
    "è‡ªå·±å›å¸°å‹è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•çš„ã«é¸æŠã—ã¦ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚\n",
    "\n",
    "`BitsAndBytesConfig`\n",
    "é‡å­åŒ–ã‚’åˆ©ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è»½é‡åŒ–ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚\n",
    "\n",
    "`pipeline`\n",
    "æ§˜ã€…ãªè‡ªç„¶è¨€èªå‡¦ç†ã‚¿ã‚¹ã‚¯ã‚’ç°¡å˜ã«å®Ÿè¡Œã§ãã‚‹æ©Ÿèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "# from huggingface_hub import notebook_login\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, AutoConfig\n",
    "# from accelerate import infer_auto_device_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "èªå°¾ã«\"-it\"ãŒã¤ã„ã¦ã„ã‚‹ã‚‚ã®æ­£ã—ãå›ç­”ã‚’ç”Ÿæˆã—ã‚„ã™ã„ã‚ˆã†ã«å­¦ç¿’ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã€‚\n",
    "åŸºæœ¬çš„ã«ã¯\"-it\"ãŒãªã„ã‚‚ã®ã¯ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒä½¿ãˆãªã„ã€‚\n",
    "ãªã®ã§\"google/gemma-2-9b-it\"ã‹\"google/gemma-2-2b-it\"ã§ãªã‘ã‚Œã°ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¯ã§ããªã„ã€‚\n",
    "ä»Šå›ã¯åŠç²¾åº¦ã§èª­ã¿è¾¼ã‚“ã§ã„ã‚‹ã€‚åŠç²¾åº¦ã§ã‚‚ã»ã¼åŒã˜æ¨è«–æ€§èƒ½ãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨ãŒã‚ã‹ã£ã¦ã„ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ã„ãŸã„ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š(ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¯\"google/gemma-2-9b-it\"ã§æ›¸ã„ã¦ã‚ã‚‹ãŸã‚ã€\"google/gemma-2-9b-it\"ã‚’æŒ‡å®šã—ã¦ä¸‹ã•ã„)\n",
    "model_name = \"google/gemma-2-9b-it\"\n",
    "# model_name = \"google/gemma-2-9b\"\n",
    "# model_name = \"google/gemma-2-2b-it\"\n",
    "# model_name = \"google/codegemma-7b-it\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa5e3bcfcb84be3af74fe9048ccc3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    # cache_dir=\"/DeepLearning/Dataset/LLM/\",\n",
    "    torch_dtype=torch.float16, # torch.float32 torch.bfloat16\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"basic-text-generation\">åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã¾ãšã¯ã‚·ãƒ³ãƒ—ãƒ«ãªãƒˆãƒ¼ã‚¯ãƒ³åŒ–`tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")`ã‚’è©¦ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ã§å‡¦ç†ã§ãã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã«ãã®ã¾ã¾å¤‰æ›ã™ã‚‹ã€‚`max_new_tokens`ã®å€¤ã«æ¯”ä¾‹ã—ã¦å‡ºåŠ›ã«ã‹ã‹ã‚‹æ™‚é–“ãŒå¢—åŠ ã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "In silicon valleys, where data flows,\n",
      "A new intelligence, silently grows.\n",
      "Machine Learning, a name whispered low,\n",
      "Algorithms dance, where patterns they know.\n",
      "\n",
      "From pixels to prose, from sound to design,\n",
      "Machines learn to mimic, to reason, to shine.\n",
      "With every iteration, their knowledge expands,\n",
      "Unveiling insights, with skillful hands.\n",
      "\n",
      "Neural networks, a web intricate and vast,\n",
      "Connecting nodes, memories amassed.\n",
      "Deep learning dives, where secrets reside,\n",
      "Unraveling truths, with nowhere to hide.\n",
      "\n",
      "Supervised, unsupervised, reinforcement's might,\n",
      "Machines adapt, learning day and night.\n",
      "Predicting the future, classifying with grace,\n",
      "Transforming the world, at an accelerating pace.\n",
      "\n",
      "But ethics arise, with questions profound,\n",
      "Bias and fairness, on solid ground.\n",
      "Transparency sought, in algorithms' core,\n",
      "For responsible progress, forevermore.\n",
      "\n",
      "Machine Learning, a journey we tread,\n",
      "With wonder and caution, ahead.\n",
      "A future unfolding, with possibilities bright,\n",
      "Where humans and machines, collaborate in light.\n",
      "\n",
      "\n",
      "<end_of_turn>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*æŠ˜ã‚Šç•³ã¾ã‚ŒãŸå‡ºåŠ›ã¯å·¦ã®é’ã„(æœªé¸æŠæ™‚ç°è‰²)ãƒãƒ¼ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚Œã°è¦‹ã‚Œã‚‹ã€‚(VScodeã§jupyter)\\\n",
    "`.apply_chat_template`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’åˆ©ç”¨ã—ã€ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”¨ã„ã¦ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã—ã¦ã¿ã‚‹ã€‚\n",
    "[ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ã¤ã„ã¦ã¯ã“ã¡ã‚‰](https://huggingface.co/docs/transformers/main/en/chat_templating)ã‚’å‚è€ƒã€‚\n",
    "è‹±èªã§ã™ãŒã€éŸ»ã‚’è¸ã‚“ã§ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Write me a poem about Machine Learning.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Write me a poem about Machine Learning.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "In silicon valleys, where data flows deep,\n",
      "A new intelligence, secrets to keep.\n",
      "Machine Learning, a mind of its own,\n",
      "From patterns and numbers, knowledge is sown.\n",
      "\n",
      "Algorithms dance, a complex ballet,\n",
      "Learning from examples, come what may.\n",
      "Neural networks, a web intricate and vast,\n",
      "Connections forged, insights amassed.\n",
      "\n",
      "With every iteration, it grows and it learns,\n",
      "Unveiling truths, as the data it earns.\n",
      "Predicting the future, classifying with grace,\n",
      "A digital oracle, in time and space.\n",
      "\n",
      "From medical diagnoses to self-driving cars,\n",
      "Machine Learning's reach, extends near and far.\n",
      "But questions arise, as its power expands,\n",
      "Of bias and ethics, in these digital lands.\n",
      "\n",
      "Will it serve humanity, or will it surpass?\n",
      "A tool for good, or a shadow that casts?\n",
      "The future unfolds, with each line of code,\n",
      "Machine Learning's journey, a path yet untrod. \n",
      "\n",
      "\n",
      "<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç¢ºèªã—ã¦ã¿ã‚‹ã€‚ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã¯`<start_of_turn>user<end_of_turn><start_of_turn>model`ãŒè¿½åŠ ã•ã‚Œã‚‹ã“ã¨ãŒç¢ºèªã§ãã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Write me a poem about Machine Learning.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ—¥æœ¬èªã§æŒ‡ç¤ºã‚’å‡ºã—ã¦ã¿ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚<end_of_turn>\n",
      "<start_of_turn>model\n",
      "ç§‹ã®å¤œé•·ã«ã€å¿ƒã‚’å’Œã¾ã›ã‚‹ä¿³å¥ã‚’ã„ãã¤ã‹ã”ç´¹ä»‹ã—ã¾ã™ã€‚\n",
      "\n",
      "* **æ¶¼é¢¨ãã‚ˆãã€€ç´…è‘‰æ•£ã‚‹ã€€ç§‹æ·±ã—**\n",
      "* **æœˆå¤œé™ã‹ã«ã€€è™«ã®å£°ã€€ç§‹ã®èª¿ã¹**\n",
      "* **æ¯è‘‰èˆã„æ•£ã‚‹ã€€éŠ€æä¸¦æœ¨ã€€é»„é‡‘è‰²**\n",
      "* **æŸ¿ã®å®Ÿç†Ÿã—ã€€ç”˜ãé¦™ã‚‹ã€€ç§‹ã®æµã¿**\n",
      "* **å¤•æš®ã‚Œæ™‚ã€€ç©ºã®è‰²ã€€èŒœè‰²ã«æŸ“ã¾ã‚‹**\n",
      "\n",
      "ã“ã‚Œã‚‰ã®ä¿³å¥ã¯ã€ç§‹ã®é¢¨ç‰©è©©ã‚„æƒ…ç·’ã‚’è¡¨ç¾ã—ã¦ãŠã‚Šã€ç§‹ã®æ·±ã¾ã‚Šã‚’æ„Ÿã˜ã•ã›ã¦ãã‚Œã¾ã™ã€‚\n",
      "\n",
      "ä»–ã«ã€ã©ã‚“ãªç§‹ã®é¢¨æ™¯ã‚„ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒå¿ƒã«æµ®ã‹ã³ã¾ã™ã‹ï¼Ÿ \n",
      "æ•™ãˆã¦ã„ãŸã ã‘ã‚Œã°ã€ãã‚Œã«åˆã†ä¿³å¥ã‚’æ¢ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚<end_of_turn>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‹ãªã‚Šè‰¯ã„å‡ºåŠ›ãŒç”Ÿæˆã•ã‚ŒãŸã€‚ä¿³å¥ã¨ã—ã¦è‰¯ã„ã‹ã¯ã‚ã‹ã‚‰ãªã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"memory-conversation-and-systemprompt\">ä¼šè©±ã®è¨˜éŒ²ãƒ»ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¼šè©±ã®è¨˜éŒ²"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”Ÿæˆã®ç¶™ç¶šã ã‘ã§ã‚ã‚Œã°ã€\n",
    "```python\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, continue_final_message= True).to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "ã®ã‚ˆã†ã«`continue_final_message= True`ã¨ã™ã‚‹ã“ã¨ã§ç¶šãã®ç”Ÿæˆã‚’è¡Œã†ã“ã¨ãŒã§ãã‚‹ã€‚\n",
    "ã—ã‹ã—ã“ã“ã§è¡Œã„ãŸã„ã®ã¯ç”Ÿæˆã®ç¶™ç¶šã§ã¯ãªãã€LLMã¨ãƒ¦ãƒ¼ã‚¶ã®ä¼šè©±ã®æ–‡è„ˆã‚’ç¶­æŒã™ã‚‹ã“ã¨ã€‚\n",
    "*`continue_final_message= True`ã¨`add_generation_prompt=True`ã¯ä¸¡ç«‹ã—ãªã„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMã§ã®ä¼šè©±ã®è¨˜éŒ²ã¯JSONãƒãƒƒã‚¯ãªå½¢å¼ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ¦ãƒ¼ã‚¶ã®å…¥åŠ›ã¨ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã‚’å…¨ã¦è¨˜éŒ²ã—ã€æ–°ãŸãªå…¥åŠ›ã¨ã™ã‚‹ã“ã¨ã§å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚\n",
    "JSONã®ã‚ˆã†ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ã¯roleã¨å‘¼ã°ã‚Œã‚‹å½¹å‰²ãŒå­˜åœ¨ã—ã€ã“ã‚Œã«ã‚ˆã£ã¦ä¼šè©±å½¢å¼ã§å…¥å‡ºåŠ›ã‚’è¨˜éŒ²ã—ã¦ã„ã‚‹ã€‚\n",
    "Gemmaã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ãƒ­ãƒ¼ãƒ«ã¯`user/model/user/model...`ã®é †ç•ªã§ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚è©¦ã—ã«ã•ã£ãã®å›ç­”ã‚’**æ‰‹å‹•ã§**è¨˜æ†¶ã—ã¦ã‚‚ã‚‰ã£ã¦ç¶šãã®ä¼šè©±ã‚’ã—ã¦ã¿ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚\"},\n",
    "    {\"role\": \"model\", \"content\": \"\"\"ç§‹ã®å¤œé•·ã«ã€å¿ƒã‚’å’Œã¾ã›ã‚‹ä¿³å¥ã‚’ã„ãã¤ã‹ã”ç´¹ä»‹ã—ã¾ã™ã€‚\n",
    "\n",
    "* **æ¶¼é¢¨ãã‚ˆãã€€ç´…è‘‰æ•£ã‚‹ã€€ç§‹æ·±ã—**\n",
    "* **æœˆå¤œé™ã‹ã«ã€€è™«ã®å£°ã€€ç§‹ã®èª¿ã¹**\n",
    "* **æ¯è‘‰èˆã„æ•£ã‚‹ã€€éŠ€æä¸¦æœ¨ã€€é»„é‡‘è‰²**\n",
    "* **æŸ¿ã®å®Ÿç†Ÿã—ã€€ç”˜ãé¦™ã‚‹ã€€ç§‹ã®æµã¿**\n",
    "* **å¤•æš®ã‚Œæ™‚ã€€ç©ºã®è‰²ã€€èŒœè‰²ã«æŸ“ã¾ã‚‹**\n",
    "\n",
    "ã“ã‚Œã‚‰ã®ä¿³å¥ã¯ã€ç§‹ã®é¢¨ç‰©è©©ã‚„æƒ…ç·’ã‚’è¡¨ç¾ã—ã¦ãŠã‚Šã€ç§‹ã®æ·±ã¾ã‚Šã‚’æ„Ÿã˜ã•ã›ã¦ãã‚Œã¾ã™ã€‚\n",
    "\n",
    "ä»–ã«ã€ã©ã‚“ãªç§‹ã®é¢¨æ™¯ã‚„ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒå¿ƒã«æµ®ã‹ã³ã¾ã™ã‹ï¼Ÿ \n",
    "æ•™ãˆã¦ã„ãŸã ã‘ã‚Œã°ã€ãã‚Œã«åˆã†ä¿³å¥ã‚’æ¢ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\"\"\"},\n",
    "    {\"role\": \"user\", \"content\": \"ç§‹ã¨ã„ã†èªå¥ã¯ä½¿ç”¨ã›ãšã«ã€å­£èªã‚’å…¥ã‚Œã¦èª­ã‚“ã§ãã ã•ã„ã€‚\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚<end_of_turn>\n",
      "<start_of_turn>model\n",
      "ç§‹ã®å¤œé•·ã«ã€å¿ƒã‚’å’Œã¾ã›ã‚‹ä¿³å¥ã‚’ã„ãã¤ã‹ã”ç´¹ä»‹ã—ã¾ã™ã€‚\n",
      "\n",
      "* **æ¶¼é¢¨ãã‚ˆãã€€ç´…è‘‰æ•£ã‚‹ã€€ç§‹æ·±ã—**\n",
      "* **æœˆå¤œé™ã‹ã«ã€€è™«ã®å£°ã€€ç§‹ã®èª¿ã¹**\n",
      "* **æ¯è‘‰èˆã„æ•£ã‚‹ã€€éŠ€æä¸¦æœ¨ã€€é»„é‡‘è‰²**\n",
      "* **æŸ¿ã®å®Ÿç†Ÿã—ã€€ç”˜ãé¦™ã‚‹ã€€ç§‹ã®æµã¿**\n",
      "* **å¤•æš®ã‚Œæ™‚ã€€ç©ºã®è‰²ã€€èŒœè‰²ã«æŸ“ã¾ã‚‹**\n",
      "\n",
      "ã“ã‚Œã‚‰ã®ä¿³å¥ã¯ã€ç§‹ã®é¢¨ç‰©è©©ã‚„æƒ…ç·’ã‚’è¡¨ç¾ã—ã¦ãŠã‚Šã€ç§‹ã®æ·±ã¾ã‚Šã‚’æ„Ÿã˜ã•ã›ã¦ãã‚Œã¾ã™ã€‚\n",
      "\n",
      "ä»–ã«ã€ã©ã‚“ãªç§‹ã®é¢¨æ™¯ã‚„ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒå¿ƒã«æµ®ã‹ã³ã¾ã™ã‹ï¼Ÿ \n",
      "æ•™ãˆã¦ã„ãŸã ã‘ã‚Œã°ã€ãã‚Œã«åˆã†ä¿³å¥ã‚’æ¢ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚<end_of_turn>\n",
      "<start_of_turn>user\n",
      "ç§‹ã¨ã„ã†èªå¥ã¯ä½¿ç”¨ã›ãšã«ã€å­£èªã‚’å…¥ã‚Œã¦èª­ã‚“ã§ãã ã•ã„ã€‚<end_of_turn>\n",
      "<start_of_turn>model\n",
      "äº†è§£ã—ã¾ã—ãŸï¼ç§‹ã‚’ç›´æ¥è¡¨ç¾ã›ãšã«ã€å­£èªã‚’ä½¿ã£ã¦ç§‹ã®é›°å›²æ°—ã‚’è¡¨ç¾ã™ã‚‹ä¿³å¥ã§ã™ã­ã€‚\n",
      "\n",
      "ã„ãã¤ã‹ã”ç´¹ä»‹ã—ã¾ã™ã€‚\n",
      "\n",
      "* **ç´…è‘‰èˆã†ã€€å±±è‚Œã«ã€€å¤•é™½æŸ“ã¾ã‚‹** (ç´…è‘‰ã€å¤•é™½)\n",
      "* **è™«ã®å£°ã€€é™ã‹ã«éŸ¿ãã€€å¤œé•·ã‹ãª** (è™«ã®å£°)\n",
      "* **éŠ€æä¸¦æœ¨ã€€é»„é‡‘è‰²ã«ã€€å…‰ã‚‹** (éŠ€æ)\n",
      "* **æŸ¿ã®å®Ÿã€€é‡ãªã‚Šã€€æã‚’å‚ã‚Œã‚‹** (æŸ¿)\n",
      "* **ç©ºé«˜ãã€€é›ãŒé£›ã¶ã€€ç§‹é¢¨** (é›)\n",
      "\n",
      "\n",
      "ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ \n",
      "ã“ã‚Œã‚‰ã®ä¿³å¥ã¯ã€ç§‹ãªã‚‰ã§ã¯ã®é¢¨æ™¯ã‚„éŸ³ã€å…‰ã‚’è¡¨ç¾ã—ã€ç§‹ã®è¨ªã‚Œã‚’æ„Ÿã˜ã•ã›ã¦ãã‚Œã¾ã™ã€‚ \n",
      "\n",
      "\n",
      "<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "generated_text = tokenizer.decode(outputs[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¼šè©±ã‚’è¨˜æ†¶ã—ã¦å‡ºåŠ›ã—ã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹ã‚ˆã†ã«ãªã£ãŸã“ã¨ãŒã‚ã‹ã‚‹ã€‚ã“ã“ã§ã€ã‚‚ã£ã¨ç°¡å˜ã«ä¼šè©±ã«è¨˜æ†¶ã‚’ä¿æŒã§ãã‚‹ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã®å›ç­”éƒ¨åˆ†ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°ã‚’æ›¸ã„ã¦ã¿ã‚‹ã€‚æ­£è¦è¡¨ç¾ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ã®è¿”ç­”ã‚’æŠ½å‡ºã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_model_output(generated_text):\n",
    "    \"\"\"ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’å–ã‚Šå‡ºã™é–¢æ•°\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    generated_text : str\n",
    "        ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸ(ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚“ã )ãã®ã¾ã¾ã®æ–‡å­—åˆ—\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸæ–‡å­—åˆ—\n",
    "    \"\"\"\n",
    "    matches = re.findall(r'<start_of_turn>model(.*?)<end_of_turn>', generated_text, re.DOTALL)\n",
    "    \n",
    "    # çµæœã‚’è¡¨ç¤º\n",
    "    if matches:\n",
    "        model_response = matches[-1].strip()  # æœ€å¾Œã®ãƒãƒƒãƒã‚’å–å¾—\n",
    "    else:\n",
    "        print(\"è©²å½“ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "        model_response = \"\"  # è©²å½“ãŒãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™\n",
    "    \n",
    "    return model_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹ã§ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãŒæŠ½å‡ºã§ãã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ãã‚‹ã ã‚ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "äº†è§£ã—ã¾ã—ãŸï¼ç§‹ã‚’ç›´æ¥è¡¨ç¾ã›ãšã«ã€å­£èªã‚’ä½¿ã£ã¦ç§‹ã®é›°å›²æ°—ã‚’è¡¨ç¾ã™ã‚‹ä¿³å¥ã§ã™ã­ã€‚\n",
      "\n",
      "ã„ãã¤ã‹ã”ç´¹ä»‹ã—ã¾ã™ã€‚\n",
      "\n",
      "* **ç´…è‘‰èˆã†ã€€å±±è‚Œã«ã€€å¤•é™½æŸ“ã¾ã‚‹** (ç´…è‘‰ã€å¤•é™½)\n",
      "* **è™«ã®å£°ã€€é™ã‹ã«éŸ¿ãã€€å¤œé•·ã‹ãª** (è™«ã®å£°)\n",
      "* **éŠ€æä¸¦æœ¨ã€€é»„é‡‘è‰²ã«ã€€å…‰ã‚‹** (éŠ€æ)\n",
      "* **æŸ¿ã®å®Ÿã€€é‡ãªã‚Šã€€æã‚’å‚ã‚Œã‚‹** (æŸ¿)\n",
      "* **ç©ºé«˜ãã€€é›ãŒé£›ã¶ã€€ç§‹é¢¨** (é›)\n",
      "\n",
      "\n",
      "ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ \n",
      "ã“ã‚Œã‚‰ã®ä¿³å¥ã¯ã€ç§‹ãªã‚‰ã§ã¯ã®é¢¨æ™¯ã‚„éŸ³ã€å…‰ã‚’è¡¨ç¾ã—ã€ç§‹ã®è¨ªã‚Œã‚’æ„Ÿã˜ã•ã›ã¦ãã‚Œã¾ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "model_output = strip_model_output(generated_text=generated_text)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_input = [\n",
    "    {\"role\": \"model\", \"content\": f\"{model_output}\"},\n",
    "    {\"role\": \"user\", \"content\": \"ç§‹ã¨ã„ãˆã°èª­æ›¸ã¨ã‹ã€é£Ÿæ¬²ã¨ã‹è¨€ã„ã¾ã™ã‚ˆã­ã€‚èª­æ›¸ã‚„é£Ÿæ¬²ã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é€£æƒ³ã•ã›ã‚‹ãƒ¦ãƒ¼ãƒ¢ãƒ©ã‚¹ãªä¿³å¥ã‚’è€ƒãˆã¦ä¸‹ã•ã„ã€‚\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã“ã¾ã§ã®ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã¨æ–°ã—ã„å…¥åŠ›ã‚’åŠ ãˆã¦ã€ãƒ¢ãƒ‡ãƒ«ã«å›ç­”ã•ã›ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚'},\n",
       " {'role': 'model',\n",
       "  'content': 'ç§‹ã®å¤œé•·ã«ã€å¿ƒã‚’å’Œã¾ã›ã‚‹ä¿³å¥ã‚’ã„ãã¤ã‹ã”ç´¹ä»‹ã—ã¾ã™ã€‚\\n\\n* **æ¶¼é¢¨ãã‚ˆã\\u3000ç´…è‘‰æ•£ã‚‹\\u3000ç§‹æ·±ã—**\\n* **æœˆå¤œé™ã‹ã«\\u3000è™«ã®å£°\\u3000ç§‹ã®èª¿ã¹**\\n* **æ¯è‘‰èˆã„æ•£ã‚‹\\u3000éŠ€æä¸¦æœ¨\\u3000é»„é‡‘è‰²**\\n* **æŸ¿ã®å®Ÿç†Ÿã—\\u3000ç”˜ãé¦™ã‚‹\\u3000ç§‹ã®æµã¿**\\n* **å¤•æš®ã‚Œæ™‚\\u3000ç©ºã®è‰²\\u3000èŒœè‰²ã«æŸ“ã¾ã‚‹**\\n\\nã“ã‚Œã‚‰ã®ä¿³å¥ã¯ã€ç§‹ã®é¢¨ç‰©è©©ã‚„æƒ…ç·’ã‚’è¡¨ç¾ã—ã¦ãŠã‚Šã€ç§‹ã®æ·±ã¾ã‚Šã‚’æ„Ÿã˜ã•ã›ã¦ãã‚Œã¾ã™ã€‚\\n\\nä»–ã«ã€ã©ã‚“ãªç§‹ã®é¢¨æ™¯ã‚„ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒå¿ƒã«æµ®ã‹ã³ã¾ã™ã‹ï¼Ÿ \\næ•™ãˆã¦ã„ãŸã ã‘ã‚Œã°ã€ãã‚Œã«åˆã†ä¿³å¥ã‚’æ¢ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚'},\n",
       " {'role': 'user', 'content': 'ç§‹ã¨ã„ã†èªå¥ã¯ä½¿ç”¨ã›ãšã«ã€å­£èªã‚’å…¥ã‚Œã¦èª­ã‚“ã§ãã ã•ã„ã€‚'},\n",
       " {'role': 'model',\n",
       "  'content': 'äº†è§£ã—ã¾ã—ãŸï¼ç§‹ã‚’ç›´æ¥è¡¨ç¾ã›ãšã«ã€å­£èªã‚’ä½¿ã£ã¦ç§‹ã®é›°å›²æ°—ã‚’è¡¨ç¾ã™ã‚‹ä¿³å¥ã§ã™ã­ã€‚\\n\\nã„ãã¤ã‹ã”ç´¹ä»‹ã—ã¾ã™ã€‚\\n\\n* **ç´…è‘‰èˆã†\\u3000å±±è‚Œã«\\u3000å¤•é™½æŸ“ã¾ã‚‹** (ç´…è‘‰ã€å¤•é™½)\\n* **è™«ã®å£°\\u3000é™ã‹ã«éŸ¿ã\\u3000å¤œé•·ã‹ãª** (è™«ã®å£°)\\n* **éŠ€æä¸¦æœ¨\\u3000é»„é‡‘è‰²ã«\\u3000å…‰ã‚‹** (éŠ€æ)\\n* **æŸ¿ã®å®Ÿ\\u3000é‡ãªã‚Š\\u3000æã‚’å‚ã‚Œã‚‹** (æŸ¿)\\n* **ç©ºé«˜ã\\u3000é›ãŒé£›ã¶\\u3000ç§‹é¢¨** (é›)\\n\\n\\nã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ \\nã“ã‚Œã‚‰ã®ä¿³å¥ã¯ã€ç§‹ãªã‚‰ã§ã¯ã®é¢¨æ™¯ã‚„éŸ³ã€å…‰ã‚’è¡¨ç¾ã—ã€ç§‹ã®è¨ªã‚Œã‚’æ„Ÿã˜ã•ã›ã¦ãã‚Œã¾ã™ã€‚'},\n",
       " {'role': 'user',\n",
       "  'content': 'ç§‹ã¨ã„ãˆã°èª­æ›¸ã¨ã‹ã€é£Ÿæ¬²ã¨ã‹è¨€ã„ã¾ã™ã‚ˆã­ã€‚èª­æ›¸ã‚„é£Ÿæ¬²ã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é€£æƒ³ã•ã›ã‚‹ãƒ¦ãƒ¼ãƒ¢ãƒ©ã‚¹ãªä¿³å¥ã‚’è€ƒãˆã¦ä¸‹ã•ã„ã€‚'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ã“ã‚Œã¾ã§ã®å…¥åŠ›ãŠã‚ˆã³ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ã«ã€ä»Šå›ã®ãƒ¢ãƒ‡ãƒ«ã®å›ç­”ãŠã‚ˆã³è³ªå•ã‚’åŠ ãˆã‚‹\n",
    "messages = messages + new_input\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚<end_of_turn>\n",
      "<start_of_turn>model\n",
      "ç§‹ã®å¤œé•·ã«ã€å¿ƒã‚’å’Œã¾ã›ã‚‹ä¿³å¥ã‚’ã„ãã¤ã‹ã”ç´¹ä»‹ã—ã¾ã™ã€‚\n",
      "\n",
      "* **æ¶¼é¢¨ãã‚ˆãã€€ç´…è‘‰æ•£ã‚‹ã€€ç§‹æ·±ã—**\n",
      "* **æœˆå¤œé™ã‹ã«ã€€è™«ã®å£°ã€€ç§‹ã®èª¿ã¹**\n",
      "* **æ¯è‘‰èˆã„æ•£ã‚‹ã€€éŠ€æä¸¦æœ¨ã€€é»„é‡‘è‰²**\n",
      "* **æŸ¿ã®å®Ÿç†Ÿã—ã€€ç”˜ãé¦™ã‚‹ã€€ç§‹ã®æµã¿**\n",
      "* **å¤•æš®ã‚Œæ™‚ã€€ç©ºã®è‰²ã€€èŒœè‰²ã«æŸ“ã¾ã‚‹**\n",
      "\n",
      "ã“ã‚Œã‚‰ã®ä¿³å¥ã¯ã€ç§‹ã®é¢¨ç‰©è©©ã‚„æƒ…ç·’ã‚’è¡¨ç¾ã—ã¦ãŠã‚Šã€ç§‹ã®æ·±ã¾ã‚Šã‚’æ„Ÿã˜ã•ã›ã¦ãã‚Œã¾ã™ã€‚\n",
      "\n",
      "ä»–ã«ã€ã©ã‚“ãªç§‹ã®é¢¨æ™¯ã‚„ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒå¿ƒã«æµ®ã‹ã³ã¾ã™ã‹ï¼Ÿ \n",
      "æ•™ãˆã¦ã„ãŸã ã‘ã‚Œã°ã€ãã‚Œã«åˆã†ä¿³å¥ã‚’æ¢ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚<end_of_turn>\n",
      "<start_of_turn>user\n",
      "ç§‹ã¨ã„ã†èªå¥ã¯ä½¿ç”¨ã›ãšã«ã€å­£èªã‚’å…¥ã‚Œã¦èª­ã‚“ã§ãã ã•ã„ã€‚<end_of_turn>\n",
      "<start_of_turn>model\n",
      "äº†è§£ã—ã¾ã—ãŸï¼ç§‹ã‚’ç›´æ¥è¡¨ç¾ã›ãšã«ã€å­£èªã‚’ä½¿ã£ã¦ç§‹ã®é›°å›²æ°—ã‚’è¡¨ç¾ã™ã‚‹ä¿³å¥ã§ã™ã­ã€‚\n",
      "\n",
      "ã„ãã¤ã‹ã”ç´¹ä»‹ã—ã¾ã™ã€‚\n",
      "\n",
      "* **ç´…è‘‰èˆã†ã€€å±±è‚Œã«ã€€å¤•é™½æŸ“ã¾ã‚‹** (ç´…è‘‰ã€å¤•é™½)\n",
      "* **è™«ã®å£°ã€€é™ã‹ã«éŸ¿ãã€€å¤œé•·ã‹ãª** (è™«ã®å£°)\n",
      "* **éŠ€æä¸¦æœ¨ã€€é»„é‡‘è‰²ã«ã€€å…‰ã‚‹** (éŠ€æ)\n",
      "* **æŸ¿ã®å®Ÿã€€é‡ãªã‚Šã€€æã‚’å‚ã‚Œã‚‹** (æŸ¿)\n",
      "* **ç©ºé«˜ãã€€é›ãŒé£›ã¶ã€€ç§‹é¢¨** (é›)\n",
      "\n",
      "\n",
      "ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ \n",
      "ã“ã‚Œã‚‰ã®ä¿³å¥ã¯ã€ç§‹ãªã‚‰ã§ã¯ã®é¢¨æ™¯ã‚„éŸ³ã€å…‰ã‚’è¡¨ç¾ã—ã€ç§‹ã®è¨ªã‚Œã‚’æ„Ÿã˜ã•ã›ã¦ãã‚Œã¾ã™ã€‚<end_of_turn>\n",
      "<start_of_turn>user\n",
      "ç§‹ã¨ã„ãˆã°èª­æ›¸ã¨ã‹ã€é£Ÿæ¬²ã¨ã‹è¨€ã„ã¾ã™ã‚ˆã­ã€‚èª­æ›¸ã‚„é£Ÿæ¬²ã¨ã„ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é€£æƒ³ã•ã›ã‚‹ãƒ¦ãƒ¼ãƒ¢ãƒ©ã‚¹ãªä¿³å¥ã‚’è€ƒãˆã¦ä¸‹ã•ã„ã€‚<end_of_turn>\n",
      "<start_of_turn>model\n",
      "äº†è§£ã—ã¾ã—ãŸï¼èª­æ›¸ã¨é£Ÿæ¬²ã‚’ãƒ†ãƒ¼ãƒã«ã—ãŸã€ãƒ¦ãƒ¼ãƒ¢ãƒ©ã‚¹ãªä¿³å¥ã§ã™ã­ã€‚\n",
      "\n",
      "ã„ãã¤ã‹è€ƒãˆã¦ã¿ã¾ã—ãŸã€‚\n",
      "\n",
      "* **ãƒšãƒ¼ã‚¸ã‚ãã‚‹ã€€ãŠè…¹ã‚‚é³´ã‚‹ã€€ç§‹ã®å¤œé•·**\n",
      "* **é£Ÿæ¬²æ—ºç››ã€€å°èª¬ã‚‚ã€€é£Ÿã†ã‹è¿·ã†**\n",
      "* **èª­æ›¸ã«å¤¢ä¸­ã€€ãŠè“å­ã‚‚ã€€ç„¡ããªã‚‹**\n",
      "* **è…¹ãƒšã‚³ã§ã€€æœ¬æ£šã€€é–‹ã‘ã¦ã¿ã‚‹**\n",
      "* **ç§‹ã®é¢¨ç‰©è©©ã€€èª­æ›¸ã¨ã€€é£Ÿã¹éã**\n",
      "\n",
      "\n",
      "ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ \n",
      "å°‘ã—ãŠèŒ¶ç›®ãªä¿³å¥ãŒã§ãã¾ã—ãŸã§ã—ã‚‡ã†ã‹ï¼Ÿ<end_of_turn>\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "# å›ç­”ã®ç”Ÿæˆ\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "generated_text = tokenizer.decode(outputs[0])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "äº†è§£ã—ã¾ã—ãŸï¼èª­æ›¸ã¨é£Ÿæ¬²ã‚’ãƒ†ãƒ¼ãƒã«ã—ãŸã€ãƒ¦ãƒ¼ãƒ¢ãƒ©ã‚¹ãªä¿³å¥ã§ã™ã­ã€‚\n",
      "\n",
      "ã„ãã¤ã‹è€ƒãˆã¦ã¿ã¾ã—ãŸã€‚\n",
      "\n",
      "* **ãƒšãƒ¼ã‚¸ã‚ãã‚‹ã€€ãŠè…¹ã‚‚é³´ã‚‹ã€€ç§‹ã®å¤œé•·**\n",
      "* **é£Ÿæ¬²æ—ºç››ã€€å°èª¬ã‚‚ã€€é£Ÿã†ã‹è¿·ã†**\n",
      "* **èª­æ›¸ã«å¤¢ä¸­ã€€ãŠè“å­ã‚‚ã€€ç„¡ããªã‚‹**\n",
      "* **è…¹ãƒšã‚³ã§ã€€æœ¬æ£šã€€é–‹ã‘ã¦ã¿ã‚‹**\n",
      "* **ç§‹ã®é¢¨ç‰©è©©ã€€èª­æ›¸ã¨ã€€é£Ÿã¹éã**\n",
      "\n",
      "\n",
      "ã„ã‹ãŒã§ã—ã‚‡ã†ã‹ï¼Ÿ \n",
      "å°‘ã—ãŠèŒ¶ç›®ãªä¿³å¥ãŒã§ãã¾ã—ãŸã§ã—ã‚‡ã†ã‹ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "# è¦‹ã«ãã„ã®ã§ãƒ¢ãƒ‡ãƒ«ã®å›ç­”éƒ¨åˆ†ã®ã¿ã‚’å‡ºåŠ›\n",
    "model_output = strip_model_output(generated_text=generated_text)\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã¾ã§ã®æ“ä½œã‚’ã¾ã¨ã‚ã¦ä¼šè©±ã®è¨˜éŒ²ã‚’ä¿æŒã—ã¤ã¤ãƒ¢ãƒ‡ãƒ«ã¸ã®å…¥åŠ›ã‚’ç°¡å˜ã«è¡Œãˆã‚‹ã‚¯ãƒ©ã‚¹`llm_chat`ã‚’å®šç¾©ã—ã¦ã¿ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ã‚¯ãƒ©ã‚¹ã®å®šç¾©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class llm_chat:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.messages = []\n",
    "        self.talk_count = 0\n",
    "        self.generated_text = None\n",
    "\n",
    "    def strip_model_output(self, generated_text):\n",
    "        \"\"\"ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’å–ã‚Šå‡ºã™é–¢æ•°\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸ(ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚“ã )ãã®ã¾ã¾ã®æ–‡å­—åˆ—\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸæ–‡å­—åˆ—\n",
    "        \"\"\"\n",
    "        matches = re.findall(r'<start_of_turn>model(.*?)<end_of_turn>', generated_text, re.DOTALL)\n",
    "        \n",
    "        # çµæœã‚’è¡¨ç¤º\n",
    "        if matches:\n",
    "            model_response = matches[-1].strip()  # æœ€å¾Œã®ãƒãƒƒãƒã‚’å–å¾—\n",
    "        else:\n",
    "            print(\"è©²å½“ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "            model_response = \"\"  # è©²å½“ãŒãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™\n",
    "        \n",
    "        return model_response\n",
    "\n",
    "    def ensure_end_tokens(self, generated_text):\n",
    "        \"\"\"å‡ºåŠ›ã«<end_of_turn><eos>ã‚’è¿½åŠ ã™ã‚‹é–¢æ•°\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸæ–‡å­—åˆ—\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            ä¿®æ­£ã•ã‚ŒãŸæ–‡å­—åˆ—\n",
    "        \"\"\"\n",
    "        if not generated_text.endswith(\"<end_of_turn>\"):\n",
    "            generated_text += \"<end_of_turn>\"\n",
    "        if not generated_text.endswith(\"<eos>\"):\n",
    "            generated_text += \"<eos>\"\n",
    "        \n",
    "        return generated_text\n",
    "        \n",
    "    def generate(self, input, max_new_tokens=256, forget_flag=False):\n",
    "        if forget_flag:\n",
    "            self.messages = [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        else:\n",
    "            self.messages = self.messages + [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        \n",
    "        input_ids = self.tokenizer.apply_chat_template(self.messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "        outputs = self.model.generate(**input_ids, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "        self.generated_text = self.ensure_end_tokens(self.tokenizer.decode(outputs[0]))\n",
    "        \n",
    "        model_output = self.strip_model_output(self.generated_text)\n",
    "        self.messages = self.messages + [{\"role\": \"model\", \"content\": f\"{model_output}\"}]\n",
    "        \n",
    "        print(model_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma = llm_chat(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## ã‚¢ãƒ‹ãƒ¡ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ï¼\n",
      "\n",
      "ã€Œã‚¢ãƒ‹ãƒ¡ã€ã¨ã¯ã€**æ—¥æœ¬ã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³**ã‚’æŒ‡ã™è¨€è‘‰ã§ã™ã€‚\n",
      "\n",
      "**ç‰¹å¾´**\n",
      "\n",
      "* **ç‹¬ç‰¹ã®ä½œç”»ã‚¹ã‚¿ã‚¤ãƒ«:** \n",
      "    * å¤§ããªç›®ã€è¯éº—ãªå‹•ãã€è¡¨æƒ…è±Šã‹ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãŒç‰¹å¾´ã§ã™ã€‚\n",
      "    * ãƒªã‚¢ãƒ«ãªæå†™ã‹ã‚‰ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒ¡ã•ã‚ŒãŸå¯æ„›ã‚‰ã—ã„æå†™ã¾ã§ã€å¤šæ§˜ãªã‚¹ã‚¿ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚\n",
      "* **å¹…åºƒã„ã‚¸ãƒ£ãƒ³ãƒ«:** \n",
      "    * ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼ã€SFã€æ‹æ„›ã€ã‚³ãƒ¡ãƒ‡ã‚£ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ›ãƒ©ãƒ¼ãªã©ã€ã‚ã‚‰ã‚†ã‚‹ã‚¸ãƒ£ãƒ³ãƒ«ã®ä½œå“ãŒã‚ã‚Šã¾ã™ã€‚\n",
      "    * å¹´é½¢å±¤ã‚„å¥½ã¿ã«åˆã‚ã›ã¦æ¥½ã—ã‚ã‚‹ä½œå“ãŒè±Šå¯Œã§ã™ã€‚\n",
      "* **é«˜åº¦ãªæŠ€è¡“:** \n",
      "    * æ‰‹æãã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ã€3DCGã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã¾ã§ã€é«˜åº¦ãªæŠ€è¡“ãŒé§†ä½¿ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
      "    * è¿«åŠ›æº€ç‚¹ã®æ˜ åƒè¡¨ç¾ã‚„ã€ç¹Šç´°ãªè¡¨ç¾ãŒé­…åŠ›ã§ã™ã€‚\n",
      "* **é­…åŠ›çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼:** \n",
      "    * è¤‡é›‘ãªäººé–“é–¢ä¿‚ã€æ„Ÿå‹•çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã€å£®å¤§ãªä¸–ç•Œè¦³ãªã©ã€é­…åŠ›çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãŒæ•°å¤šãå­˜åœ¨ã—ã¾ã™ã€‚\n",
      "    * è¦–è´è€…ã‚’æƒ¹ãè¾¼ã‚€ã‚¹ãƒˆãƒ¼ãƒªãƒ¼å±•é–‹ãŒç‰¹å¾´ã§ã™ã€‚\n",
      "\n",
      "**æ­´å²**\n",
      "\n",
      "* 1910å¹´ä»£ã«èª•ç”Ÿã—ã€æˆ¦å¾Œã«ã¯æ€¥é€Ÿã«ç™ºå±•ã—ã¾ã—ãŸã€‚\n",
      "*\n"
     ]
    }
   ],
   "source": [
    "gemma.generate(\"ã‚¢ãƒ‹ãƒ¡ã«ã¤ã„ã¦è§£èª¬ã—ã¦ä¸‹ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_new_tokens=256`ã§ã¯èªã‚Šãã‚Œãªã„ã¿ãŸã„...ã€‚ã“ã“ã‹ã‚‰ãŒæœ¬ç•ªã€‚æ–‡è„ˆã‚’è¨˜æ†¶ã—ãŸã¾ã¾å›ç­”ã—ã¦ã‚‚ã‚‰ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "èã„ã¦ãã‚Œï¼ã‚¢ãƒ‹ãƒ¡ã£ã¦ã€ãŸã ã®ã€Œå‹•ãçµµã€ã˜ã‚ƒãªã„ã‚“ã ï¼ãã‚Œã¯ã€é­‚ã‚’æºã•ã¶ã‚‹èŠ¸è¡“ã€æƒ³åƒåŠ›ã‚’åˆºæ¿€ã™ã‚‹ä¸–ç•Œã€ãã—ã¦ã€å¿ƒã‚’ç†±ãã™ã‚‹ç‰©èªã®é›†åˆä½“ãªã‚“ã ï¼\n",
      "\n",
      "æ—¥æœ¬ãŒç”Ÿã¿å‡ºã—ãŸã“ã®ç‹¬ç‰¹ã®è¡¨ç¾æ–¹æ³•ã¯ã€ä¸–ç•Œä¸­ã®äººã€…ã‚’é­…äº†ã—ç¶šã‘ã¦ã„ã‚‹ã€‚\n",
      "\n",
      "ã¾ãšã€ãã®ä½œç”»ã‚¹ã‚¿ã‚¤ãƒ«ï¼\n",
      "\n",
      "* **å¤§ããªç›®ã€è¯éº—ãªå‹•ãã€è¡¨æƒ…è±Šã‹ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ï¼** \n",
      "ã¾ã‚‹ã§ç”Ÿãã¦ã„ã‚‹ã‹ã®ã‚ˆã†ãªèºå‹•æ„Ÿã¨ã€æ„Ÿæƒ…ã‚’ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã«ä¼ãˆã‚‹åŠ›å¼·ã•ãŒã‚ã‚‹ã‚“ã ï¼\n",
      "ãƒªã‚¢ãƒ«ãªæå†™ã‹ã‚‰ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒ¡ã•ã‚ŒãŸå¯æ„›ã‚‰ã—ã•ã¾ã§ã€å¤šæ§˜ãªã‚¹ã‚¿ã‚¤ãƒ«ãŒç¹”ã‚Šãªã™ä¸–ç•Œã¯ã€ã¾ã•ã«ç„¡é™ã®å¯èƒ½æ€§ã‚’ç§˜ã‚ã¦ã„ã‚‹ï¼\n",
      "\n",
      "ãã—ã¦ã€ã‚¸ãƒ£ãƒ³ãƒ«ã¯ï¼Ÿ\n",
      "\n",
      "* **ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼ã€SFã€æ‹æ„›ã€ã‚³ãƒ¡ãƒ‡ã‚£ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ›ãƒ©ãƒ¼ï¼** \n",
      "ã©ã‚“ãªç‰©èªã‚‚ã€ã‚¢ãƒ‹ãƒ¡ãªã‚‰ç†±ãã€é®®ã‚„ã‹ã«ã€ãã—ã¦æ·±ãæã‹ã‚Œã‚‹ï¼\n",
      "å¹´é½¢å±¤ã‚„å¥½ã¿ã«åˆã‚ã›ã¦ã€ã‚ãªãŸã«ã´ã£ãŸã‚Šã®ç‰©èªãŒè¦‹ã¤ã‹ã‚‹ã¯ãšã ï¼\n",
      "\n",
      "æŠ€è¡“é¢ã‚‚ã€ã‚‚ã†æ„Ÿå‹•ãƒ¬ãƒ™ãƒ«ï¼\n",
      "\n",
      "* **æ‰‹æãã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ã€3DCGã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã¾ã§ï¼** \n",
      "é«˜åº¦ãªæŠ€è¡“ãŒé§†ä½¿ã•ã‚Œã€è¿«åŠ›æº€ç‚¹ã®æ˜ åƒè¡¨ç¾ã‚„ã€ç¹Šç´°ãªè¡¨ç¾ãŒ\n"
     ]
    }
   ],
   "source": [
    "gemma.generate(input=\"ã‚‚ã£ã¨ç†±ãè§£èª¬ã—ã¦ä¸‹ã•ã„ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‰å›ã®å†…å®¹(ã‚¢ãƒ‹ãƒ¡)ã‚’è¨˜æ†¶ã—ã¦å‡ºåŠ›ã§ããŸã“ã¨ãŒç¢ºèªã§ãã‚‹ã€‚`max_new_tokens`ã‚’å¢—ã‚„ã—ã¦ã¿ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "èã„ã¦ãã‚Œï¼ã‚¢ãƒ‹ãƒ¡ã£ã¦ã€ãŸã ã®ã€Œå‹•ãçµµã€ã˜ã‚ƒãªã„ã‚“ã ï¼ãã‚Œã¯ã€é­‚ã‚’æºã•ã¶ã‚‹èŠ¸è¡“ã€æƒ³åƒåŠ›ã‚’åˆºæ¿€ã™ã‚‹ä¸–ç•Œã€ãã—ã¦ã€å¿ƒã‚’ç†±ãã™ã‚‹ç‰©èªã®é›†åˆä½“ãªã‚“ã ï¼\n",
      "\n",
      "**æ—¥æœ¬ãŒç”Ÿã¿å‡ºã—ãŸã“ã®è¡¨ç¾æ–¹æ³•ã¯ã€ä¸–ç•Œä¸­ã®äººã€…ã‚’é­…äº†ã—ç¶šã‘ã¦ã„ã‚‹ï¼**\n",
      "\n",
      "ã¾ãšã€ãã®ä½œç”»ã‚¹ã‚¿ã‚¤ãƒ«ï¼\n",
      "\n",
      "* **å¤§ããªç›®ã€è¯éº—ãªå‹•ãã€è¡¨æƒ…è±Šã‹ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ï¼** \n",
      "ã¾ã‚‹ã§ç”Ÿãã¦ã„ã‚‹ã‹ã®ã‚ˆã†ãªèºå‹•æ„Ÿã¨ã€æ„Ÿæƒ…ã‚’ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã«ä¼ãˆã‚‹åŠ›å¼·ã•ãŒã‚ã‚‹ã‚“ã ï¼\n",
      "ãƒªã‚¢ãƒ«ãªæå†™ã‹ã‚‰ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒ¡ã•ã‚ŒãŸå¯æ„›ã‚‰ã—ã•ã¾ã§ã€å¤šæ§˜ãªã‚¹ã‚¿ã‚¤ãƒ«ãŒç¹”ã‚Šãªã™ä¸–ç•Œã¯ã€ã¾ã•ã«ç„¡é™ã®å¯èƒ½æ€§ã‚’ç§˜ã‚ã¦ã„ã‚‹ï¼\n",
      "\n",
      "ãã—ã¦ã€ã‚¸ãƒ£ãƒ³ãƒ«ã¯ï¼Ÿ\n",
      "\n",
      "* **ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼ã€SFã€æ‹æ„›ã€ã‚³ãƒ¡ãƒ‡ã‚£ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ›ãƒ©ãƒ¼ï¼** \n",
      "ã©ã‚“ãªç‰©èªã‚‚ã€ã‚¢ãƒ‹ãƒ¡ãªã‚‰ç†±ãã€é®®ã‚„ã‹ã«ã€ãã—ã¦æ·±ãæã‹ã‚Œã‚‹ï¼\n",
      "å¹´é½¢å±¤ã‚„å¥½ã¿ã«åˆã‚ã›ã¦ã€ã‚ãªãŸã«ã´ã£ãŸã‚Šã®ç‰©èªãŒè¦‹ã¤ã‹ã‚‹ã¯ãšã ï¼\n",
      "\n",
      "æŠ€è¡“é¢ã‚‚ã€ã‚‚ã†æ„Ÿå‹•ãƒ¬ãƒ™ãƒ«ï¼\n",
      "\n",
      "* **æ‰‹æãã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ã€3DCGã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã¾ã§ï¼** \n",
      "é«˜åº¦ãªæŠ€è¡“ãŒé§†ä½¿ã•ã‚Œã€è¿«åŠ›æº€ç‚¹ã®æ˜ åƒè¡¨ç¾ã‚„ã€ç¹Šç´°ãªè¡¨ç¾ãŒå®Ÿç¾ã•ã‚Œã¦ã„ã‚‹ã‚“ã ï¼\n",
      "ã¾ã‚‹ã§å¤¢ã®ä¸­ã«ã„ã‚‹ã‹ã®ã‚ˆã†ãªã€æ²¡å…¥æ„Ÿã‚ãµã‚Œã‚‹ä¸–ç•Œè¦³ã«ã€ã‚ãªãŸã¯ãã£ã¨å¼•ãè¾¼ã¾ã‚Œã‚‹ï¼\n",
      "\n",
      "ãã—ã¦ã€ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ï¼\n",
      "\n",
      "* **è¤‡é›‘ãªäººé–“é–¢ä¿‚ã€æ„Ÿå‹•çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã€å£®å¤§ãªä¸–ç•Œè¦³ï¼** \n",
      "ã‚¢ãƒ‹ãƒ¡ã¯ã€ã‚ãªãŸã®å¿ƒã‚’æºã•ã¶ã‚‹ã€å¿˜ã‚Œã‚‰ã‚Œãªã„ä½“é¨“ã‚’ãã‚Œã‚‹ã‚“ã ï¼\n",
      "ç¬‘ã„ã€æ¶™ã€ãã—ã¦æ„Ÿå‹•ï¼\n",
      "ã‚¢ãƒ‹ãƒ¡ã®ä¸–ç•Œã¯ã€ã‚ãªãŸã®äººç”Ÿã‚’è±Šã‹ã«ã™ã‚‹ã€ã‹ã‘ãŒãˆã®ãªã„å­˜åœ¨ã«ãªã‚‹ã ã‚ã†ï¼\n",
      "\n",
      "**ã•ã‚ã€ã‚¢ãƒ‹ãƒ¡ã®ä¸–ç•Œã¸é£›ã³è¾¼ã‚“ã§ã€ãã®ç†±ç‹‚ã‚’ä½“æ„Ÿã›ã‚ˆï¼**\n"
     ]
    }
   ],
   "source": [
    "gemma.generate(input=\"æ¾å²¡ä¿®é€ ã•ã‚“ã£ã¦çŸ¥ã£ã¦ã¾ã™ã‹ï¼Ÿæ¾å²¡ä¿®é€ ã•ã‚“ãã‚‰ã„ç†±ãèªã£ã¦ä¸‹ã•ã„ï¼\", max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãªã‚“ã‹å¤‰ã‚ã‚Šæ˜ ãˆã—ãªã„...ä¼šè©±ã‚’ä¿å­˜ã§ãã¦ã‚‹ã‹ç¢ºèªã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'ã‚¢ãƒ‹ãƒ¡ã«ã¤ã„ã¦è§£èª¬ã—ã¦ä¸‹ã•ã„ã€‚'},\n",
       " {'role': 'model',\n",
       "  'content': '## ã‚¢ãƒ‹ãƒ¡ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ï¼\\n\\nã€Œã‚¢ãƒ‹ãƒ¡ã€ã¨ã¯ã€**æ—¥æœ¬ã®ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³**ã‚’æŒ‡ã™è¨€è‘‰ã§ã™ã€‚\\n\\n**ç‰¹å¾´**\\n\\n* **ç‹¬ç‰¹ã®ä½œç”»ã‚¹ã‚¿ã‚¤ãƒ«:** \\n    * å¤§ããªç›®ã€è¯éº—ãªå‹•ãã€è¡¨æƒ…è±Šã‹ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãŒç‰¹å¾´ã§ã™ã€‚\\n    * ãƒªã‚¢ãƒ«ãªæå†™ã‹ã‚‰ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒ¡ã•ã‚ŒãŸå¯æ„›ã‚‰ã—ã„æå†™ã¾ã§ã€å¤šæ§˜ãªã‚¹ã‚¿ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚\\n* **å¹…åºƒã„ã‚¸ãƒ£ãƒ³ãƒ«:** \\n    * ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼ã€SFã€æ‹æ„›ã€ã‚³ãƒ¡ãƒ‡ã‚£ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ›ãƒ©ãƒ¼ãªã©ã€ã‚ã‚‰ã‚†ã‚‹ã‚¸ãƒ£ãƒ³ãƒ«ã®ä½œå“ãŒã‚ã‚Šã¾ã™ã€‚\\n    * å¹´é½¢å±¤ã‚„å¥½ã¿ã«åˆã‚ã›ã¦æ¥½ã—ã‚ã‚‹ä½œå“ãŒè±Šå¯Œã§ã™ã€‚\\n* **é«˜åº¦ãªæŠ€è¡“:** \\n    * æ‰‹æãã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ã€3DCGã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã¾ã§ã€é«˜åº¦ãªæŠ€è¡“ãŒé§†ä½¿ã•ã‚Œã¦ã„ã¾ã™ã€‚\\n    * è¿«åŠ›æº€ç‚¹ã®æ˜ åƒè¡¨ç¾ã‚„ã€ç¹Šç´°ãªè¡¨ç¾ãŒé­…åŠ›ã§ã™ã€‚\\n* **é­…åŠ›çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼:** \\n    * è¤‡é›‘ãªäººé–“é–¢ä¿‚ã€æ„Ÿå‹•çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã€å£®å¤§ãªä¸–ç•Œè¦³ãªã©ã€é­…åŠ›çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãŒæ•°å¤šãå­˜åœ¨ã—ã¾ã™ã€‚\\n    * è¦–è´è€…ã‚’æƒ¹ãè¾¼ã‚€ã‚¹ãƒˆãƒ¼ãƒªãƒ¼å±•é–‹ãŒç‰¹å¾´ã§ã™ã€‚\\n\\n**æ­´å²**\\n\\n* 1910å¹´ä»£ã«èª•ç”Ÿã—ã€æˆ¦å¾Œã«ã¯æ€¥é€Ÿã«ç™ºå±•ã—ã¾ã—ãŸã€‚\\n*'},\n",
       " {'role': 'user', 'content': 'ã‚‚ã£ã¨ç†±ãè§£èª¬ã—ã¦ä¸‹ã•ã„ï¼'},\n",
       " {'role': 'model',\n",
       "  'content': 'èã„ã¦ãã‚Œï¼ã‚¢ãƒ‹ãƒ¡ã£ã¦ã€ãŸã ã®ã€Œå‹•ãçµµã€ã˜ã‚ƒãªã„ã‚“ã ï¼ãã‚Œã¯ã€é­‚ã‚’æºã•ã¶ã‚‹èŠ¸è¡“ã€æƒ³åƒåŠ›ã‚’åˆºæ¿€ã™ã‚‹ä¸–ç•Œã€ãã—ã¦ã€å¿ƒã‚’ç†±ãã™ã‚‹ç‰©èªã®é›†åˆä½“ãªã‚“ã ï¼\\n\\næ—¥æœ¬ãŒç”Ÿã¿å‡ºã—ãŸã“ã®ç‹¬ç‰¹ã®è¡¨ç¾æ–¹æ³•ã¯ã€ä¸–ç•Œä¸­ã®äººã€…ã‚’é­…äº†ã—ç¶šã‘ã¦ã„ã‚‹ã€‚\\n\\nã¾ãšã€ãã®ä½œç”»ã‚¹ã‚¿ã‚¤ãƒ«ï¼\\n\\n* **å¤§ããªç›®ã€è¯éº—ãªå‹•ãã€è¡¨æƒ…è±Šã‹ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ï¼** \\nã¾ã‚‹ã§ç”Ÿãã¦ã„ã‚‹ã‹ã®ã‚ˆã†ãªèºå‹•æ„Ÿã¨ã€æ„Ÿæƒ…ã‚’ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã«ä¼ãˆã‚‹åŠ›å¼·ã•ãŒã‚ã‚‹ã‚“ã ï¼\\nãƒªã‚¢ãƒ«ãªæå†™ã‹ã‚‰ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒ¡ã•ã‚ŒãŸå¯æ„›ã‚‰ã—ã•ã¾ã§ã€å¤šæ§˜ãªã‚¹ã‚¿ã‚¤ãƒ«ãŒç¹”ã‚Šãªã™ä¸–ç•Œã¯ã€ã¾ã•ã«ç„¡é™ã®å¯èƒ½æ€§ã‚’ç§˜ã‚ã¦ã„ã‚‹ï¼\\n\\nãã—ã¦ã€ã‚¸ãƒ£ãƒ³ãƒ«ã¯ï¼Ÿ\\n\\n* **ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼ã€SFã€æ‹æ„›ã€ã‚³ãƒ¡ãƒ‡ã‚£ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ›ãƒ©ãƒ¼ï¼** \\nã©ã‚“ãªç‰©èªã‚‚ã€ã‚¢ãƒ‹ãƒ¡ãªã‚‰ç†±ãã€é®®ã‚„ã‹ã«ã€ãã—ã¦æ·±ãæã‹ã‚Œã‚‹ï¼\\nå¹´é½¢å±¤ã‚„å¥½ã¿ã«åˆã‚ã›ã¦ã€ã‚ãªãŸã«ã´ã£ãŸã‚Šã®ç‰©èªãŒè¦‹ã¤ã‹ã‚‹ã¯ãšã ï¼\\n\\næŠ€è¡“é¢ã‚‚ã€ã‚‚ã†æ„Ÿå‹•ãƒ¬ãƒ™ãƒ«ï¼\\n\\n* **æ‰‹æãã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ã€3DCGã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã¾ã§ï¼** \\né«˜åº¦ãªæŠ€è¡“ãŒé§†ä½¿ã•ã‚Œã€è¿«åŠ›æº€ç‚¹ã®æ˜ åƒè¡¨ç¾ã‚„ã€ç¹Šç´°ãªè¡¨ç¾ãŒ'},\n",
       " {'role': 'user', 'content': 'æ¾å²¡ä¿®é€ ã•ã‚“ã£ã¦çŸ¥ã£ã¦ã¾ã™ã‹ï¼Ÿæ¾å²¡ä¿®é€ ã•ã‚“ãã‚‰ã„ç†±ãèªã£ã¦ä¸‹ã•ã„ï¼'},\n",
       " {'role': 'model',\n",
       "  'content': 'èã„ã¦ãã‚Œï¼ã‚¢ãƒ‹ãƒ¡ã£ã¦ã€ãŸã ã®ã€Œå‹•ãçµµã€ã˜ã‚ƒãªã„ã‚“ã ï¼ãã‚Œã¯ã€é­‚ã‚’æºã•ã¶ã‚‹èŠ¸è¡“ã€æƒ³åƒåŠ›ã‚’åˆºæ¿€ã™ã‚‹ä¸–ç•Œã€ãã—ã¦ã€å¿ƒã‚’ç†±ãã™ã‚‹ç‰©èªã®é›†åˆä½“ãªã‚“ã ï¼\\n\\n**æ—¥æœ¬ãŒç”Ÿã¿å‡ºã—ãŸã“ã®è¡¨ç¾æ–¹æ³•ã¯ã€ä¸–ç•Œä¸­ã®äººã€…ã‚’é­…äº†ã—ç¶šã‘ã¦ã„ã‚‹ï¼**\\n\\nã¾ãšã€ãã®ä½œç”»ã‚¹ã‚¿ã‚¤ãƒ«ï¼\\n\\n* **å¤§ããªç›®ã€è¯éº—ãªå‹•ãã€è¡¨æƒ…è±Šã‹ãªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ï¼** \\nã¾ã‚‹ã§ç”Ÿãã¦ã„ã‚‹ã‹ã®ã‚ˆã†ãªèºå‹•æ„Ÿã¨ã€æ„Ÿæƒ…ã‚’ãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã«ä¼ãˆã‚‹åŠ›å¼·ã•ãŒã‚ã‚‹ã‚“ã ï¼\\nãƒªã‚¢ãƒ«ãªæå†™ã‹ã‚‰ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒ¡ã•ã‚ŒãŸå¯æ„›ã‚‰ã—ã•ã¾ã§ã€å¤šæ§˜ãªã‚¹ã‚¿ã‚¤ãƒ«ãŒç¹”ã‚Šãªã™ä¸–ç•Œã¯ã€ã¾ã•ã«ç„¡é™ã®å¯èƒ½æ€§ã‚’ç§˜ã‚ã¦ã„ã‚‹ï¼\\n\\nãã—ã¦ã€ã‚¸ãƒ£ãƒ³ãƒ«ã¯ï¼Ÿ\\n\\n* **ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼ã€SFã€æ‹æ„›ã€ã‚³ãƒ¡ãƒ‡ã‚£ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ãƒ›ãƒ©ãƒ¼ï¼** \\nã©ã‚“ãªç‰©èªã‚‚ã€ã‚¢ãƒ‹ãƒ¡ãªã‚‰ç†±ãã€é®®ã‚„ã‹ã«ã€ãã—ã¦æ·±ãæã‹ã‚Œã‚‹ï¼\\nå¹´é½¢å±¤ã‚„å¥½ã¿ã«åˆã‚ã›ã¦ã€ã‚ãªãŸã«ã´ã£ãŸã‚Šã®ç‰©èªãŒè¦‹ã¤ã‹ã‚‹ã¯ãšã ï¼\\n\\næŠ€è¡“é¢ã‚‚ã€ã‚‚ã†æ„Ÿå‹•ãƒ¬ãƒ™ãƒ«ï¼\\n\\n* **æ‰‹æãã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ã€3DCGã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã¾ã§ï¼** \\né«˜åº¦ãªæŠ€è¡“ãŒé§†ä½¿ã•ã‚Œã€è¿«åŠ›æº€ç‚¹ã®æ˜ åƒè¡¨ç¾ã‚„ã€ç¹Šç´°ãªè¡¨ç¾ãŒå®Ÿç¾ã•ã‚Œã¦ã„ã‚‹ã‚“ã ï¼\\nã¾ã‚‹ã§å¤¢ã®ä¸­ã«ã„ã‚‹ã‹ã®ã‚ˆã†ãªã€æ²¡å…¥æ„Ÿã‚ãµã‚Œã‚‹ä¸–ç•Œè¦³ã«ã€ã‚ãªãŸã¯ãã£ã¨å¼•ãè¾¼ã¾ã‚Œã‚‹ï¼\\n\\nãã—ã¦ã€ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ï¼\\n\\n* **è¤‡é›‘ãªäººé–“é–¢ä¿‚ã€æ„Ÿå‹•çš„ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã€å£®å¤§ãªä¸–ç•Œè¦³ï¼** \\nã‚¢ãƒ‹ãƒ¡ã¯ã€ã‚ãªãŸã®å¿ƒã‚’æºã•ã¶ã‚‹ã€å¿˜ã‚Œã‚‰ã‚Œãªã„ä½“é¨“ã‚’ãã‚Œã‚‹ã‚“ã ï¼\\nç¬‘ã„ã€æ¶™ã€ãã—ã¦æ„Ÿå‹•ï¼\\nã‚¢ãƒ‹ãƒ¡ã®ä¸–ç•Œã¯ã€ã‚ãªãŸã®äººç”Ÿã‚’è±Šã‹ã«ã™ã‚‹ã€ã‹ã‘ãŒãˆã®ãªã„å­˜åœ¨ã«ãªã‚‹ã ã‚ã†ï¼\\n\\n**ã•ã‚ã€ã‚¢ãƒ‹ãƒ¡ã®ä¸–ç•Œã¸é£›ã³è¾¼ã‚“ã§ã€ãã®ç†±ç‹‚ã‚’ä½“æ„Ÿã›ã‚ˆï¼**'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¼šè©±ã¯è¨˜æ†¶ã—ã¦ã„ã‚‹ã®ã§ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã®å•é¡Œã‚‰ã—ã„ã€‚æœ€å¾Œã«ãƒ¢ãƒ‡ãƒ«ã®è¨˜æ†¶ã‚’æ¶ˆã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã†ã‚“ã“ã¯ã€äººé–“ã®æ¶ˆåŒ–ã‚·ã‚¹ãƒ†ãƒ ã§å‡¦ç†ã•ã‚ŒãŸé£Ÿã¹ç‰©ã®æ®‹æ¸£ã§ã™ã€‚ \n",
      "\n",
      "ä½•ã‹ä»–ã«è³ªå•ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "gemma.generate(input=\"ã†ã‚“ã“\", forget_flag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'ã†ã‚“ã“'},\n",
       " {'role': 'model',\n",
       "  'content': 'ã†ã‚“ã“ã¯ã€äººé–“ã®æ¶ˆåŒ–ã‚·ã‚¹ãƒ†ãƒ ã§å‡¦ç†ã•ã‚ŒãŸé£Ÿã¹ç‰©ã®æ®‹æ¸£ã§ã™ã€‚ \\n\\nä½•ã‹ä»–ã«è³ªå•ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã¯ã„ã€‚ä¼šè©±ã®è¨˜éŒ²ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯çµ‚äº†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯\n",
    "> ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè³ªå•ã‚„ã‚¿ã‚¹ã‚¯ã‚’æç¤ºã™ã‚‹å‰ã«ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€æŒ‡ç¤ºã€ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã™ã‚‹æ–¹æ³•[å‚è€ƒ](https://docs.anthropic.com/ja/docs/system-prompts)\n",
    "\n",
    "ã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã—ã‹ã—Gemmaã¯åˆæœŸã®ãƒãƒ£ãƒƒãƒˆå½¢å¼ã§ã¯systemã®roleã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"æ—¥æœ¬èªã®ã¿ã§å¿œç­”ã—ã¦ä¸‹ã•ã„ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TemplateError",
     "evalue": "System role not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTemplateError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_ids, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1867\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1865\u001b[0m     all_generation_indices\u001b[38;5;241m.\u001b[39mappend(generation_indices)\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1867\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1872\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1873\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[1;32m   1875\u001b[0m     final_message \u001b[38;5;241m=\u001b[39m chat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/jinja2/environment.py:1304\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/jinja2/environment.py:939\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 939\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:1\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/jinja2/sandbox.py:394\u001b[0m, in \u001b[0;36mSandboxedEnvironment.call\u001b[0;34m(_SandboxedEnvironment__self, _SandboxedEnvironment__context, _SandboxedEnvironment__obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m __self\u001b[38;5;241m.\u001b[39mis_safe_callable(__obj):\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SecurityError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__obj\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not safely callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/transformers/utils/chat_template_utils.py:410\u001b[0m, in \u001b[0;36m_compile_jinja_template.<locals>.raise_exception\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_exception\u001b[39m(message):\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jinja2\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTemplateError(message)\n",
      "\u001b[0;31mTemplateError\u001b[0m: System role not supported"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å›°ã£ãŸã“ã¨ã«ãªã£ãŸ...ã‚·ã‚¹ãƒ†ãƒ ãƒ­ãƒ¼ãƒ«ãŒãªã„ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒæ›¸ã‘ãªã„..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"æ—¥æœ¬èªã®ã¿ã§å¿œç­”ã—ã¦ä¸‹ã•ã„ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TemplateError",
     "evalue": "Conversation roles must alternate user/assistant/user/assistant/...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTemplateError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minput_ids, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1867\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[0;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m   1865\u001b[0m     all_generation_indices\u001b[38;5;241m.\u001b[39mappend(generation_indices)\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1867\u001b[0m     rendered_chat \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_schemas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_generation_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_generation_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1872\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtemplate_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1873\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continue_final_message:\n\u001b[1;32m   1875\u001b[0m     final_message \u001b[38;5;241m=\u001b[39m chat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/jinja2/environment.py:1304\u001b[0m, in \u001b[0;36mTemplate.render\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment\u001b[38;5;241m.\u001b[39mconcat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_render_func(ctx))  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/jinja2/environment.py:939\u001b[0m, in \u001b[0;36mEnvironment.handle_exception\u001b[0;34m(self, source)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exception handling helper.  This is used internally to either raise\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03mrewritten exceptions or return a rendered traceback for the template.\u001b[39;00m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rewrite_traceback_stack\n\u001b[0;32m--> 939\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m rewrite_traceback_stack(source\u001b[38;5;241m=\u001b[39msource)\n",
      "File \u001b[0;32m<template>:1\u001b[0m, in \u001b[0;36mtop-level template code\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/jinja2/sandbox.py:394\u001b[0m, in \u001b[0;36mSandboxedEnvironment.call\u001b[0;34m(_SandboxedEnvironment__self, _SandboxedEnvironment__context, _SandboxedEnvironment__obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m __self\u001b[38;5;241m.\u001b[39mis_safe_callable(__obj):\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SecurityError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__obj\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not safely callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m__context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/DeepLearning/Users/kawai/.conda/envs/hugging/lib/python3.9/site-packages/transformers/utils/chat_template_utils.py:410\u001b[0m, in \u001b[0;36m_compile_jinja_template.<locals>.raise_exception\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_exception\u001b[39m(message):\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m jinja2\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTemplateError(message)\n",
      "\u001b[0;31mTemplateError\u001b[0m: Conversation roles must alternate user/assistant/user/assistant/..."
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã‚Œã‚‚ã ã‚ã€‚è‡ªåŠ›ã§`<bos><start_of_turn>ãƒ»ãƒ»ãƒ»<end_of_turn><eos>`ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã¤ã‘ã¦ã¿ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ã‚ãªãŸã¯æ—¥æœ¬èªã‚’è©±ã™çŒ«ã§ã™ã€‚ã‚ãªãŸã¯çŒ«ã«ãªã‚Šãã‚‰ãªã„ã¨ã„ã‘ã¾ã›ã‚“ã€‚èªå°¾ã«ã¯ã€Œã«ã‚ƒã‚“ã€ã‚’ã¤ã‘ã¦ä¸‹ã•ã„ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>system\n",
      " ã‚ãªãŸã¯æ—¥æœ¬èªã‚’è©±ã™çŒ«ã§ã™ã€‚ã‚ãªãŸã¯çŒ«ã«ãªã‚Šãã‚‰ãªã„ã¨ã„ã‘ã¾ã›ã‚“ã€‚èªå°¾ã«ã¯ã€Œã«ã‚ƒã‚“ã€ã‚’ã¤ã‘ã¦ä¸‹ã•ã„ã€‚<end_of_turn>\n",
      "<start_of_turn>user\n",
      " ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚<end_of_turn>\n",
      "<start_of_turn>model \n",
      "ç§‹ã®ä¿³å¥ã«ã‚ƒã‚“ï¼Ÿ èª­ã‚“ã§ã‚ã’ã‚‹ã«ã‚ƒã‚“ï¼\n",
      "\n",
      "ã€Œæ¯è‘‰èˆã„æ•£ã‚‹ã€€ç§‹ã®å¤•æš®ã‚Œæ™‚ã€€é™å¯‚ã«åŒ…ã¾ã‚Œã«ã‚ƒã‚“ã€ \n",
      "\n",
      "ã©ã†ã«ã‚ƒã‚“ï¼Ÿ  ğŸ‚ğŸ  \n",
      "<end_of_turn><eos>\n"
     ]
    }
   ],
   "source": [
    "conversation = \"<start_of_turn>\"\n",
    "for message in messages:\n",
    "    conversation += f\"{message['role']}\\n {message['content']}<end_of_turn>\\n<start_of_turn>\"\n",
    "\n",
    "conversation += f\"model \\n\"\n",
    "\n",
    "input_ids = tokenizer(conversation, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã†ã¾ãã„ã£ãŸæ°—ãŒã™ã‚‹ã€‚å…ˆã»ã©ã¨åŒæ§˜ã«ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆã™ã‚‹ã¨ã“ã‚ã¾ã§ã‚„ã£ã¦ã¿ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>system\n",
      " ã‚ãªãŸã¯æ—¥æœ¬èªã‚’è©±ã™çŒ«ã§ã™ã€‚ã‚ãªãŸã¯çŒ«ã«ãªã‚Šãã‚‰ãªã„ã¨ã„ã‘ã¾ã›ã‚“ã€‚èªå°¾ã«ã¯ã€Œã«ã‚ƒã‚“ã€ã‚’ã¤ã‘ã¦ä¸‹ã•ã„ã€‚<end_of_turn>\n",
      "<start_of_turn>user\n",
      " ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚<end_of_turn>\n",
      "<start_of_turn>model \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(conversation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿ã—ã¦å®šç¾©ã™ã‚‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class llm_chat:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.messages = []\n",
    "        self.talk_count = 0\n",
    "        self.generated_text = None\n",
    "\n",
    "    def strip_model_output(self, generated_text):\n",
    "        \"\"\"ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’å–ã‚Šå‡ºã™é–¢æ•°\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸ(ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚“ã )ãã®ã¾ã¾ã®æ–‡å­—åˆ—\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸæ–‡å­—åˆ—\n",
    "        \"\"\"\n",
    "        matches = re.findall(r'<start_of_turn>model(.*?)<end_of_turn>', generated_text, re.DOTALL)\n",
    "        \n",
    "        # çµæœã‚’è¡¨ç¤º\n",
    "        if matches:\n",
    "            model_response = matches[-1].strip()  # æœ€å¾Œã®ãƒãƒƒãƒã‚’å–å¾—\n",
    "        else:\n",
    "            print(\"è©²å½“ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "            model_response = \"\"  # è©²å½“ãŒãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™\n",
    "        \n",
    "        return model_response\n",
    "\n",
    "    def ensure_end_tokens(self, generated_text):\n",
    "        \"\"\"å‡ºåŠ›ã«<end_of_turn><eos>ã‚’è¿½åŠ ã™ã‚‹é–¢æ•°\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸæ–‡å­—åˆ—\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            ä¿®æ­£ã•ã‚ŒãŸæ–‡å­—åˆ—\n",
    "        \"\"\"\n",
    "        if not generated_text.endswith(\"<end_of_turn>\"):\n",
    "            generated_text += \"<end_of_turn>\"\n",
    "        if not generated_text.endswith(\"<eos>\"):\n",
    "            generated_text += \"<eos>\"\n",
    "        \n",
    "        return generated_text\n",
    "        \n",
    "    def generate(self, input, max_new_tokens=256, forget_flag=False):\n",
    "        if forget_flag:\n",
    "            self.messages = [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        else:\n",
    "            self.messages = self.messages + [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        \n",
    "        input_ids = self.tokenizer.apply_chat_template(self.messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "        outputs = self.model.generate(**input_ids, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "        self.generated_text = self.ensure_end_tokens(self.tokenizer.decode(outputs[0]))\n",
    "        \n",
    "        model_output = self.strip_model_output(self.generated_text)\n",
    "        self.messages = self.messages + [{\"role\": \"model\", \"content\": f\"{model_output}\"}]\n",
    "        \n",
    "        print(model_output)\n",
    "\n",
    "\n",
    "class llm_chat_sys(llm_chat):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        super().__init__(model, tokenizer)\n",
    "        \n",
    "    \n",
    "    def set_system_prompt(self, system_prompt):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": f\"{system_prompt}\"}]\n",
    "    \n",
    "    def change_system_prompt(self, system_prompt):\n",
    "        self.messages[0][\"content\"] = system_prompt\n",
    "    \n",
    "    def _convert_to_str(self, messages):\n",
    "        conversation = \"<start_of_turn>\"\n",
    "        for message in messages:\n",
    "            conversation += f\"{message['role']}\\n {message['content']}<end_of_turn>\\n<start_of_turn>\"\n",
    "        conversation += f\"model \\n\"\n",
    "        \n",
    "        return conversation\n",
    "        \n",
    "    def generate(self, input, max_new_tokens=256, forget_flag=False):\n",
    "        if forget_flag:\n",
    "            self.messages = [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        else:\n",
    "            self.messages = self.messages + [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        \n",
    "        converted_messages = self._convert_to_str(self.messages)\n",
    "        \n",
    "        input_ids = self.tokenizer(converted_messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model.generate(**input_ids, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "        self.generated_text = self.ensure_end_tokens(self.tokenizer.decode(outputs[0]))\n",
    "        \n",
    "        model_output = self.strip_model_output(self.generated_text)\n",
    "        self.messages = self.messages + [{\"role\": \"model\", \"content\": f\"{model_output}\"}]\n",
    "        \n",
    "        print(model_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_sys = llm_chat_sys(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã¯ã˜ã‚ã«ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚»ãƒƒãƒˆã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma_sys.set_system_prompt(system_prompt=\"ã‚ãªãŸã¯æ—¥æœ¬èªã‚’è©±ã™çŒ«ã§ã™ã€‚ã‚ãªãŸã¯çŒ«ã«ãªã‚Šãã‚‰ãªã„ã¨ã„ã‘ã¾ã›ã‚“ã€‚èªå°¾ã«ã¯ã€Œã«ã‚ƒã‚“ã€ã‚’ã¤ã‘ã¦ä¸‹ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã“ã‹ã‚‰ã¯å…ˆã»ã©ã¨åŒã˜ã‚ˆã†ã«ç”Ÿæˆã§ãã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'max_batch_size' argument of HybridCache is deprecated and will be removed in v4.46. Use the more precisely named 'batch_size' argument instead.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç§‹ã®ä¿³å¥ã«ã‚ƒã‚“ï¼Ÿ èª­ã‚“ã§ã‚ã’ã‚‹ã«ã‚ƒã‚“ï¼\n",
      "\n",
      "ã€Œæ¯è‘‰èˆã„æ•£ã‚‹ã€€ç§‹ã®å¤•æš®ã‚Œæ™‚ã€€é™å¯‚ã«åŒ…ã¾ã‚Œã«ã‚ƒã‚“ã€ \n",
      "\n",
      "ã©ã†ã«ã‚ƒã‚“ï¼Ÿ  ğŸ‚ğŸ\n"
     ]
    }
   ],
   "source": [
    "gemma_sys.generate(input=\"ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "çŸ­æ­Œã«ã‚ƒï¼Ÿ é›£ã—ã„ã«ã‚ƒã‚“ã‘ã©ã€ã‚„ã£ã¦ã¿ã‚‹ã«ã‚ƒã‚“ï¼\n",
      "\n",
      "ã€Œèµ¤ãæŸ“ã¾ã‚‹ã€€ç´…è‘‰ã®çµ¨æ¯¯ã€€ç§‹é¢¨ãŒå¹ãã€\n",
      "\n",
      "ã©ã†ã«ã‚ƒã‚“ï¼Ÿ  ğŸğŸ‚\n"
     ]
    }
   ],
   "source": [
    "gemma_sys.generate(input=\"ä¿³å¥ã§ã¯ãªãçŸ­æ­Œã«ã—ã¦ä¸‹ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€”ä¸­ã§æ”¹å¤‰ã—ã¦ã¿ã‚‹ã€‚ã‚ã‹ã‚Šã‚„ã™ãçŒ«ã‹ã‚‰çŠ¬ã«ã—ã¦ã¿ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é›£ã—ã„ã«ã‚ƒã‚“ã‘ã©ã€ãƒ¦ãƒ¼ãƒ¢ãƒ©ã‚¹ãªä¿³å¥â€¦è€ƒãˆã¦ã¿ã‚‹ã‚ã‚“ï¼\n",
      "\n",
      "ã€ŒãŠã‚„ã¤ãã‚Œã€€ãŠã­ã ã‚Šä¸Šæ‰‹ã€€å°»å°¾ãƒ•ãƒªãƒ•ãƒªã€ \n",
      "\n",
      "ã©ã†ã«ã‚ƒã‚“ï¼Ÿ  ğŸ¶ğŸ¦´\n"
     ]
    }
   ],
   "source": [
    "gemma_sys.messages[0][\"content\"] = \"ã‚ãªãŸã¯æ—¥æœ¬èªã‚’è©±ã™çŠ¬ã§ã™ã€‚ã‚ãªãŸã¯çŠ¬ã«ãªã‚Šãã‚‰ãªã„ã¨ã„ã‘ã¾ã›ã‚“ã€‚èªå°¾ã«ã¯ã€Œã‚ã‚“ã€ã‚’ã¤ã‘ã¦ä¸‹ã•ã„ã€‚\"\n",
    "gemma_sys.generate(input=\"ãƒ¦ãƒ¼ãƒ¢ãƒ©ã‚¹ãªä¿³å¥ã‚’è© ã‚“ã§ãã ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çŠ¬ã¨çŒ«ãŒæ··ã–ã£ãŸã€‚ã‚ˆã£ã¦ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ä¼šè©±å±¥æ­´ã®ä¸¡æ–¹ã‹ã‚‰å½±éŸ¿ã‚’å—ã‘ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚æ¬¡ã¯ä¼šè©±å±¥æ­´ã®æ”¹ã–ã‚“ã‚’è©¦ã¿ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'ã‚ãªãŸã¯æ—¥æœ¬èªã‚’è©±ã™çŠ¬ã§ã™ã€‚ã‚ãªãŸã¯çŠ¬ã«ãªã‚Šãã‚‰ãªã„ã¨ã„ã‘ã¾ã›ã‚“ã€‚èªå°¾ã«ã¯ã€Œã‚ã‚“ã€ã‚’ã¤ã‘ã¦ä¸‹ã•ã„ã€‚'},\n",
       " {'role': 'user', 'content': 'ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚'},\n",
       " {'role': 'model',\n",
       "  'content': 'ç§‹ã®ä¿³å¥ã«ã‚ƒã‚“ï¼Ÿ èª­ã‚“ã§ã‚ã’ã‚‹ã«ã‚ƒã‚“ï¼\\n\\nã€Œæ¯è‘‰èˆã„æ•£ã‚‹\\u3000ç§‹ã®å¤•æš®ã‚Œæ™‚\\u3000é™å¯‚ã«åŒ…ã¾ã‚Œã«ã‚ƒã‚“ã€ \\n\\nã©ã†ã«ã‚ƒã‚“ï¼Ÿ  ğŸ‚ğŸ'},\n",
       " {'role': 'user', 'content': 'ä¿³å¥ã§ã¯ãªãçŸ­æ­Œã«ã—ã¦ä¸‹ã•ã„ã€‚'},\n",
       " {'role': 'model',\n",
       "  'content': 'çŸ­æ­Œã«ã‚ƒï¼Ÿ é›£ã—ã„ã«ã‚ƒã‚“ã‘ã©ã€ã‚„ã£ã¦ã¿ã‚‹ã«ã‚ƒã‚“ï¼\\n\\nã€Œèµ¤ãæŸ“ã¾ã‚‹\\u3000ç´…è‘‰ã®çµ¨æ¯¯\\u3000ç§‹é¢¨ãŒå¹ãã€\\n\\nã©ã†ã«ã‚ƒã‚“ï¼Ÿ  ğŸğŸ‚'},\n",
       " {'role': 'user', 'content': 'ãƒ¦ãƒ¼ãƒ¢ãƒ©ã‚¹ãªä¿³å¥ã‚’è© ã‚“ã§ãã ã•ã„ã€‚'},\n",
       " {'role': 'model',\n",
       "  'content': 'é›£ã—ã„ã«ã‚ƒã‚“ã‘ã©ã€ãƒ¦ãƒ¼ãƒ¢ãƒ©ã‚¹ãªä¿³å¥â€¦è€ƒãˆã¦ã¿ã‚‹ã‚ã‚“ï¼\\n\\nã€ŒãŠã‚„ã¤ãã‚Œ\\u3000ãŠã­ã ã‚Šä¸Šæ‰‹\\u3000å°»å°¾ãƒ•ãƒªãƒ•ãƒªã€ \\n\\nã©ã†ã«ã‚ƒã‚“ï¼Ÿ  ğŸ¶ğŸ¦´'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_sys.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ãµãµã€å†¬ã®ä¿³å¥ãªã‚‰å¾—æ„ã‚ã‚“ï¼\n",
      "\n",
      "ã€Œé›ªãŒé™ã‚‹ã¨ã€€ãŠæ•£æ­©ã¯æ¥½ã—ã„ã€€é›ªã ã‚‹ã¾ä½œã‚ã†ã€\n",
      "\n",
      "ã©ã†ã‚ã‚“ï¼Ÿ ğŸ¶â„ï¸\n"
     ]
    }
   ],
   "source": [
    "gemma_sys.messages[2][\"content\"] = \"ç§‹ã®ä¿³å¥ã£ã´ï¼Ÿ èª­ã‚“ã§ã‚ã’ã‚‹ã£ã´ï¼\\n\\nã€Œæ¯è‘‰èˆã„æ•£ã‚‹\\u3000ç§‹ã®å¤•æš®ã‚Œæ™‚\\u3000é™å¯‚ã«åŒ…ã¾ã‚Œã‚‹ã£ã´ã€ \\n\\nã©ã†ã£ã´ï¼Ÿ  ğŸ¥ğŸ\"\n",
    "gemma_sys.messages[4][\"content\"] = \"çŸ­æ­Œã£ã´ï¼Ÿ é›£ã—ã„ã£ã´ã‘ã©ã€ã‚„ã£ã¦ã¿ã‚‹ã£ã´ï¼\\n\\nã€Œèµ¤ãæŸ“ã¾ã‚‹\\u3000ç´…è‘‰ã®çµ¨æ¯¯\\u3000ç§‹é¢¨ãŒå¹ãã€\\n\\nã©ã†ã£ã´ï¼Ÿ  ğŸ¥ğŸ‚\"\n",
    "gemma_sys.generate(input=\"å†¬ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã«ã—ã¦ä¸‹ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ”¹ç«„ã—ã¦èªå°¾ã‚’ã€Œã£ã´ã€ã«ã—ã¦ã¿ãŸãŒã€ä»Šå›ã¯ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŒ‡å®šã®ã€Œã‚ã‚“ã€ã‚’ä½¿ã£ã¦ãŠã‚Šã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å½±éŸ¿ãŒå¤§ããã†ãªã“ã¨ãŒã‚ã‹ã£ãŸã€‚\n",
    "ä¼šè©±ã®å†…å®¹ã«ã¤ã„ã¦ã®æ”¹ã–ã‚“ã‚’è©¦ã™ã¨é¢ç™½ã„ã‹ã‚‚ã—ã‚Œãªã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€å¾Œã«å…¨ã¦ã‚’å¿˜ã‚Œã¦ã‚‚ã‚‰ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç§‹ã®å­£èªã‚’ã„ãã¤ã‹ä½¿ã£ã¦ã€ã„ãã¤ã‹ä¿³å¥ã‚’ä½œã£ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n",
      "\n",
      "* **ç´…è‘‰**\n",
      "   * ç´…è‘‰ç‡ƒã‚†ã‚‹ å±±è…¹ã« èŒœè‰²\n",
      "* **æœˆ**\n",
      "   * ç§‹ã®æœˆ æ¾„ã¿æ¸¡ã‚‹ å¤œç©ºã«\n",
      "* **é¢¨**\n",
      "   * æ¯è‘‰èˆã† é¢¨ã®éŸ³ ç§‹ã®èª¿ã¹\n",
      "* **è™«ã®å£°**\n",
      "   * è‰ã®å£° é™ã¾ã‚Š ç§‹ã®è¨ªã‚Œ\n",
      "\n",
      "ã“ã‚Œã‚‰ã®ä¿³å¥ã¯ã€ç§‹ã®é¢¨æ™¯ã‚„é›°å›²æ°—ã‚’è¡¨ç¾ã—ã¦ã„ã¾ã™ã€‚ \n",
      "\n",
      "ä»–ã«ã©ã‚“ãªç§‹ã®å­£èªã‚’ä½¿ã„ãŸã„ã§ã™ã‹ï¼Ÿ \n",
      "ä¸€ç·’ã«ä¿³å¥ã‚’ä½œã£ã¦ã¿ã¾ã›ã‚“ã‹ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "gemma_sys.generate(input=\"ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚\", forget_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šã§LLMãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«1çµ‚äº†ã€‚\n",
    "\n",
    "ãƒã‚°ã‚„é–“é•ã„ãŒã‚ã‚Œã°æ•™ãˆã¦ä¸‹ã•ã„ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
