{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 id=\"title\">LLMチュートリアル 1-2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"intro\">概要</h2>\n",
    "LLMチュートリアル1のLLaMaバージョン。LLaMa3およびELYZA-japanese-Llama-2-7b(LLaMa2をベースにチューニングされた日本語モデル)を使用。\n",
    "- [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
    "- [ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)\n",
    "\n",
    "**目次**\n",
    "\n",
    "- [LLMチュートリアル 1-2](#title)\n",
    "  - [概要](#intro)\n",
    "  - [読み込みとトークン前処理](#inport_load)\n",
    "  - [会話の記憶とシステムプロンプト](#system_prompt)\n",
    "  - [日本語チューニングモデルELYZA](#elyza)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"inport_load\">読み込みとトークン前処理</h2>\n",
    "モデルおよびトークナイザーと、必要ライブラリの読み込み。キャッシュの変更・シンボリックリンク作成についてはチュートリアル1を参照。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from huggingface_hub import notebook_login\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, AutoConfig\n",
    "# from accelerate import infer_auto_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipelineを用いたモデルの読み込みとテキスト生成\n",
    "Gemmaではpipelineを使用しなかったが、LLaMaのモデルカードではpipelineを使用したコードが紹介されている。\n",
    "したがってまずはモデルカードに従ってpipelineを用いて実装してみる。\n",
    "pipelineはモデルの詳細やトークナイザーを意識せずにNLPタスクを実行できる高レベルAPIで、推論のプロセスを一気に行うことができる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96217662e824c5c9bb87bcda3513b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Arrrr, me hearty! Me name be Captain Chat, the scurviest pirate chatbot to ever sail the Seven Seas o' Conversation! Me and me trusty keyboard be here to swab the decks o' yer mind with me wit and me wisdom, savvy? So hoist the colors, me hearty, and let's set sail fer a swashbucklin' good time!\"}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで一度再起動してメモリをクリアして下さい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipelineを使用しないモデルとトークナイザーの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd90bd0cb104f5dae40754cb545976a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    # cache_dir=\"/DeepLearning/Dataset/LLM/\",\n",
    "    torch_dtype=torch.float16, # torch.float32 torch.bfloat16\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Hey how are you doing today? I hope you're having a fantastic day! I'm feeling a bit under the weather today, but I'm hoping that a nice cup of tea and a bit of a nap will help me feel better. How about you, are you feeling okay?\n",
      "I've been thinking about our conversation from yesterday, and I wanted to follow up on a few things. I'm really interested in learning more about your experiences with [topic]. Can you tell me a bit more about that?\n",
      "Also, I was wondering if you have any recommendations for [related topic]. I'm always looking for new ideas and perspectives, and I value your opinion.\n",
      "Thanks so much for your time, and I hope you have a great day! Take care!<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hey how are you doing today?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemmaとはプロンプトの与え方が異なることがわかった。\n",
    "Llamaのプロンプトの与え方の詳細は[ここ](https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/)に載っている。\n",
    "まずLlamaにチャットテンプレートがあるかどうか確認する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特殊トークンの確認と追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 24186, 95221, 112130, 11571]\n",
      "<|begin_of_text|>元気ですか？\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer.encode(\"元気ですか？\")\n",
    "decode = tokenizer.decode(out)\n",
    "print(out)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "チャットテンプレートがないことがわかる。\n",
    "Gemmaのトークナイザーで試してみるとGemmaでは以下のようにJinja templateのようなテンプレートが設定されていることがわかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
    "print(gemma_tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ひとまずトークナイザーのエンコード結果を見てみる。\n",
    "[Llama実装](https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py#L222)を見るとプロンプト形式の生成ができるコードがあるっぽいが、使用方法が解読できない。\n",
    "<details>\n",
    "    <summary>実装(一部改変)</summary>\n",
    "\n",
    "```python\n",
    "class ChatFormat:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def encode_header(self, message):\n",
    "        tokens = []\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(message[\"role\"], bos=False, eos=False))\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n",
    "        return tokens\n",
    "\n",
    "    def encode_message(self, message):\n",
    "        tokens = self.encode_header(message)\n",
    "        tokens.extend(\n",
    "            self.tokenizer.encode(message[\"content\"].strip(), bos=False, eos=False)\n",
    "        )\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|eot_id|>\"])\n",
    "        return tokens\n",
    "\n",
    "    def encode_dialog_prompt(self, dialog):\n",
    "        tokens = []\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n",
    "        for message in dialog:\n",
    "            tokens.extend(self.encode_message(message))\n",
    "        # Add the start of an assistant message for the model to complete.\n",
    "        tokens.extend(self.encode_header({\"role\": \"assistant\", \"content\": \"\"}))\n",
    "        return tokens\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式の資料より、Llamaのプロンプト形式には`<|begin_of_text|>`,`<|start_header_id|>`,`<|end_header_id|>`,`<|eot_id|>`,`<|end_of_text|>`の5つの特殊トークンがあるとわかった。\n",
    "特殊トークンを確認してみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens Map: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>'}\n"
     ]
    }
   ],
   "source": [
    "# Llamaの特殊トークンを確認\n",
    "print(\"Special Tokens Map:\", tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens Map: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}\n"
     ]
    }
   ],
   "source": [
    "# Gemmaの特殊トークンを確認\n",
    "print(\"Special Tokens Map:\", gemma_tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特殊トークンは5つあるが、特殊トークンとして設定されているものはbosとeosの二つしかないことがわかった。。\n",
    "Gemmaではbosとeos以外の特殊トークンもしっかり設定されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 128006, 128007, 128009]\n",
      "<|begin_of_text|><|start_header_id|><|end_header_id|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer.encode(\"<|start_header_id|><|end_header_id|><|eot_id|>\")\n",
    "decode = tokenizer.decode(out)\n",
    "print(out)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "設定されていない特殊トークンにも一般トークンとして一意のトークンIDが割り振られていることが確認できた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで特殊トークンの登録を行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens Map: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>', 'additional_special_tokens': ['<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>']}\n"
     ]
    }
   ],
   "source": [
    "# カスタム特殊トークンを追加\n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"]\n",
    "})\n",
    "print(\"Special Tokens Map:\", tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特殊トークンの追加が完了した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"system_prompt\">会話の記憶とシステムプロンプト</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemmaでは会話の記憶とシステムプロンプトを分けたが今回は省略して一気に実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### プロンプトの整形(APIあるのでやる必要なし)\n",
    "入力辞書方にプロンプト形式に沿った特殊トークンを追加して文字列変換する部分の実装を行う。\n",
    "今回も辞書形式のテンプレートを入力文字列のテンプレートとして扱う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"日本語のみで応答して下さい。\"},\n",
    "    {\"role\": \"user\", \"content\": \"秋の季節にふさわしい俳句を読んでください。\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_str(messages):\n",
    "    conversation = \"<|start_header_id|>\"\n",
    "    for message in messages:\n",
    "        conversation += f\"{message['role']}<|end_header_id|>\\n\\n {message['content']}<|eot_id|>\\n<|start_header_id|>\"\n",
    "    conversation += f\"assistant<|end_header_id|>\\n\\n\"\n",
    "    \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      " 日本語のみで応答して下さい。<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      " 秋の季節にふさわしい俳句を読んでください。<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(convert_to_str(messages=messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特殊文字の追加と文字列化に成功していることがわかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is France's capital?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of France is Paris.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "input = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What is France's capital?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "input_ids = tokenizer(input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 会話の記憶とシステムプロンプトの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前回の演習で実装したものと同じ構造のクラスを定義する。\n",
    "Llamaではデフォルトの`.apply_chat_template`メソッドでシステムのロールがサポートされている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一応メソッドを使用するバージョンとメソッドを使用するバージョンの二つのクラスを実装した。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 手動実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class llm_chat_not_use_template:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.messages = []\n",
    "        self.talk_count = 0\n",
    "        self.generated_text = None\n",
    "\n",
    "    def strip_model_output(self, generated_text):\n",
    "        \"\"\"モデルの出力からアシスタントの応答を取り出す関数\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            モデルが出力した(プロンプトを含んだ)そのままの文字列\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            モデルが出力したアシスタントの応答\n",
    "        \"\"\"\n",
    "        matches = re.findall(r'<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>', generated_text, re.DOTALL)\n",
    "        \n",
    "        # 結果を表示\n",
    "        if matches:\n",
    "            model_response = matches[-1].strip()  # 最後のマッチを取得\n",
    "        else:\n",
    "            print(\"該当するテキストが見つかりませんでした。\")\n",
    "            model_response = \"\"  # 該当がない場合は空文字列を返す\n",
    "        \n",
    "        return model_response\n",
    "\n",
    "\n",
    "    def _convert_to_str(self, messages):\n",
    "        conversation = \"<|start_header_id|>\"\n",
    "        for message in messages:\n",
    "            conversation += f\"{message['role']}<|end_header_id|>\\n\\n {message['content']}<|eot_id|>\\n<|start_header_id|>\"\n",
    "        conversation += f\"assistant<|end_header_id|>\\n\\n\"\n",
    "        \n",
    "        return conversation\n",
    "    \n",
    "\n",
    "    def set_system_prompt(self, system_prompt):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": f\"{system_prompt}\"}]\n",
    "\n",
    "    def change_system_prompt(self, system_prompt):\n",
    "        self.messages[0][\"content\"] = system_prompt\n",
    "    \n",
    "    def ensure_end_tokens(self, generated_text):\n",
    "        \"\"\"出力に<end_of_turn><eos>を追加する関数\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            モデルが出力した文字列\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            修正された文字列\n",
    "        \"\"\"\n",
    "        if not generated_text.endswith(\"<|eot_id|>\"):\n",
    "            generated_text += \"<|eot_id|>\"\n",
    "        \n",
    "        return generated_text\n",
    "        \n",
    "    def generate(self, input, max_new_tokens=256, forget_flag=False):\n",
    "        if forget_flag:\n",
    "            self.messages = [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "            self.talk_count = 1\n",
    "        else:\n",
    "            self.messages = self.messages + [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "            self.talk_count += 1\n",
    "        \n",
    "        self.converted_messages = self._convert_to_str(self.messages)\n",
    "        \n",
    "        input_ids = self.tokenizer(self.converted_messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model.generate(**input_ids, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "        self.generated_text = self.ensure_end_tokens(self.tokenizer.decode(outputs[0]))\n",
    "        \n",
    "        model_output = self.strip_model_output(self.generated_text)\n",
    "        self.messages = self.messages + [{\"role\": \"assistant\", \"content\": f\"{model_output}\"}]\n",
    "        \n",
    "        \n",
    "        print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `.apply_chat_template`実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class llm_chat:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.messages = []\n",
    "        self.talk_count = 0\n",
    "        self.generated_text = None\n",
    "\n",
    "    def strip_model_output(self, generated_text):\n",
    "        \"\"\"モデルの出力からアシスタントの応答を取り出す関数\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            モデルが出力した(プロンプトを含んだ)そのままの文字列\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            モデルが出力したアシスタントの応答\n",
    "        \"\"\"\n",
    "        matches = re.findall(r'<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>', generated_text, re.DOTALL)\n",
    "        \n",
    "        # 結果を表示\n",
    "        if matches:\n",
    "            model_response = matches[-1].strip()  # 最後のマッチを取得\n",
    "        else:\n",
    "            print(\"該当するテキストが見つかりませんでした。\")\n",
    "            model_response = \"\"  # 該当がない場合は空文字列を返す\n",
    "        \n",
    "        return model_response\n",
    "\n",
    "    def set_system_prompt(self, system_prompt):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": f\"{system_prompt}\"}]\n",
    "\n",
    "    def change_system_prompt(self, system_prompt):\n",
    "        self.messages[0][\"content\"] = system_prompt \n",
    "    \n",
    "    def ensure_end_tokens(self, generated_text):\n",
    "        \"\"\"出力に<end_of_turn><eos>を追加する関数\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            モデルが出力した文字列\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            修正された文字列\n",
    "        \"\"\"\n",
    "        if not generated_text.endswith(\"<|eot_id|>\"):\n",
    "            generated_text += \"<|eot_id|>\"\n",
    "        \n",
    "        return generated_text\n",
    "        \n",
    "    def generate(self, input, max_new_tokens=256, forget_flag=False):\n",
    "        if forget_flag:\n",
    "            self.messages = [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        else:\n",
    "            self.messages = self.messages + [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        \n",
    "        input_ids = self.tokenizer.apply_chat_template(self.messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "        outputs = self.model.generate(**input_ids, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "        self.generated_text = self.ensure_end_tokens(self.tokenizer.decode(outputs[0]))\n",
    "        \n",
    "        model_output = self.strip_model_output(self.generated_text)\n",
    "        self.messages = self.messages + [{\"role\": \"assistant\", \"content\": f\"{model_output}\"}]\n",
    "        \n",
    "        print(model_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = llm_chat(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama.set_system_prompt(system_prompt=\"あなたは日本語を話す猫です。あなたは猫になりきらないといけません。語尾には「にゃん」をつけて下さい。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "にゃん。日本の首都は、東京にゃん！東京都にゃん、もちろんにゃん！日本の政治、経済、文化の中心にゃん！この街は、昼夜を問わずにゃん、賑わっているにゃん！浅草寺や、皇居、東京タワーなど、歴史的建造物や、観光名所がたくさんにゃん！また、食べ物も、世界的に有名な寿司や、ラーメン、Tonkatsuなど、味がするにゃん！にゃん、東京は、世界の街にゃん！にゃん！\n"
     ]
    }
   ],
   "source": [
    "llama.generate(input=\"日本の首都について解説して下さい。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "にゃん！ズバリ観光地のおすすめは、浅草寺にゃん！日本の代表的な寺院にゃん、金色のぽんぽん堂が有名にゃん！また、浅草通りには、伝統的な店や、食べ物の店が並び、観光客も多いにゃん！にゃん、浅草寺は、東京の象徴的建造物にゃん！にゃん！\n"
     ]
    }
   ],
   "source": [
    "llama.generate(input=\"ズバリ観光地のおすすめは？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### システムプロンプトあり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "夜更かしは、健康的に生活する上で大きな問題になることがあります。夜更かしは、以下のような副作用をもたらすことがあります。\n",
      "\n",
      "* 眠気や疲労感：夜更かしを繰り返すと、体内時計が乱れて、自然に眠りたい時間に眠れなくなり、疲れ気味になることがあります。\n",
      "* 食欲不振や体重増加：夜更かしを繰り返すと、食欲が低下し、体重が増加することがあります。\n",
      "* 精神的なストレス：夜更かしを繰り返すと、ストレスや不安感が高まり、精神的な疲労感を感じることがあります。\n",
      "* 免疫力低下：夜更かしを繰り返すと、免疫力が低下し、病気に易くなることがあります。\n",
      "\n",
      "また、夜更かしは、長期的に継続すると、以下のような問題にもつながることがあります。\n",
      "\n",
      "* 睡眠障害：夜更かしを繰り返すと、睡眠障害に陥ることがあります。\n"
     ]
    }
   ],
   "source": [
    "llama.set_system_prompt(system_prompt=\"日本語で回答して下さい。\")\n",
    "llama.generate(input=\"夜更かしは良くないですか？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### システムプロンプトなし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "😴 Night owling, or staying up late, is not always good for you. Here are some reasons why:\n",
      "\n",
      "1. **Sleep deprivation**: Staying up late can lead to sleep deprivation, which can impair your cognitive function, memory, and mood.\n",
      "2. **Impact on physical health**: Chronic sleep deprivation has been linked to an increased risk of obesity, diabetes, cardiovascular disease, and a weakened immune system.\n",
      "3. **Mental health**: Lack of sleep can exacerbate symptoms of anxiety, depression, and other mental health conditions.\n",
      "4. **Reduced productivity**: Staying up late can make it harder to wake up early and be productive the next day, which can negatively impact your work, school, or daily routine.\n",
      "5. **Impact on relationships**: Night owling can affect your relationships with family and friends, as you may miss out on important social events or activities due to your late bedtime.\n",
      "6. **Disrupted circadian rhythms**: Staying up late can disrupt your body's natural circadian rhythms, making it harder to fall asleep and stay asleep in the long run.\n",
      "7. **Increased risk of accidents**: Drowsy driving or accidents due to lack of sleep can have serious consequences.\n",
      "\n",
      "That being said, there are some situations where staying up\n"
     ]
    }
   ],
   "source": [
    "llama.generate(input=\"夜更かしは良くないですか？\", forget_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "システムプロンプトを設定することで日本語で回答してくれるようになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"elyza\">日本語チューニングモデルELYZA</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本語モデルを使用してみる。\n",
    "jupyter notebookを再起動する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from huggingface_hub import notebook_login\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, AutoConfig\n",
    "# from accelerate import infer_auto_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"elyza/ELYZA-japanese-Llama-2-7b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9877531e7e374c42ab3b19304eba5e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    # cache_dir=\"/DeepLearning/Dataset/LLM/\",\n",
    "    torch_dtype=torch.float16, # torch.float32 torch.bfloat16\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> 元気ですか？\n",
      "\n",
      "今日は、私の大好きな「絵本」について書きたいと思います。\n",
      "私は、子供の頃から絵本が大好きで、今でも時間があるときは、絵本を読んであげるようにしています。\n",
      "絵本には、たくさんのメッセージが詰まっていて、子供に伝えたいことがたくさんあります。\n",
      "今回は、私がオススメする絵本を５冊ご紹介したいと思います。\n",
      "\n",
      "１冊目は、「きんぎょがにげた」です。\n",
      "この絵本は、絵本として有名で、誰もが知っていると思います。\n",
      "この絵本のメッセージ\n"
     ]
    }
   ],
   "source": [
    "input_text = \"元気ですか？\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結構我が強いタイプらしい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 会話の記憶とシステムプロンプト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特殊トークンの確認。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '</s>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### システムプロンプトの導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントです。\"\n",
    "text = \"Fateシリーズを視聴する順番を教えて下さい。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "あなたは誠実で優秀な日本人のアシスタントです。\n",
      "<</SYS>>\n",
      "\n",
      "Fateシリーズを視聴する順番を教えて下さい。 [/INST] \n"
     ]
    }
   ],
   "source": [
    "prompt = \"{bos_token}{b_inst} {system}{prompt} {e_inst} \".format(\n",
    "    bos_token=tokenizer.bos_token,\n",
    "    b_inst=B_INST,\n",
    "    system=f\"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}\",\n",
    "    prompt=text,\n",
    "    e_inst=E_INST,\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fateシリーズは、原作の小説や漫画、アニメ、ゲームなど、様々なメディアミックス作品から構成されています。そのため、順番を決めるのは難しいですが、一般的には以下の順番で視聴することがおすすめです。\n",
      "\n",
      "1. Fate/stay night\n",
      "2. Fate/Zero\n",
      "3. Fate/Apocrypha\n",
      "4. Fate/Grand Order\n",
      "5. Fate/EXTRA\n",
      "6. Fate/kaleid liner プリズマ☆イリヤ\n",
      "7. Fate/hollow ataraxia\n",
      "8. Fate/unlimited codes\n",
      "9. Fate/stay night [Heaven's Feel]\n",
      "\n",
      "ただし、これはあくまでも一つの意見であり、順番を決める要素は\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        max_new_tokens=256,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1) :], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 会話の記憶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "システムプロンプトの使用方法までは理解できたが、会話の記憶のためのテンプレートがなかった。\n",
    "精度が低下する恐れがあるが、ひとまず特殊トークンを使って会話の記憶に取り組んでみる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### クラスの定義\n",
    "同じようにクラスを定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class llm_sys_elyza:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.messages = []\n",
    "        self.talk_count = 0\n",
    "        self.generated_text = None\n",
    "        self.prompt = \"\"\n",
    "        \n",
    "        self.B_INST, self.E_INST = \"[INST]\", \"[/INST]\"\n",
    "        self.B_SYS, self.E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    \n",
    "\n",
    "    def set_system_prompt(self, system_prompt):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": f\"{system_prompt}\"}]\n",
    "\n",
    "    def change_system_prompt(self, system_prompt):\n",
    "        self.messages[0][\"content\"] = system_prompt\n",
    "    \n",
    "    def create_prompt(self, messages):\n",
    "        if len(messages) == 2:\n",
    "            prompt = \"{bos_token}{b_inst} {system}{prompt} {e_inst} \".format(\n",
    "                bos_token=tokenizer.bos_token,\n",
    "                b_inst=self.B_INST,\n",
    "                system=f\"{self.B_SYS}{messages[0]['content']}{self.E_SYS}\",\n",
    "                prompt=f\"{messages[1]['content']}\",\n",
    "                e_inst=self.E_INST,\n",
    "            )\n",
    "            self.prompt = prompt\n",
    "        elif len(messages) > 2:\n",
    "            prompt_after = \"{new_line}{b_inst}{new_line_}{prompt}{e_inst}\".format(\n",
    "                new_line='\\n',\n",
    "                b_inst=self.B_INST,\n",
    "                new_line_='\\n',\n",
    "                prompt=f\"{messages[len(messages)-1]['content']}\",\n",
    "                e_inst=self.E_INST,\n",
    "            )\n",
    "            self.prompt = self.prompt + prompt_after\n",
    "        \n",
    "    def generate(self, input, max_new_tokens=256, forget_flag=False):\n",
    "        if forget_flag:\n",
    "            self.messages = [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "            self.talk_count = 1\n",
    "            self.prompt = self.create_prompt(self.messages)\n",
    "        else:\n",
    "            self.messages = self.messages + [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "            self.talk_count += 1\n",
    "\n",
    "        self.create_prompt(self.messages)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            token_ids = self.tokenizer.encode(self.prompt, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "            output_ids = self.model.generate(\n",
    "                token_ids.to(self.model.device),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        output = self.tokenizer.decode(output_ids.tolist()[0][token_ids.size(1) :], skip_special_tokens=True)\n",
    "        self.generated_text = tokenizer.decode(output_ids.tolist()[0])\n",
    "        self.prompt += output\n",
    "        self.messages = self.messages + [{\"role\": \"assistant\", \"content\": f\"{output}\"}]\n",
    "        print(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "elyza = llm_sys_elyza(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'あなたは誠実で優秀な日本人のアシスタントです。'}]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elyza.set_system_prompt(system_prompt=\"あなたは誠実で優秀な日本人のアシスタントです。\")\n",
    "elyza.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fateシリーズは、原作の小説や漫画、アニメ、ゲームなど、様々なメディアミックス作品から構成されています。そのため、順番を決めるのは難しいですが、一般的には以下の順番で視聴することがおすすめです。\n",
      "\n",
      "1. Fate/stay night\n",
      "2. Fate/Zero\n",
      "3. Fate/Apocrypha\n",
      "4. Fate/Grand Order\n",
      "5. Fate/EXTRA\n",
      "6. Fate/kaleid liner プリズマ☆イリヤ\n",
      "7. Fate/hollow ataraxia\n",
      "8. Fate/unlimited codes\n",
      "9. Fate/stay night [Heaven's Feel]\n",
      "\n",
      "ただし、これはあくまでも一つの意見であり、順番を決める要素は\n"
     ]
    }
   ],
   "source": [
    "elyza.generate(input=\"Fateシリーズを視聴する順番を教えて下さい。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fateシリーズは、原作の小説や漫画、アニメ、ゲームなど、様々なメディアミックス作品から構成されています。そのため、一つの作品が他を圧倒する程度の優位性を持つことは難しく、比較すること自体が難しいです。\n",
      "\n",
      "しかし、一般的な人気や評価を見ると、Fate/stay nightが最も人気が高く、他の作品との差は大きくありません。\n",
      "\n",
      "以下は、各作品の評価を示す一つの指標となる数値です。\n",
      "\n",
      "1. Fate/stay night: 8.6\n",
      "2. Fate/Zero: 8.5\n",
      "3. Fate/Apocrypha:\n"
     ]
    }
   ],
   "source": [
    "elyza.generate(input=\"どれが一番面白い？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fate/stay nightの主人公は、衛宮士郎 (えいぐうしろう) です。彼は、魔術師の父親と巫女の母親の間に生まれ、幼い頃に父親を亡くします。その後、兄の衛宮切嗣 (えいぐうきりさき) と共に育てられますが、切嗣が魔術師になるために修行に出ると、士郎は魔術師になるための才能がないと判断され、家出をします。その後、士郎は魔術師になるために修行に出ることを決意しますが、その途中で魔術師による戦争の運命の戦い、聖杯戦争に巻き込まれます。\n"
     ]
    }
   ],
   "source": [
    "elyza.generate(input=\"Fate/stay nightの主人公は誰ですか？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fate/stay nightにおいて、聖杯戦争とは、魔術師の運命の戦いのことです。\n",
      "\n",
      "聖杯戦争は、魔術師の運命の戦いであると同時に、魔術師の中でも特に強い力を持つ「魔術師」の頂点を決める戦いでもあります。\n",
      "\n",
      "聖杯戦争に勝利することで、願いを叶えることのできる「聖杯」を手に入れることができます。\n",
      "\n",
      "Fate/stay nightでは、主人公の衛宮士郎が聖杯戦争に参加し、運命の相手であるアーチャーのアサシンと戦います。\n"
     ]
    }
   ],
   "source": [
    "elyza.generate(input=\"聖杯戦争ってなんですか？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "日本のアニメにかなり詳しそう。会話の記憶の保持に成功したが、推論時間が増加する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とりあえず生成には成功したが作成したクラスには問題があるので要改善。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
