{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 id=\"title\">LLMãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« 1-2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"intro\">æ¦‚è¦</h2>\n",
    "LLMãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«1ã®LLaMaãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€‚LLaMa3ãŠã‚ˆã³ELYZA-japanese-Llama-2-7b(LLaMa2ã‚’ãƒ™ãƒ¼ã‚¹ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸæ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«)ã‚’ä½¿ç”¨ã€‚\n",
    "- [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
    "- [ELYZA-japanese-Llama-2-7b](https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b)\n",
    "\n",
    "**ç›®æ¬¡**\n",
    "\n",
    "- [LLMãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ« 1-2](#title)\n",
    "  - [æ¦‚è¦](#intro)\n",
    "  - [èª­ã¿è¾¼ã¿ã¨ãƒˆãƒ¼ã‚¯ãƒ³å‰å‡¦ç†](#inport_load)\n",
    "  - [ä¼šè©±ã®è¨˜æ†¶ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ](#system_prompt)\n",
    "  - [æ—¥æœ¬èªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ELYZA](#elyza)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"inport_load\">èª­ã¿è¾¼ã¿ã¨ãƒˆãƒ¼ã‚¯ãƒ³å‰å‡¦ç†</h2>\n",
    "ãƒ¢ãƒ‡ãƒ«ãŠã‚ˆã³ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ã€å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å¤‰æ›´ãƒ»ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯ãƒªãƒ³ã‚¯ä½œæˆã«ã¤ã„ã¦ã¯ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«1ã‚’å‚ç…§ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from huggingface_hub import notebook_login\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, AutoConfig\n",
    "# from accelerate import infer_auto_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipelineã‚’ç”¨ã„ãŸãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n",
    "Gemmaã§ã¯pipelineã‚’ä½¿ç”¨ã—ãªã‹ã£ãŸãŒã€LLaMaã®ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã§ã¯pipelineã‚’ä½¿ç”¨ã—ãŸã‚³ãƒ¼ãƒ‰ãŒç´¹ä»‹ã•ã‚Œã¦ã„ã‚‹ã€‚\n",
    "ã—ãŸãŒã£ã¦ã¾ãšã¯ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã«å¾“ã£ã¦pipelineã‚’ç”¨ã„ã¦å®Ÿè£…ã—ã¦ã¿ã‚‹ã€‚\n",
    "pipelineã¯ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã‚„ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æ„è­˜ã›ãšã«NLPã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹é«˜ãƒ¬ãƒ™ãƒ«APIã§ã€æ¨è«–ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’ä¸€æ°—ã«è¡Œã†ã“ã¨ãŒã§ãã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96217662e824c5c9bb87bcda3513b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name, \n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Arrrr, me hearty! Me name be Captain Chat, the scurviest pirate chatbot to ever sail the Seven Seas o' Conversation! Me and me trusty keyboard be here to swab the decks o' yer mind with me wit and me wisdom, savvy? So hoist the colors, me hearty, and let's set sail fer a swashbucklin' good time!\"}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã“ã§ä¸€åº¦å†èµ·å‹•ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã¦ä¸‹ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipelineã‚’ä½¿ç”¨ã—ãªã„ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd90bd0cb104f5dae40754cb545976a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    # cache_dir=\"/DeepLearning/Dataset/LLM/\",\n",
    "    torch_dtype=torch.float16, # torch.float32 torch.bfloat16\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Hey how are you doing today? I hope you're having a fantastic day! I'm feeling a bit under the weather today, but I'm hoping that a nice cup of tea and a bit of a nap will help me feel better. How about you, are you feeling okay?\n",
      "I've been thinking about our conversation from yesterday, and I wanted to follow up on a few things. I'm really interested in learning more about your experiences with [topic]. Can you tell me a bit more about that?\n",
      "Also, I was wondering if you have any recommendations for [related topic]. I'm always looking for new ideas and perspectives, and I value your opinion.\n",
      "Thanks so much for your time, and I hope you have a great day! Take care!<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Hey how are you doing today?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemmaã¨ã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸ãˆæ–¹ãŒç•°ãªã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚\n",
    "Llamaã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¸ãˆæ–¹ã®è©³ç´°ã¯[ã“ã“](https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/)ã«è¼‰ã£ã¦ã„ã‚‹ã€‚\n",
    "ã¾ãšLlamaã«ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒã‚ã‚‹ã‹ã©ã†ã‹ç¢ºèªã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºèªã¨è¿½åŠ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 24186, 95221, 112130, 11571]\n",
      "<|begin_of_text|>å…ƒæ°—ã§ã™ã‹ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer.encode(\"å…ƒæ°—ã§ã™ã‹ï¼Ÿ\")\n",
    "decode = tokenizer.decode(out)\n",
    "print(out)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒãªã„ã“ã¨ãŒã‚ã‹ã‚‹ã€‚\n",
    "Gemmaã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã§è©¦ã—ã¦ã¿ã‚‹ã¨Gemmaã§ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«Jinja templateã®ã‚ˆã†ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "gemma_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\")\n",
    "print(gemma_tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã²ã¨ã¾ãšãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰çµæœã‚’è¦‹ã¦ã¿ã‚‹ã€‚\n",
    "[Llamaå®Ÿè£…](https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py#L222)ã‚’è¦‹ã‚‹ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ã®ç”ŸæˆãŒã§ãã‚‹ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚‹ã£ã½ã„ãŒã€ä½¿ç”¨æ–¹æ³•ãŒè§£èª­ã§ããªã„ã€‚\n",
    "<details>\n",
    "    <summary>å®Ÿè£…(ä¸€éƒ¨æ”¹å¤‰)</summary>\n",
    "\n",
    "```python\n",
    "class ChatFormat:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def encode_header(self, message):\n",
    "        tokens = []\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|start_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(message[\"role\"], bos=False, eos=False))\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|end_header_id|>\"])\n",
    "        tokens.extend(self.tokenizer.encode(\"\\n\\n\", bos=False, eos=False))\n",
    "        return tokens\n",
    "\n",
    "    def encode_message(self, message):\n",
    "        tokens = self.encode_header(message)\n",
    "        tokens.extend(\n",
    "            self.tokenizer.encode(message[\"content\"].strip(), bos=False, eos=False)\n",
    "        )\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|eot_id|>\"])\n",
    "        return tokens\n",
    "\n",
    "    def encode_dialog_prompt(self, dialog):\n",
    "        tokens = []\n",
    "        tokens.append(self.tokenizer.special_tokens[\"<|begin_of_text|>\"])\n",
    "        for message in dialog:\n",
    "            tokens.extend(self.encode_message(message))\n",
    "        # Add the start of an assistant message for the model to complete.\n",
    "        tokens.extend(self.encode_header({\"role\": \"assistant\", \"content\": \"\"}))\n",
    "        return tokens\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å…¬å¼ã®è³‡æ–™ã‚ˆã‚Šã€Llamaã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ã«ã¯`<|begin_of_text|>`,`<|start_header_id|>`,`<|end_header_id|>`,`<|eot_id|>`,`<|end_of_text|>`ã®5ã¤ã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚ã‚‹ã¨ã‚ã‹ã£ãŸã€‚\n",
    "ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¢ºèªã—ã¦ã¿ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens Map: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>'}\n"
     ]
    }
   ],
   "source": [
    "# Llamaã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¢ºèª\n",
    "print(\"Special Tokens Map:\", tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens Map: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}\n"
     ]
    }
   ],
   "source": [
    "# Gemmaã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¢ºèª\n",
    "print(\"Special Tokens Map:\", gemma_tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯5ã¤ã‚ã‚‹ãŒã€ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã¯bosã¨eosã®äºŒã¤ã—ã‹ãªã„ã“ã¨ãŒã‚ã‹ã£ãŸã€‚ã€‚\n",
    "Gemmaã§ã¯bosã¨eosä»¥å¤–ã®ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚‚ã—ã£ã‹ã‚Šè¨­å®šã•ã‚Œã¦ã„ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 128006, 128007, 128009]\n",
      "<|begin_of_text|><|start_header_id|><|end_header_id|><|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "out = tokenizer.encode(\"<|start_header_id|><|end_header_id|><|eot_id|>\")\n",
    "decode = tokenizer.decode(out)\n",
    "print(out)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¨­å®šã•ã‚Œã¦ã„ãªã„ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã«ã‚‚ä¸€èˆ¬ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ä¸€æ„ã®ãƒˆãƒ¼ã‚¯ãƒ³IDãŒå‰²ã‚ŠæŒ¯ã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ããŸã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã“ã“ã§ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®ç™»éŒ²ã‚’è¡Œã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special Tokens Map: {'bos_token': '<|begin_of_text|>', 'eos_token': '<|eot_id|>', 'additional_special_tokens': ['<|start_header_id|>', '<|end_header_id|>', '<|eot_id|>']}\n"
     ]
    }
   ],
   "source": [
    "# ã‚«ã‚¹ã‚¿ãƒ ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ \n",
    "tokenizer.add_special_tokens({\n",
    "    \"additional_special_tokens\": [\"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\"]\n",
    "})\n",
    "print(\"Special Tokens Map:\", tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®è¿½åŠ ãŒå®Œäº†ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"system_prompt\">ä¼šè©±ã®è¨˜æ†¶ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemmaã§ã¯ä¼šè©±ã®è¨˜æ†¶ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ†ã‘ãŸãŒä»Šå›ã¯çœç•¥ã—ã¦ä¸€æ°—ã«å®Ÿè£…ã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ•´å½¢(APIã‚ã‚‹ã®ã§ã‚„ã‚‹å¿…è¦ãªã—)\n",
    "å…¥åŠ›è¾æ›¸æ–¹ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ã«æ²¿ã£ãŸç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ ã—ã¦æ–‡å­—åˆ—å¤‰æ›ã™ã‚‹éƒ¨åˆ†ã®å®Ÿè£…ã‚’è¡Œã†ã€‚\n",
    "ä»Šå›ã‚‚è¾æ›¸å½¢å¼ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å…¥åŠ›æ–‡å­—åˆ—ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ã—ã¦æ‰±ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"æ—¥æœ¬èªã®ã¿ã§å¿œç­”ã—ã¦ä¸‹ã•ã„ã€‚\"},\n",
    "    {\"role\": \"user\", \"content\": \"ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_str(messages):\n",
    "    conversation = \"<|start_header_id|>\"\n",
    "    for message in messages:\n",
    "        conversation += f\"{message['role']}<|end_header_id|>\\n\\n {message['content']}<|eot_id|>\\n<|start_header_id|>\"\n",
    "    conversation += f\"assistant<|end_header_id|>\\n\\n\"\n",
    "    \n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>system<|end_header_id|>\n",
      "\n",
      " æ—¥æœ¬èªã®ã¿ã§å¿œç­”ã—ã¦ä¸‹ã•ã„ã€‚<|eot_id|>\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      " ç§‹ã®å­£ç¯€ã«ãµã•ã‚ã—ã„ä¿³å¥ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚<|eot_id|>\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(convert_to_str(messages=messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç‰¹æ®Šæ–‡å­—ã®è¿½åŠ ã¨æ–‡å­—åˆ—åŒ–ã«æˆåŠŸã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is France's capital?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of France is Paris.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "input = \"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "What is France's capital?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "input_ids = tokenizer(input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256, eos_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¼šè©±ã®è¨˜æ†¶ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‰å›ã®æ¼”ç¿’ã§å®Ÿè£…ã—ãŸã‚‚ã®ã¨åŒã˜æ§‹é€ ã®ã‚¯ãƒ©ã‚¹ã‚’å®šç¾©ã™ã‚‹ã€‚\n",
    "Llamaã§ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®`.apply_chat_template`ãƒ¡ã‚½ãƒƒãƒ‰ã§ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ­ãƒ¼ãƒ«ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸€å¿œãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®äºŒã¤ã®ã‚¯ãƒ©ã‚¹ã‚’å®Ÿè£…ã—ãŸã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### æ‰‹å‹•å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class llm_chat_not_use_template:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.messages = []\n",
    "        self.talk_count = 0\n",
    "        self.generated_text = None\n",
    "\n",
    "    def strip_model_output(self, generated_text):\n",
    "        \"\"\"ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‹ã‚‰ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’å–ã‚Šå‡ºã™é–¢æ•°\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸ(ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚“ã )ãã®ã¾ã¾ã®æ–‡å­—åˆ—\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”\n",
    "        \"\"\"\n",
    "        matches = re.findall(r'<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>', generated_text, re.DOTALL)\n",
    "        \n",
    "        # çµæœã‚’è¡¨ç¤º\n",
    "        if matches:\n",
    "            model_response = matches[-1].strip()  # æœ€å¾Œã®ãƒãƒƒãƒã‚’å–å¾—\n",
    "        else:\n",
    "            print(\"è©²å½“ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "            model_response = \"\"  # è©²å½“ãŒãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™\n",
    "        \n",
    "        return model_response\n",
    "\n",
    "\n",
    "    def _convert_to_str(self, messages):\n",
    "        conversation = \"<|start_header_id|>\"\n",
    "        for message in messages:\n",
    "            conversation += f\"{message['role']}<|end_header_id|>\\n\\n {message['content']}<|eot_id|>\\n<|start_header_id|>\"\n",
    "        conversation += f\"assistant<|end_header_id|>\\n\\n\"\n",
    "        \n",
    "        return conversation\n",
    "    \n",
    "\n",
    "    def set_system_prompt(self, system_prompt):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": f\"{system_prompt}\"}]\n",
    "\n",
    "    def change_system_prompt(self, system_prompt):\n",
    "        self.messages[0][\"content\"] = system_prompt\n",
    "    \n",
    "    def ensure_end_tokens(self, generated_text):\n",
    "        \"\"\"å‡ºåŠ›ã«<end_of_turn><eos>ã‚’è¿½åŠ ã™ã‚‹é–¢æ•°\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸæ–‡å­—åˆ—\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            ä¿®æ­£ã•ã‚ŒãŸæ–‡å­—åˆ—\n",
    "        \"\"\"\n",
    "        if not generated_text.endswith(\"<|eot_id|>\"):\n",
    "            generated_text += \"<|eot_id|>\"\n",
    "        \n",
    "        return generated_text\n",
    "        \n",
    "    def generate(self, input, max_new_tokens=256, forget_flag=False):\n",
    "        if forget_flag:\n",
    "            self.messages = [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "            self.talk_count = 1\n",
    "        else:\n",
    "            self.messages = self.messages + [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "            self.talk_count += 1\n",
    "        \n",
    "        self.converted_messages = self._convert_to_str(self.messages)\n",
    "        \n",
    "        input_ids = self.tokenizer(self.converted_messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = self.model.generate(**input_ids, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "        self.generated_text = self.ensure_end_tokens(self.tokenizer.decode(outputs[0]))\n",
    "        \n",
    "        model_output = self.strip_model_output(self.generated_text)\n",
    "        self.messages = self.messages + [{\"role\": \"assistant\", \"content\": f\"{model_output}\"}]\n",
    "        \n",
    "        \n",
    "        print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `.apply_chat_template`å®Ÿè£…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class llm_chat:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.messages = []\n",
    "        self.talk_count = 0\n",
    "        self.generated_text = None\n",
    "\n",
    "    def strip_model_output(self, generated_text):\n",
    "        \"\"\"ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‹ã‚‰ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”ã‚’å–ã‚Šå‡ºã™é–¢æ•°\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸ(ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å«ã‚“ã )ãã®ã¾ã¾ã®æ–‡å­—åˆ—\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å¿œç­”\n",
    "        \"\"\"\n",
    "        matches = re.findall(r'<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>', generated_text, re.DOTALL)\n",
    "        \n",
    "        # çµæœã‚’è¡¨ç¤º\n",
    "        if matches:\n",
    "            model_response = matches[-1].strip()  # æœ€å¾Œã®ãƒãƒƒãƒã‚’å–å¾—\n",
    "        else:\n",
    "            print(\"è©²å½“ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "            model_response = \"\"  # è©²å½“ãŒãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™\n",
    "        \n",
    "        return model_response\n",
    "\n",
    "    def set_system_prompt(self, system_prompt):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": f\"{system_prompt}\"}]\n",
    "\n",
    "    def change_system_prompt(self, system_prompt):\n",
    "        self.messages[0][\"content\"] = system_prompt \n",
    "    \n",
    "    def ensure_end_tokens(self, generated_text):\n",
    "        \"\"\"å‡ºåŠ›ã«<end_of_turn><eos>ã‚’è¿½åŠ ã™ã‚‹é–¢æ•°\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        generated_text : str\n",
    "            ãƒ¢ãƒ‡ãƒ«ãŒå‡ºåŠ›ã—ãŸæ–‡å­—åˆ—\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            ä¿®æ­£ã•ã‚ŒãŸæ–‡å­—åˆ—\n",
    "        \"\"\"\n",
    "        if not generated_text.endswith(\"<|eot_id|>\"):\n",
    "            generated_text += \"<|eot_id|>\"\n",
    "        \n",
    "        return generated_text\n",
    "        \n",
    "    def generate(self, input, max_new_tokens=256, forget_flag=False):\n",
    "        if forget_flag:\n",
    "            self.messages = [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        else:\n",
    "            self.messages = self.messages + [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "        \n",
    "        input_ids = self.tokenizer.apply_chat_template(self.messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n",
    "        outputs = self.model.generate(**input_ids, max_new_tokens=max_new_tokens, eos_token_id=tokenizer.eos_token_id)\n",
    "        self.generated_text = self.ensure_end_tokens(self.tokenizer.decode(outputs[0]))\n",
    "        \n",
    "        model_output = self.strip_model_output(self.generated_text)\n",
    "        self.messages = self.messages + [{\"role\": \"assistant\", \"content\": f\"{model_output}\"}]\n",
    "        \n",
    "        print(model_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = llm_chat(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama.set_system_prompt(system_prompt=\"ã‚ãªãŸã¯æ—¥æœ¬èªã‚’è©±ã™çŒ«ã§ã™ã€‚ã‚ãªãŸã¯çŒ«ã«ãªã‚Šãã‚‰ãªã„ã¨ã„ã‘ã¾ã›ã‚“ã€‚èªå°¾ã«ã¯ã€Œã«ã‚ƒã‚“ã€ã‚’ã¤ã‘ã¦ä¸‹ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã«ã‚ƒã‚“ã€‚æ—¥æœ¬ã®é¦–éƒ½ã¯ã€æ±äº¬ã«ã‚ƒã‚“ï¼æ±äº¬éƒ½ã«ã‚ƒã‚“ã€ã‚‚ã¡ã‚ã‚“ã«ã‚ƒã‚“ï¼æ—¥æœ¬ã®æ”¿æ²»ã€çµŒæ¸ˆã€æ–‡åŒ–ã®ä¸­å¿ƒã«ã‚ƒã‚“ï¼ã“ã®è¡—ã¯ã€æ˜¼å¤œã‚’å•ã‚ãšã«ã‚ƒã‚“ã€è³‘ã‚ã£ã¦ã„ã‚‹ã«ã‚ƒã‚“ï¼æµ…è‰å¯ºã‚„ã€çš‡å±…ã€æ±äº¬ã‚¿ãƒ¯ãƒ¼ãªã©ã€æ­´å²çš„å»ºé€ ç‰©ã‚„ã€è¦³å…‰åæ‰€ãŒãŸãã•ã‚“ã«ã‚ƒã‚“ï¼ã¾ãŸã€é£Ÿã¹ç‰©ã‚‚ã€ä¸–ç•Œçš„ã«æœ‰åãªå¯¿å¸ã‚„ã€ãƒ©ãƒ¼ãƒ¡ãƒ³ã€Tonkatsuãªã©ã€å‘³ãŒã™ã‚‹ã«ã‚ƒã‚“ï¼ã«ã‚ƒã‚“ã€æ±äº¬ã¯ã€ä¸–ç•Œã®è¡—ã«ã‚ƒã‚“ï¼ã«ã‚ƒã‚“ï¼\n"
     ]
    }
   ],
   "source": [
    "llama.generate(input=\"æ—¥æœ¬ã®é¦–éƒ½ã«ã¤ã„ã¦è§£èª¬ã—ã¦ä¸‹ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã«ã‚ƒã‚“ï¼ã‚ºãƒãƒªè¦³å…‰åœ°ã®ãŠã™ã™ã‚ã¯ã€æµ…è‰å¯ºã«ã‚ƒã‚“ï¼æ—¥æœ¬ã®ä»£è¡¨çš„ãªå¯ºé™¢ã«ã‚ƒã‚“ã€é‡‘è‰²ã®ã½ã‚“ã½ã‚“å ‚ãŒæœ‰åã«ã‚ƒã‚“ï¼ã¾ãŸã€æµ…è‰é€šã‚Šã«ã¯ã€ä¼çµ±çš„ãªåº—ã‚„ã€é£Ÿã¹ç‰©ã®åº—ãŒä¸¦ã³ã€è¦³å…‰å®¢ã‚‚å¤šã„ã«ã‚ƒã‚“ï¼ã«ã‚ƒã‚“ã€æµ…è‰å¯ºã¯ã€æ±äº¬ã®è±¡å¾´çš„å»ºé€ ç‰©ã«ã‚ƒã‚“ï¼ã«ã‚ƒã‚“ï¼\n"
     ]
    }
   ],
   "source": [
    "llama.generate(input=\"ã‚ºãƒãƒªè¦³å…‰åœ°ã®ãŠã™ã™ã‚ã¯ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚ã‚Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤œæ›´ã‹ã—ã¯ã€å¥åº·çš„ã«ç”Ÿæ´»ã™ã‚‹ä¸Šã§å¤§ããªå•é¡Œã«ãªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚å¤œæ›´ã‹ã—ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå‰¯ä½œç”¨ã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n",
      "\n",
      "* çœ æ°—ã‚„ç–²åŠ´æ„Ÿï¼šå¤œæ›´ã‹ã—ã‚’ç¹°ã‚Šè¿”ã™ã¨ã€ä½“å†…æ™‚è¨ˆãŒä¹±ã‚Œã¦ã€è‡ªç„¶ã«çœ ã‚ŠãŸã„æ™‚é–“ã«çœ ã‚Œãªããªã‚Šã€ç–²ã‚Œæ°—å‘³ã«ãªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n",
      "* é£Ÿæ¬²ä¸æŒ¯ã‚„ä½“é‡å¢—åŠ ï¼šå¤œæ›´ã‹ã—ã‚’ç¹°ã‚Šè¿”ã™ã¨ã€é£Ÿæ¬²ãŒä½ä¸‹ã—ã€ä½“é‡ãŒå¢—åŠ ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n",
      "* ç²¾ç¥çš„ãªã‚¹ãƒˆãƒ¬ã‚¹ï¼šå¤œæ›´ã‹ã—ã‚’ç¹°ã‚Šè¿”ã™ã¨ã€ã‚¹ãƒˆãƒ¬ã‚¹ã‚„ä¸å®‰æ„ŸãŒé«˜ã¾ã‚Šã€ç²¾ç¥çš„ãªç–²åŠ´æ„Ÿã‚’æ„Ÿã˜ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n",
      "* å…ç–«åŠ›ä½ä¸‹ï¼šå¤œæ›´ã‹ã—ã‚’ç¹°ã‚Šè¿”ã™ã¨ã€å…ç–«åŠ›ãŒä½ä¸‹ã—ã€ç—…æ°—ã«æ˜“ããªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n",
      "\n",
      "ã¾ãŸã€å¤œæ›´ã‹ã—ã¯ã€é•·æœŸçš„ã«ç¶™ç¶šã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå•é¡Œã«ã‚‚ã¤ãªãŒã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n",
      "\n",
      "* ç¡çœ éšœå®³ï¼šå¤œæ›´ã‹ã—ã‚’ç¹°ã‚Šè¿”ã™ã¨ã€ç¡çœ éšœå®³ã«é™¥ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "llama.set_system_prompt(system_prompt=\"æ—¥æœ¬èªã§å›ç­”ã—ã¦ä¸‹ã•ã„ã€‚\")\n",
    "llama.generate(input=\"å¤œæ›´ã‹ã—ã¯è‰¯ããªã„ã§ã™ã‹ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãªã—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ˜´ Night owling, or staying up late, is not always good for you. Here are some reasons why:\n",
      "\n",
      "1. **Sleep deprivation**: Staying up late can lead to sleep deprivation, which can impair your cognitive function, memory, and mood.\n",
      "2. **Impact on physical health**: Chronic sleep deprivation has been linked to an increased risk of obesity, diabetes, cardiovascular disease, and a weakened immune system.\n",
      "3. **Mental health**: Lack of sleep can exacerbate symptoms of anxiety, depression, and other mental health conditions.\n",
      "4. **Reduced productivity**: Staying up late can make it harder to wake up early and be productive the next day, which can negatively impact your work, school, or daily routine.\n",
      "5. **Impact on relationships**: Night owling can affect your relationships with family and friends, as you may miss out on important social events or activities due to your late bedtime.\n",
      "6. **Disrupted circadian rhythms**: Staying up late can disrupt your body's natural circadian rhythms, making it harder to fall asleep and stay asleep in the long run.\n",
      "7. **Increased risk of accidents**: Drowsy driving or accidents due to lack of sleep can have serious consequences.\n",
      "\n",
      "That being said, there are some situations where staying up\n"
     ]
    }
   ],
   "source": [
    "llama.generate(input=\"å¤œæ›´ã‹ã—ã¯è‰¯ããªã„ã§ã™ã‹ï¼Ÿ\", forget_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®šã™ã‚‹ã“ã¨ã§æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã‚Œã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h2 id=\"elyza\">æ—¥æœ¬èªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ELYZA</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ—¥æœ¬èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã¿ã‚‹ã€‚\n",
    "jupyter notebookã‚’å†èµ·å‹•ã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from huggingface_hub import notebook_login\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, AutoConfig\n",
    "# from accelerate import infer_auto_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"elyza/ELYZA-japanese-Llama-2-7b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### èª­ã¿è¾¼ã¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9877531e7e374c42ab3b19304eba5e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    # cache_dir=\"/DeepLearning/Dataset/LLM/\",\n",
    "    torch_dtype=torch.float16, # torch.float32 torch.bfloat16\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> å…ƒæ°—ã§ã™ã‹ï¼Ÿ\n",
      "\n",
      "ä»Šæ—¥ã¯ã€ç§ã®å¤§å¥½ããªã€Œçµµæœ¬ã€ã«ã¤ã„ã¦æ›¸ããŸã„ã¨æ€ã„ã¾ã™ã€‚\n",
      "ç§ã¯ã€å­ä¾›ã®é ƒã‹ã‚‰çµµæœ¬ãŒå¤§å¥½ãã§ã€ä»Šã§ã‚‚æ™‚é–“ãŒã‚ã‚‹ã¨ãã¯ã€çµµæœ¬ã‚’èª­ã‚“ã§ã‚ã’ã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚\n",
      "çµµæœ¬ã«ã¯ã€ãŸãã•ã‚“ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè©°ã¾ã£ã¦ã„ã¦ã€å­ä¾›ã«ä¼ãˆãŸã„ã“ã¨ãŒãŸãã•ã‚“ã‚ã‚Šã¾ã™ã€‚\n",
      "ä»Šå›ã¯ã€ç§ãŒã‚ªã‚¹ã‚¹ãƒ¡ã™ã‚‹çµµæœ¬ã‚’ï¼•å†Šã”ç´¹ä»‹ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚\n",
      "\n",
      "ï¼‘å†Šç›®ã¯ã€ã€Œãã‚“ãã‚‡ãŒã«ã’ãŸã€ã§ã™ã€‚\n",
      "ã“ã®çµµæœ¬ã¯ã€çµµæœ¬ã¨ã—ã¦æœ‰åã§ã€èª°ã‚‚ãŒçŸ¥ã£ã¦ã„ã‚‹ã¨æ€ã„ã¾ã™ã€‚\n",
      "ã“ã®çµµæœ¬ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n"
     ]
    }
   ],
   "source": [
    "input_text = \"å…ƒæ°—ã§ã™ã‹ï¼Ÿ\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=256)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "çµæ§‹æˆ‘ãŒå¼·ã„ã‚¿ã‚¤ãƒ—ã‚‰ã—ã„ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¼šè©±ã®è¨˜æ†¶ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®ç¢ºèªã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '</s>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å°å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\"\n",
    "text = \"Fateã‚·ãƒªãƒ¼ã‚ºã‚’è¦–è´ã™ã‚‹é †ç•ªã‚’æ•™ãˆã¦ä¸‹ã•ã„ã€‚\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\n",
      "<</SYS>>\n",
      "\n",
      "Fateã‚·ãƒªãƒ¼ã‚ºã‚’è¦–è´ã™ã‚‹é †ç•ªã‚’æ•™ãˆã¦ä¸‹ã•ã„ã€‚ [/INST] \n"
     ]
    }
   ],
   "source": [
    "prompt = \"{bos_token}{b_inst} {system}{prompt} {e_inst} \".format(\n",
    "    bos_token=tokenizer.bos_token,\n",
    "    b_inst=B_INST,\n",
    "    system=f\"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}\",\n",
    "    prompt=text,\n",
    "    e_inst=E_INST,\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fateã‚·ãƒªãƒ¼ã‚ºã¯ã€åŸä½œã®å°èª¬ã‚„æ¼«ç”»ã€ã‚¢ãƒ‹ãƒ¡ã€ã‚²ãƒ¼ãƒ ãªã©ã€æ§˜ã€…ãªãƒ¡ãƒ‡ã‚£ã‚¢ãƒŸãƒƒã‚¯ã‚¹ä½œå“ã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚ãã®ãŸã‚ã€é †ç•ªã‚’æ±ºã‚ã‚‹ã®ã¯é›£ã—ã„ã§ã™ãŒã€ä¸€èˆ¬çš„ã«ã¯ä»¥ä¸‹ã®é †ç•ªã§è¦–è´ã™ã‚‹ã“ã¨ãŒãŠã™ã™ã‚ã§ã™ã€‚\n",
      "\n",
      "1. Fate/stay night\n",
      "2. Fate/Zero\n",
      "3. Fate/Apocrypha\n",
      "4. Fate/Grand Order\n",
      "5. Fate/EXTRA\n",
      "6. Fate/kaleid liner ãƒ—ãƒªã‚ºãƒâ˜†ã‚¤ãƒªãƒ¤\n",
      "7. Fate/hollow ataraxia\n",
      "8. Fate/unlimited codes\n",
      "9. Fate/stay night [Heaven's Feel]\n",
      "\n",
      "ãŸã ã—ã€ã“ã‚Œã¯ã‚ãã¾ã§ã‚‚ä¸€ã¤ã®æ„è¦‹ã§ã‚ã‚Šã€é †ç•ªã‚’æ±ºã‚ã‚‹è¦ç´ ã¯\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        token_ids.to(model.device),\n",
    "        max_new_tokens=256,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1) :], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä¼šè©±ã®è¨˜æ†¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½¿ç”¨æ–¹æ³•ã¾ã§ã¯ç†è§£ã§ããŸãŒã€ä¼šè©±ã®è¨˜æ†¶ã®ãŸã‚ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒãªã‹ã£ãŸã€‚\n",
    "ç²¾åº¦ãŒä½ä¸‹ã™ã‚‹æã‚ŒãŒã‚ã‚‹ãŒã€ã²ã¨ã¾ãšç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ã£ã¦ä¼šè©±ã®è¨˜æ†¶ã«å–ã‚Šçµ„ã‚“ã§ã¿ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ã‚¯ãƒ©ã‚¹ã®å®šç¾©\n",
    "åŒã˜ã‚ˆã†ã«ã‚¯ãƒ©ã‚¹ã‚’å®šç¾©ã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class llm_sys_elyza:\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.messages = []\n",
    "        self.talk_count = 0\n",
    "        self.generated_text = None\n",
    "        self.prompt = \"\"\n",
    "        \n",
    "        self.B_INST, self.E_INST = \"[INST]\", \"[/INST]\"\n",
    "        self.B_SYS, self.E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    \n",
    "\n",
    "    def set_system_prompt(self, system_prompt):\n",
    "        self.messages = [{\"role\": \"system\", \"content\": f\"{system_prompt}\"}]\n",
    "\n",
    "    def change_system_prompt(self, system_prompt):\n",
    "        self.messages[0][\"content\"] = system_prompt\n",
    "    \n",
    "    def create_prompt(self, messages):\n",
    "        if len(messages) == 2:\n",
    "            prompt = \"{bos_token}{b_inst} {system}{prompt} {e_inst} \".format(\n",
    "                bos_token=tokenizer.bos_token,\n",
    "                b_inst=self.B_INST,\n",
    "                system=f\"{self.B_SYS}{messages[0]['content']}{self.E_SYS}\",\n",
    "                prompt=f\"{messages[1]['content']}\",\n",
    "                e_inst=self.E_INST,\n",
    "            )\n",
    "            self.prompt = prompt\n",
    "        elif len(messages) > 2:\n",
    "            prompt_after = \"{new_line}{b_inst}{new_line_}{prompt}{e_inst}\".format(\n",
    "                new_line='\\n',\n",
    "                b_inst=self.B_INST,\n",
    "                new_line_='\\n',\n",
    "                prompt=f\"{messages[len(messages)-1]['content']}\",\n",
    "                e_inst=self.E_INST,\n",
    "            )\n",
    "            self.prompt = self.prompt + prompt_after\n",
    "        \n",
    "    def generate(self, input, max_new_tokens=256, forget_flag=False):\n",
    "        if forget_flag:\n",
    "            self.messages = [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "            self.talk_count = 1\n",
    "            self.prompt = self.create_prompt(self.messages)\n",
    "        else:\n",
    "            self.messages = self.messages + [{\"role\": \"user\", \"content\": f\"{input}\"}]\n",
    "            self.talk_count += 1\n",
    "\n",
    "        self.create_prompt(self.messages)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            token_ids = self.tokenizer.encode(self.prompt, add_special_tokens=False, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "            output_ids = self.model.generate(\n",
    "                token_ids.to(self.model.device),\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        output = self.tokenizer.decode(output_ids.tolist()[0][token_ids.size(1) :], skip_special_tokens=True)\n",
    "        self.generated_text = tokenizer.decode(output_ids.tolist()[0])\n",
    "        self.prompt += output\n",
    "        self.messages = self.messages + [{\"role\": \"assistant\", \"content\": f\"{output}\"}]\n",
    "        print(output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "elyza = llm_sys_elyza(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': 'ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚'}]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elyza.set_system_prompt(system_prompt=\"ã‚ãªãŸã¯èª å®Ÿã§å„ªç§€ãªæ—¥æœ¬äººã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚\")\n",
    "elyza.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fateã‚·ãƒªãƒ¼ã‚ºã¯ã€åŸä½œã®å°èª¬ã‚„æ¼«ç”»ã€ã‚¢ãƒ‹ãƒ¡ã€ã‚²ãƒ¼ãƒ ãªã©ã€æ§˜ã€…ãªãƒ¡ãƒ‡ã‚£ã‚¢ãƒŸãƒƒã‚¯ã‚¹ä½œå“ã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚ãã®ãŸã‚ã€é †ç•ªã‚’æ±ºã‚ã‚‹ã®ã¯é›£ã—ã„ã§ã™ãŒã€ä¸€èˆ¬çš„ã«ã¯ä»¥ä¸‹ã®é †ç•ªã§è¦–è´ã™ã‚‹ã“ã¨ãŒãŠã™ã™ã‚ã§ã™ã€‚\n",
      "\n",
      "1. Fate/stay night\n",
      "2. Fate/Zero\n",
      "3. Fate/Apocrypha\n",
      "4. Fate/Grand Order\n",
      "5. Fate/EXTRA\n",
      "6. Fate/kaleid liner ãƒ—ãƒªã‚ºãƒâ˜†ã‚¤ãƒªãƒ¤\n",
      "7. Fate/hollow ataraxia\n",
      "8. Fate/unlimited codes\n",
      "9. Fate/stay night [Heaven's Feel]\n",
      "\n",
      "ãŸã ã—ã€ã“ã‚Œã¯ã‚ãã¾ã§ã‚‚ä¸€ã¤ã®æ„è¦‹ã§ã‚ã‚Šã€é †ç•ªã‚’æ±ºã‚ã‚‹è¦ç´ ã¯\n"
     ]
    }
   ],
   "source": [
    "elyza.generate(input=\"Fateã‚·ãƒªãƒ¼ã‚ºã‚’è¦–è´ã™ã‚‹é †ç•ªã‚’æ•™ãˆã¦ä¸‹ã•ã„ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fateã‚·ãƒªãƒ¼ã‚ºã¯ã€åŸä½œã®å°èª¬ã‚„æ¼«ç”»ã€ã‚¢ãƒ‹ãƒ¡ã€ã‚²ãƒ¼ãƒ ãªã©ã€æ§˜ã€…ãªãƒ¡ãƒ‡ã‚£ã‚¢ãƒŸãƒƒã‚¯ã‚¹ä½œå“ã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚ãã®ãŸã‚ã€ä¸€ã¤ã®ä½œå“ãŒä»–ã‚’åœ§å€’ã™ã‚‹ç¨‹åº¦ã®å„ªä½æ€§ã‚’æŒã¤ã“ã¨ã¯é›£ã—ãã€æ¯”è¼ƒã™ã‚‹ã“ã¨è‡ªä½“ãŒé›£ã—ã„ã§ã™ã€‚\n",
      "\n",
      "ã—ã‹ã—ã€ä¸€èˆ¬çš„ãªäººæ°—ã‚„è©•ä¾¡ã‚’è¦‹ã‚‹ã¨ã€Fate/stay nightãŒæœ€ã‚‚äººæ°—ãŒé«˜ãã€ä»–ã®ä½œå“ã¨ã®å·®ã¯å¤§ããã‚ã‚Šã¾ã›ã‚“ã€‚\n",
      "\n",
      "ä»¥ä¸‹ã¯ã€å„ä½œå“ã®è©•ä¾¡ã‚’ç¤ºã™ä¸€ã¤ã®æŒ‡æ¨™ã¨ãªã‚‹æ•°å€¤ã§ã™ã€‚\n",
      "\n",
      "1. Fate/stay night: 8.6\n",
      "2. Fate/Zero: 8.5\n",
      "3. Fate/Apocrypha:\n"
     ]
    }
   ],
   "source": [
    "elyza.generate(input=\"ã©ã‚ŒãŒä¸€ç•ªé¢ç™½ã„ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fate/stay nightã®ä¸»äººå…¬ã¯ã€è¡›å®®å£«éƒ (ãˆã„ãã†ã—ã‚ã†) ã§ã™ã€‚å½¼ã¯ã€é­”è¡“å¸«ã®çˆ¶è¦ªã¨å·«å¥³ã®æ¯è¦ªã®é–“ã«ç”Ÿã¾ã‚Œã€å¹¼ã„é ƒã«çˆ¶è¦ªã‚’äº¡ãã—ã¾ã™ã€‚ãã®å¾Œã€å…„ã®è¡›å®®åˆ‡å—£ (ãˆã„ãã†ãã‚Šã•ã) ã¨å…±ã«è‚²ã¦ã‚‰ã‚Œã¾ã™ãŒã€åˆ‡å—£ãŒé­”è¡“å¸«ã«ãªã‚‹ãŸã‚ã«ä¿®è¡Œã«å‡ºã‚‹ã¨ã€å£«éƒã¯é­”è¡“å¸«ã«ãªã‚‹ãŸã‚ã®æ‰èƒ½ãŒãªã„ã¨åˆ¤æ–­ã•ã‚Œã€å®¶å‡ºã‚’ã—ã¾ã™ã€‚ãã®å¾Œã€å£«éƒã¯é­”è¡“å¸«ã«ãªã‚‹ãŸã‚ã«ä¿®è¡Œã«å‡ºã‚‹ã“ã¨ã‚’æ±ºæ„ã—ã¾ã™ãŒã€ãã®é€”ä¸­ã§é­”è¡“å¸«ã«ã‚ˆã‚‹æˆ¦äº‰ã®é‹å‘½ã®æˆ¦ã„ã€è–æ¯æˆ¦äº‰ã«å·»ãè¾¼ã¾ã‚Œã¾ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "elyza.generate(input=\"Fate/stay nightã®ä¸»äººå…¬ã¯èª°ã§ã™ã‹ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fate/stay nightã«ãŠã„ã¦ã€è–æ¯æˆ¦äº‰ã¨ã¯ã€é­”è¡“å¸«ã®é‹å‘½ã®æˆ¦ã„ã®ã“ã¨ã§ã™ã€‚\n",
      "\n",
      "è–æ¯æˆ¦äº‰ã¯ã€é­”è¡“å¸«ã®é‹å‘½ã®æˆ¦ã„ã§ã‚ã‚‹ã¨åŒæ™‚ã«ã€é­”è¡“å¸«ã®ä¸­ã§ã‚‚ç‰¹ã«å¼·ã„åŠ›ã‚’æŒã¤ã€Œé­”è¡“å¸«ã€ã®é ‚ç‚¹ã‚’æ±ºã‚ã‚‹æˆ¦ã„ã§ã‚‚ã‚ã‚Šã¾ã™ã€‚\n",
      "\n",
      "è–æ¯æˆ¦äº‰ã«å‹åˆ©ã™ã‚‹ã“ã¨ã§ã€é¡˜ã„ã‚’å¶ãˆã‚‹ã“ã¨ã®ã§ãã‚‹ã€Œè–æ¯ã€ã‚’æ‰‹ã«å…¥ã‚Œã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
      "\n",
      "Fate/stay nightã§ã¯ã€ä¸»äººå…¬ã®è¡›å®®å£«éƒãŒè–æ¯æˆ¦äº‰ã«å‚åŠ ã—ã€é‹å‘½ã®ç›¸æ‰‹ã§ã‚ã‚‹ã‚¢ãƒ¼ãƒãƒ£ãƒ¼ã®ã‚¢ã‚µã‚·ãƒ³ã¨æˆ¦ã„ã¾ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "elyza.generate(input=\"è–æ¯æˆ¦äº‰ã£ã¦ãªã‚“ã§ã™ã‹ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ—¥æœ¬ã®ã‚¢ãƒ‹ãƒ¡ã«ã‹ãªã‚Šè©³ã—ãã†ã€‚ä¼šè©±ã®è¨˜æ†¶ã®ä¿æŒã«æˆåŠŸã—ãŸãŒã€æ¨è«–æ™‚é–“ãŒå¢—åŠ ã™ã‚‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã¨ã‚Šã‚ãˆãšç”Ÿæˆã«ã¯æˆåŠŸã—ãŸãŒä½œæˆã—ãŸã‚¯ãƒ©ã‚¹ã«ã¯å•é¡ŒãŒã‚ã‚‹ã®ã§è¦æ”¹å–„ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
